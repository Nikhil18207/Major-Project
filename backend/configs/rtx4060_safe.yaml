# XR2Text Safe Configuration for RTX 4060 Laptop GPU
# =====================================================
# Optimized to prevent overheating and ensure stable training
# Authors: S. Nikhil, Dadhania Omkumar
# Supervisor: Dr. Damodar Panigrahy

# Model Configuration
model:
  image_size: 384
  freeze_encoder: false
  use_anatomical_attention: true  # Enable HAQT-ARR (Novel)

  encoder:
    model_name: "base"  # tiny, small, base, large
    pretrained: true
    freeze_layers: 0
    output_dim: 1024
    drop_rate: 0.1
    attn_drop_rate: 0.1

  # HAQT-ARR Projection Layer (Novel Contribution)
  projection:
    language_dim: 1024                # FIXED: Must match BioBART-Large hidden dim
    num_regions: 7
    num_global_queries: 8
    num_region_queries: 4
    num_attention_heads: 8
    num_cross_region_layers: 2
    feature_size: 12
    dropout: 0.1
    use_spatial_priors: true
    use_adaptive_routing: true
    use_cross_region: true
    num_projection_layers: 2
    num_queries: 32
    use_cross_attention: true
    use_residual: true

  decoder:
    model_name: "biobart-large"       # FIXED: Use BioBART-Large for better generation
    max_length: 512
    freeze_embeddings: false
    freeze_layers: 0
    use_cache: true
    dropout: 0.1

# Training Configuration - BULLETPROOF FOR RTX 4060 (8GB VRAM)
training:
  epochs: 100                         # FIXED: Full training for curriculum learning
  batch_size: 1                       # FIXED: BioBART-Large needs batch_size=1 for 8GB VRAM
  learning_rate: 1.0e-4               # FIXED: Higher LR for faster convergence
  weight_decay: 0.05                  # FIXED: More regularization
  warmup_steps: 1000                  # FIXED: Longer warmup
  gradient_accumulation_steps: 32     # FIXED: Effective batch size = 1 * 32 = 32
  max_grad_norm: 1.0
  use_amp: true  # ESSENTIAL - Mixed precision reduces memory and heat

  scheduler: "cosine"

  patience: 15

  label_smoothing: 0.1

  use_scheduled_sampling: true
  scheduled_sampling_start: 1.0
  scheduled_sampling_end: 0.5
  scheduled_sampling_warmup: 3

  use_region_regularization: true
  region_regularization_weight: 0.01

  # R-Drop regularization (DISABLED for 2x faster training)
  use_rdrop: false
  rdrop_alpha: 0.7

  # NOVEL: Novel loss functions (can disable to reduce compute/heat)
  use_novel_losses: true
  use_anatomical_consistency_loss: true
  use_clinical_entity_loss: true
  use_region_focal_loss: true
  use_cross_modal_loss: false
  anatomical_loss_weight: 0.1
  clinical_loss_weight: 0.2
  focal_loss_weight: 0.15
  alignment_loss_weight: 0.1

  # NOVEL: Curriculum learning
  use_curriculum_learning: true

  # NOVEL: Clinical validation
  use_clinical_validation: true

  # GPU Temperature Monitoring (ADJUSTED FOR ACTIVE COOLING)
  enable_temp_monitoring: true
  max_gpu_temp: 95        # Hard stop temperature (safety limit)
  warning_gpu_temp: 85    # Warning threshold (just logs, no action)
  pause_gpu_temp: 90      # Pause training threshold (only if very hot)
  temp_check_interval: 50 # Check every 50 batches (less frequent)
  gpu_cooldown_time: 30   # Wait 30 seconds when paused

  # MEMORY OPTIMIZATION (BULLETPROOF OOM PREVENTION)
  gradient_checkpointing: true  # Enables gradient checkpointing in encoder (saves ~30% VRAM)
  validate_every: 2  # FIXED: Validate every 2 epochs to see BLEU-4 and ROUGE-L
  val_fraction: 0.25  # Use only 25% of validation set during training

  # OOM Recovery (NEW)
  oom_recovery_enabled: true  # Auto-recover from OOM by skipping batch
  clear_cache_frequency: 100  # Clear CUDA cache every N batches

  checkpoint_dir: "checkpoints"
  save_every: 3
  log_every: 50
  experiment_name: "xr2text_haqt_arr_rtx4060_safe"

# Data Configuration
data:
  dataset: "itsanmolgupta/mimic-cxr-dataset"
  image_size: 384
  max_length: 256
  target_type: "both"
  num_workers: 2  # Reduced to save CPU/RAM
  pin_memory: true
  prefetch_factor: 2

# Generation Configuration
generation:
  max_length: 256
  min_length: 20
  num_beams: 4  # For inference/final evaluation
  val_num_beams: 2  # REDUCED for validation during training (saves memory)
  temperature: 1.0
  top_k: 50
  top_p: 0.95
  repetition_penalty: 1.2
  length_penalty: 1.0
  no_repeat_ngram_size: 3
  early_stopping: true
  do_sample: false

# API Configuration
api:
  host: "0.0.0.0"
  port: 8000
  workers: 1
  reload: false

# Logging Configuration
logging:
  level: "INFO"
  log_dir: "logs"
  log_to_file: true
  log_to_console: true

# ============================================
# RTX 4060 SAFETY NOTES (Updated for BioBART-Large):
# ============================================
# 1. Batch size is set to 1 (required for BioBART-Large 8GB VRAM)
#    - gradient_accumulation_steps: 32 gives effective batch of 32
#    - R-Drop DISABLED for 2x faster training
#
# 2. Temperature monitoring will automatically:
#    - Warn at 85°C
#    - Pause training at 90°C
#    - Stop if reaches 95°C (safety limit)
#
# 3. If overheating occurs:
#    - Disable novel losses temporarily
#    - Ensure laptop has good ventilation
#    - Consider using a cooling pad
#
# 4. Monitor GPU temperature:
#    - Check nvidia-smi periodically
#    - Training will log temperature every 50 batches
#    - Watch for repeated pauses (indicates need for adjustment)
#
# 5. Performance tips:
#    - Keep laptop elevated for better airflow
#    - Close other GPU-intensive applications
#    - Use Windows "High Performance" power plan
#    - gradient_checkpointing: true saves ~30% VRAM

