# XR2Text Configuration for RunPod A100 PCIe (80 GB VRAM)
# =============================================================================
# A100 80GB - BEST FOR DEEP LEARNING - Superior Memory Bandwidth & Tensor Cores
# 80GB VRAM + 117GB RAM + 12 vCPU
# =============================================================================
# Authors: S. Nikhil, Dadhania Omkumar
# Supervisor: Dr. Damodar Panigrahy
# =============================================================================
# TARGET: BLEU-4 > 0.15, ROUGE-L > 0.35 (competitive with SOTA)
# =============================================================================
# FIXES APPLIED:
# 1. Fixed curriculum learning criteria (was too restrictive)
# 2. Added min_lr_ratio to prevent LR dropping to 0
# 3. Increased dropout for regularization
# 4. Better scheduled sampling parameters
# 5. EMA (Exponential Moving Average) for stable training
# =============================================================================

# =============================================================================
# MODEL CONFIGURATION - Full 541M Parameter Model
# =============================================================================
model:
  image_size: 512                         # High resolution for detail
  freeze_encoder: false                   # Train everything!
  use_anatomical_attention: true          # HAQT-ARR (NOVEL)
  gradient_checkpointing: false           # NOT NEEDED - 80GB is plenty!

  # ALL NOVEL ENHANCEMENT MODULES ENABLED
  use_uncertainty: true                   # Uncertainty Quantification (NOVEL)
  use_grounding: true                     # Factual Grounding (NOVEL)
  use_explainability: true                # Explainability (NOVEL)
  use_multitask: true                     # Multi-Task Learning (NOVEL)

  encoder:
    model_name: "base"                    # Swin-Base (88M params)
    pretrained: true
    freeze_layers: 0                      # Unfreeze ALL layers
    output_dim: 1024
    drop_rate: 0.15                       # INCREASED from 0.1 for regularization
    attn_drop_rate: 0.15                  # INCREASED from 0.1

  # HAQT-ARR Projection Layer (NOVEL - Core Contribution)
  projection:
    language_dim: 1024
    num_regions: 7                        # 7 anatomical regions
    num_global_queries: 16                # Doubled from 8
    num_region_queries: 8                 # Doubled from 4
    num_attention_heads: 16               # Doubled from 8
    num_cross_region_layers: 3            # Deeper cross-region
    feature_size: 16                      # 512/32 = 16x16 patches
    dropout: 0.15                         # INCREASED from 0.1
    use_spatial_priors: true              # Learnable 2D Gaussian priors (NOVEL)
    use_adaptive_routing: true            # Dynamic region routing (NOVEL)
    use_cross_region: true                # Cross-region transformers (NOVEL)
    num_projection_layers: 3
    num_queries: 64                       # Doubled from 32
    use_cross_attention: true
    use_residual: true

  decoder:
    model_name: "biobart-large"           # BioBART-Large (406M params)
    max_length: 512
    freeze_embeddings: false
    freeze_layers: 0                      # Train all decoder layers
    use_cache: true
    dropout: 0.15                         # INCREASED from 0.1
    use_focal_loss: true                  # IMPROVED: Focus on hard-to-predict tokens
    focal_gamma: 2.0                      # Focal loss gamma (higher = more focus on hard)
    focal_gamma_warmup_epochs: 5          # IMPROVED: Warmup epochs for focal loss (start from 0)

# =============================================================================
# TRAINING CONFIGURATION - OPTIMIZED FOR A100 80GB + BETTER CONVERGENCE
# =============================================================================
# MEMORY BUDGET BREAKDOWN (A100 80GB):
# ┌─────────────────────────────────────────────────────────────────────────┐
# │ Component                          │ VRAM Usage      │ Notes            │
# ├─────────────────────────────────────────────────────────────────────────┤
# │ Model Parameters (541M @ FP16)     │ ~1.1 GB         │ Fixed            │
# │ Optimizer States (AdamW)           │ ~4.4 GB         │ Fixed            │
# │ Activations (batch=48, 512px)      │ ~35-40 GB       │ Main cost        │
# │ Gradients                          │ ~2.2 GB         │ Fixed            │
# │ Generation (beam=6, diverse)       │ ~3-4 GB         │ During inference │
# │ Buffer/Fragmentation               │ ~15-20 GB       │ Safety margin    │
# ├─────────────────────────────────────────────────────────────────────────┤
# │ TOTAL ESTIMATED                    │ ~55-65 GB       │ ✓ SAFE (80GB)    │
# └─────────────────────────────────────────────────────────────────────────┘
# =============================================================================
training:
  epochs: 50                              # Full 50 epochs

  # ==========================================================================
  # BATCH SIZE - SAFE FOR 80GB VRAM (with buffer for spikes)
  # ==========================================================================
  batch_size: 32                          # FIXED: Reduced from 48 to avoid OOM
  gradient_accumulation_steps: 1          # No accumulation needed

  # ==========================================================================
  # LEARNING RATE - TUNED FOR BETTER CONVERGENCE
  # ==========================================================================
  learning_rate: 5.0e-5                   # REDUCED from 1e-4 for stability
  encoder_lr: 1.0e-5                      # REDUCED - pretrained needs lower LR
  decoder_lr: 5.0e-5                      # REDUCED for stability
  projection_lr: 1.0e-4                   # Higher for new HAQT-ARR layers

  weight_decay: 0.05                      # INCREASED from 0.01 for regularization
  warmup_steps: 1500                      # INCREASED for more stable warmup
  max_grad_norm: 0.5                      # REDUCED from 1.0 for stability
  use_amp: true                           # FP16 - A100 has excellent FP16 perf

  # ==========================================================================
  # SCHEDULER - FIXED: Plain cosine is more stable with curriculum learning
  # ==========================================================================
  scheduler: "cosine"                     # CHANGED from cosine_with_restarts - more stable
  num_cycles: 1                           # Not used for plain cosine
  min_lr_ratio: 0.1                       # CRITICAL: Don't let LR drop below 10% of base

  patience: 30                            # INCREASED - let model train longer

  # VALIDATION
  val_fraction: 0.5                       # 50% of val set
  validate_every: 10                      # Every 10 epochs (5 validations total, saves ~1.4 hours)

  label_smoothing: 0.15                   # INCREASED from 0.1 for better generalization

  # ==========================================================================
  # SCHEDULED SAMPLING - MORE GRADUAL DECAY
  # ==========================================================================
  use_scheduled_sampling: true
  scheduled_sampling_start: 1.0           # Start with 100% teacher forcing
  scheduled_sampling_end: 0.8             # INCREASED from 0.7 - more teacher forcing
  scheduled_sampling_warmup: 15           # INCREASED from 10 - longer warmup

  # ==========================================================================
  # R-DROP - REDUCED ALPHA FOR STABILITY
  # ==========================================================================
  use_rdrop: false                        # DISABLED - causes 2x VRAM (OOM with batch=48)
  rdrop_alpha: 0.1                        # Only used if use_rdrop: true

  use_region_regularization: true
  region_regularization_weight: 0.001

  # ==========================================================================
  # EMA (Exponential Moving Average) - IMPROVED: Stable training
  # ==========================================================================
  use_ema: true                           # IMPROVED: Use EMA for more stable weights
  ema_decay: 0.9999                       # High decay for smooth updates

  # ==========================================================================
  # NOVEL LOSSES - MINIMAL WEIGHTS TO FOCUS ON MAIN TASK
  # ==========================================================================
  use_novel_losses: true
  use_anatomical_consistency_loss: true
  use_clinical_entity_loss: false         # DISABLED - non-differentiable
  use_region_focal_loss: true
  use_cross_modal_loss: false             # DISABLED

  # MINIMAL auxiliary loss weights - FOCUS ON MAIN CE LOSS
  anatomical_loss_weight: 0.005           # REDUCED from 0.01
  clinical_loss_weight: 0.0
  focal_loss_weight: 0.005                # REDUCED from 0.01
  alignment_loss_weight: 0.0

  # ==========================================================================
  # CURRICULUM LEARNING - FIXED CRITERIA (was too restrictive!)
  # ==========================================================================
  use_curriculum_learning: true
  curriculum_stages:
    # Stage 1: Warmup - Simple cases with few findings (NOT just normal!)
    - name: "warmup"
      epoch_start: 0
      epoch_end: 5
      criteria: {max_findings: 1, max_regions: 2}  # FIXED: removed severity:normal

    # Stage 2: Easy - Cases with 2-3 findings
    - name: "easy"
      epoch_start: 5
      epoch_end: 12
      criteria: {max_findings: 2, max_regions: 3}

    # Stage 3: Medium - Cases with up to 4 findings
    - name: "medium"
      epoch_start: 12
      epoch_end: 25
      criteria: {max_findings: 4, max_regions: 5}

    # Stage 4: Hard - All cases
    - name: "hard"
      epoch_start: 25
      epoch_end: 40
      criteria: {}  # All cases

    # Stage 5: Fine-tune - All cases with lower LR (handled by scheduler)
    - name: "finetune"
      epoch_start: 40
      epoch_end: 50
      criteria: {}  # All cases

  # Clinical Validation
  use_clinical_validation: true

  # Uncertainty Training
  use_uncertainty_training: true
  uncertainty_dropout: 0.1
  mc_samples: 5
  use_calibration: true

  # Multi-Task Learning - REDUCED weights
  use_multi_task_learning: true
  auxiliary_task_weights:
    region_classification: 0.02           # REDUCED from 0.05
    severity_prediction: 0.02             # REDUCED from 0.05
    finding_detection: 0.05               # REDUCED from 0.1
    length_prediction: 0.01               # REDUCED from 0.02

  # Factual Grounding
  use_factual_grounding: true
  grounding_loss_weight: 0.02             # REDUCED from 0.05
  grounding_threshold: 0.15

  use_pretraining: false
  use_ood_detection: true
  ood_threshold: 0.5

  # ==========================================================================
  # EMA (Exponential Moving Average) - NEW: For stable training
  # ==========================================================================
  use_ema: true
  ema_decay: 0.9999                       # Standard EMA decay

  # ==========================================================================
  # GPU SETTINGS - A100 PCIe (SPEED OPTIMIZED)
  # ==========================================================================
  enable_temp_monitoring: false             # A100 has excellent thermal design

  cublas_retry_enabled: true
  cublas_max_retries: 3
  cublas_retry_delay: 3                     # REDUCED - faster retry
  clear_cache_every_steps: 500              # INCREASED - less overhead
  max_oom_retries: 3

  # A100 SPECIFIC OPTIMIZATIONS
  use_tf32: true                            # TF32 for faster matmul on A100
  cudnn_benchmark: true                     # Auto-tune convolutions
  compile_model: false                      # torch.compile (set true for PyTorch 2.0+)

  # ==========================================================================
  # CHECKPOINTING
  # ==========================================================================
  checkpoint_dir: "checkpoints"
  log_every: 10

  experiment_name: "xr2text_a100_80gb_fixed"

# =============================================================================
# DATA CONFIGURATION - SPEED OPTIMIZED FOR A100 + 12 vCPU
# =============================================================================
data:
  dataset: "itsanmolgupta/mimic-cxr-dataset"
  image_size: 512
  max_length: 300
  target_type: "both"
  num_workers: 12                           # INCREASED - match vCPU count
  pin_memory: true
  prefetch_factor: 4                        # Prefetch 4 batches per worker
  persistent_workers: true                  # Keep workers alive between epochs

# =============================================================================
# GENERATION CONFIGURATION - OPTIMIZED FOR HIGH BLEU/ROUGE
# =============================================================================
# IMPROVED: Better generation parameters for clinical text quality
# Memory-safe: These settings use ~2-3GB extra VRAM max (well within 80GB budget)
generation:
  max_length: 350                           # Complete reports
  min_length: 50                            # Avoid truncated reports

  # ==========================================================================
  # BEAM SEARCH SETTINGS (Primary - for evaluation metrics)
  # ==========================================================================
  num_beams: 4                              # REDUCED from 6 to avoid OOM
  val_num_beams: 3                          # REDUCED - faster validation
  early_stopping: true                      # Stop when all beams finish
  length_penalty: 1.5                       # Encourage complete reports
  repetition_penalty: 1.3                   # Reduce repetition
  no_repeat_ngram_size: 4                   # Prevent phrase repetition

  # ==========================================================================
  # DIVERSE BEAM SEARCH (NEW - for better exploration)
  # ==========================================================================
  # Memory: Only ~5% more VRAM than regular beam search
  use_diverse_beam_search: true             # NEW: Enable diverse beam search
  num_beam_groups: 2                        # NEW: 2 groups of 3 beams each
  diversity_penalty: 0.5                    # NEW: Penalty for similar beams

  # ==========================================================================
  # NUCLEUS SAMPLING OPTIONS (for demo mode - more diverse outputs)
  # ==========================================================================
  # Set do_sample: true to enable sampling mode (for demos/diversity)
  do_sample: false                          # false = beam search (better metrics)
  temperature: 0.9                          # Lower = more focused
  top_k: 50                                 # Top-k filtering
  top_p: 0.92                               # Nucleus sampling threshold

  # ==========================================================================
  # CONSTRAINED GENERATION (Optional - for structured outputs)
  # ==========================================================================
  bad_words_ids: null                       # Tokens to avoid (e.g., profanity)
  forced_bos_token_id: null                 # Force start token
  forced_eos_token_id: null                 # Force end token

  # ==========================================================================
  # DEMO MODE PRESET (set demo_mode: true for diverse outputs)
  # ==========================================================================
  demo_mode: false                          # NEW: Quick toggle for demo settings
  # When demo_mode is true, uses: do_sample=true, temperature=0.85, top_p=0.92

# =============================================================================
# LOGGING CONFIGURATION
# =============================================================================
logging:
  level: "INFO"
  log_dir: "logs"
  log_to_file: true
  log_to_console: true

# =============================================================================
# EVALUATION CONFIGURATION
# =============================================================================
evaluation:
  use_bleu: true
  use_rouge: true
  use_meteor: true
  use_cider: true
  use_bertscore: true                             # Semantic similarity via BERT embeddings
  use_clinical_f1: true
  use_radgraph: true
  use_chexbert: true
  use_calibration_metrics: true
  use_ece: true
