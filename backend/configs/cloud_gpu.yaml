# XR2Text Configuration for Cloud GPU (20GB+ VRAM)
# =============================================================================
# OPTIMIZED FOR RTX 4000 Ada / RTX 4090 / A100
# Much faster training with larger batch sizes
# =============================================================================

# Model Configuration (same as local)
model:
  image_size: 384
  freeze_encoder: false
  use_anatomical_attention: true
  gradient_checkpointing: false  # Not needed with more VRAM

  use_uncertainty: true
  use_grounding: true
  use_explainability: true
  use_multitask: true

  encoder:
    model_name: "base"
    pretrained: true
    freeze_layers: 0
    output_dim: 1024
    drop_rate: 0.1
    attn_drop_rate: 0.1

  projection:
    language_dim: 1024
    num_regions: 7
    num_global_queries: 8
    num_region_queries: 4
    num_attention_heads: 8
    num_cross_region_layers: 2
    feature_size: 12
    dropout: 0.1
    use_spatial_priors: true
    use_adaptive_routing: true
    use_cross_region: true
    num_projection_layers: 2
    num_queries: 32
    use_cross_attention: true
    use_residual: true

  decoder:
    model_name: "biobart-large"
    max_length: 512
    freeze_embeddings: false
    freeze_layers: 0
    use_cache: true
    dropout: 0.1

# Training Configuration - CLOUD OPTIMIZED (20GB+ VRAM)
training:
  epochs: 50
  batch_size: 8                              # 8x batch for 20GB+ VRAM GPUs

  # ==========================================================================
  # LEARNING RATES - LAYER-WISE FOR FASTER CONVERGENCE
  # ==========================================================================
  learning_rate: 2.0e-4                      # Base LR (doubled for faster learning)
  encoder_lr: 5.0e-5                         # Lower LR for pretrained Swin encoder
  decoder_lr: 2.0e-4                         # Standard LR for BioBART decoder
  projection_lr: 3.0e-4                      # Higher LR for new HAQT-ARR layers

  weight_decay: 0.01                         # Reduced from 0.05 for less regularization
  warmup_steps: 500                          # Warmup steps for stability
  gradient_accumulation_steps: 8             # Effective batch = 8*8 = 64
  max_grad_norm: 1.0
  use_amp: true

  scheduler: "cosine_with_restarts"
  num_cycles: 2                              # 2 cycles (restart at epoch 25)
  min_lr_ratio: 0.01                         # Don't decay LR too much

  patience: 20

  val_fraction: 0.15                         # 15% of val set for validation
  validate_every: 1                          # Validate every epoch

  label_smoothing: 0.1                       # For better generalization

  use_scheduled_sampling: true
  scheduled_sampling_start: 1.0
  scheduled_sampling_end: 0.6                # More teacher forcing for stability
  scheduled_sampling_warmup: 5               # Reduced warmup

  use_rdrop: false                           # Disabled for faster training
  rdrop_alpha: 0.5

  use_region_regularization: true
  region_regularization_weight: 0.005        # Reduced

  # Novel losses - REBALANCED FOR FASTER CONVERGENCE
  use_novel_losses: true
  use_anatomical_consistency_loss: true
  use_clinical_entity_loss: true
  use_region_focal_loss: true
  use_cross_modal_loss: false                # Disabled initially - enable after epoch 10
  anatomical_loss_weight: 0.05               # Reduced from 0.1
  clinical_loss_weight: 0.1                  # Reduced from 0.2
  focal_loss_weight: 0.05                    # Reduced from 0.15
  alignment_loss_weight: 0.0                 # Disabled initially

  # Curriculum learning - FASTER PROGRESSION
  use_curriculum_learning: true
  curriculum_stages:
    - name: "warmup"
      epoch_start: 0
      epoch_end: 5
      criteria: {max_findings: 1, severity: "normal"}
    - name: "easy"
      epoch_start: 5
      epoch_end: 12
      criteria: {max_findings: 2, max_regions: 2}
    - name: "medium"
      epoch_start: 12
      epoch_end: 25
      criteria: {max_findings: 4, max_regions: 4}
    - name: "hard"
      epoch_start: 25
      epoch_end: 40
      criteria: {}
    - name: "finetune"
      epoch_start: 40
      epoch_end: 50
      criteria: {}

  use_clinical_validation: true

  use_uncertainty_training: true
  uncertainty_dropout: 0.1
  mc_samples: 5
  use_calibration: true

  use_multi_task_learning: true
  auxiliary_task_weights:
    region_classification: 0.1
    severity_prediction: 0.1
    finding_detection: 0.15
    length_prediction: 0.05

  use_factual_grounding: true
  grounding_loss_weight: 0.1
  grounding_threshold: 0.15

  use_pretraining: false
  use_ood_detection: true
  ood_threshold: 0.5

  # GPU monitoring (less aggressive for cloud)
  enable_temp_monitoring: false             # Cloud GPUs have good cooling

  # Error recovery
  cublas_retry_enabled: true
  cublas_max_retries: 3
  cublas_retry_delay: 10
  clear_cache_every_steps: 20               # Less frequent on cloud
  max_oom_retries: 5

  checkpoint_dir: "checkpoints"
  save_every: 1
  log_every: 50                             # More frequent logging

  experiment_name: "xr2text_cloud"

# Data Configuration
data:
  dataset: "itsanmolgupta/mimic-cxr-dataset"
  image_size: 384
  max_length: 256
  target_type: "both"
  num_workers: 4                            # INCREASED: Cloud has more CPU
  pin_memory: true
  prefetch_factor: 4                        # INCREASED

# Generation Configuration
generation:
  max_length: 200
  min_length: 20
  num_beams: 2
  temperature: 1.0
  top_k: 50
  top_p: 0.92
  repetition_penalty: 1.3
  length_penalty: 1.0
  no_repeat_ngram_size: 3
  early_stopping: true
  do_sample: false
  num_beam_groups: 1
  diversity_penalty: 0.0

# API Configuration
api:
  host: "0.0.0.0"
  port: 8000
  workers: 1
  reload: false

# Logging Configuration
logging:
  level: "INFO"
  log_dir: "logs"
  log_to_file: true
  log_to_console: true

# Evaluation Configuration
evaluation:
  use_bleu: true
  use_rouge: true
  use_meteor: true
  use_cider: true
  use_clinical_f1: true
  use_radgraph: true
  use_chexbert: true
  use_calibration_metrics: true
  use_ece: true
