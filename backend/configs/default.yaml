# XR2Text IMPROVED Configuration with HAQT-ARR (Novel)
# =====================================================
# OPTIMIZED FOR RTX 4060 8GB VRAM
# Authors: S. Nikhil, Dadhania Omkumar
# Supervisor: Dr. Damodar Panigrahy
#
# IMPROVEMENTS MADE:
# 1. BioBART-Large decoder for better generation
# 2. Optimized learning rate and schedule
# 3. R-Drop regularization enabled
# 4. Cross-modal loss enabled
# 5. Better generation parameters
# 6. Gradient checkpointing for memory efficiency
# 7. Extended curriculum learning stages

# Model Configuration
model:
  image_size: 384  # Keep 384 for RTX 4060 memory (512 would OOM)
  freeze_encoder: false
  use_anatomical_attention: true  # Enable HAQT-ARR (Novel)
  gradient_checkpointing: true    # IMPROVED: Enable for memory efficiency

  # NOVEL: Enhancement Modules (10/10 Novelty)
  use_uncertainty: true          # Uncertainty quantification
  use_grounding: true            # Factual grounding & hallucination detection
  use_explainability: true       # Explainability & evidence regions
  use_multitask: true            # Multi-task learning heads

  encoder:
    model_name: "base"  # Swin-Base (keep for RTX 4060 memory)
    pretrained: true
    freeze_layers: 0
    output_dim: 1024
    drop_rate: 0.1
    attn_drop_rate: 0.1

  # HAQT-ARR Projection Layer (Novel Contribution)
  projection:
    language_dim: 1024                # FIXED: Must match BioBART-Large hidden dim
    num_regions: 7              # 7 anatomical regions
    num_global_queries: 8       # Global context queries
    num_region_queries: 4       # Queries per anatomical region (total: 8 + 7*4 = 36)
    num_attention_heads: 8
    num_cross_region_layers: 2  # Cross-region interaction layers
    feature_size: 12            # 384/32 = 12x12 patch grid
    dropout: 0.1
    # Novel components
    use_spatial_priors: true    # Learnable 2D Gaussian priors
    use_adaptive_routing: true  # Dynamic region importance
    use_cross_region: true      # Cross-region interaction
    # Standard projection parameters (fallback)
    num_projection_layers: 2
    num_queries: 32
    use_cross_attention: true
    use_residual: true

  decoder:
    # IMPROVED: Use BioBART-Large for better generation quality
    model_name: "biobart-large"  # Changed from "biobart" (base)
    max_length: 512
    freeze_embeddings: false
    freeze_layers: 0
    use_cache: true
    dropout: 0.1

# Training Configuration - OPTIMIZED FOR RTX 4060 8GB
training:
  epochs: 50                              # 50 epochs for full training
  batch_size: 1                           # RTX 4060 can only handle batch_size=1

  # ==========================================================================
  # LEARNING RATES - LAYER-WISE FOR FASTER CONVERGENCE
  # ==========================================================================
  learning_rate: 2.0e-4                   # Base LR (doubled for faster learning)
  encoder_lr: 5.0e-5                      # Lower LR for pretrained Swin encoder
  decoder_lr: 2.0e-4                      # Standard LR for BioBART decoder
  projection_lr: 3.0e-4                   # Higher LR for new HAQT-ARR layers

  weight_decay: 0.01                      # Reduced from 0.05 for less regularization
  warmup_steps: 500                       # Warmup steps for stability
  gradient_accumulation_steps: 64         # Effective batch = 64 (more gradient updates)
  max_grad_norm: 1.0
  use_amp: true                           # Essential for RTX 4060

  # Scheduler
  scheduler: "cosine_with_restarts"       # Restarts help escape local minima
  num_cycles: 2                           # 2 cycles (restart at epoch 25)
  min_lr_ratio: 0.01                      # Don't decay LR too much

  # Early stopping
  patience: 20                            # More patience for convergence

  # Validation
  val_fraction: 0.10                      # 10% of val set for faster validation
  validate_every: 2                       # Validate every 2 epochs (RTX 4060 is slow)

  # Label smoothing
  label_smoothing: 0.1                    # For better generalization

  # Scheduled sampling for improved generation
  use_scheduled_sampling: true
  scheduled_sampling_start: 1.0
  scheduled_sampling_end: 0.6             # More teacher forcing for stability
  scheduled_sampling_warmup: 5            # Reduced warmup

  # R-Drop regularization (DISABLED for RTX 4060 - too slow)
  use_rdrop: false
  rdrop_alpha: 0.5

  # Region weight regularization
  use_region_regularization: true
  region_regularization_weight: 0.005     # Reduced

  # ==========================================================================
  # NOVEL LOSS FUNCTIONS - REBALANCED FOR FASTER CONVERGENCE
  # ==========================================================================
  use_novel_losses: true
  use_anatomical_consistency_loss: true
  use_clinical_entity_loss: true
  use_region_focal_loss: true
  use_cross_modal_loss: false             # Disabled initially - enable after epoch 10

  # Reduced weights to focus on main task
  anatomical_loss_weight: 0.05
  clinical_loss_weight: 0.1
  focal_loss_weight: 0.05
  alignment_loss_weight: 0.0

  # NOVEL: Curriculum learning - COMPRESSED for 50 epochs
  use_curriculum_learning: true
  curriculum_stages:
    - name: "warmup"
      epoch_start: 0
      epoch_end: 5
      criteria: {max_findings: 1, severity: "normal"}
    - name: "easy"
      epoch_start: 5
      epoch_end: 12
      criteria: {max_findings: 2, max_regions: 2}
    - name: "medium"
      epoch_start: 12
      epoch_end: 25
      criteria: {max_findings: 4, max_regions: 4}
    - name: "hard"
      epoch_start: 25
      epoch_end: 40
      criteria: {}  # All samples
    - name: "finetune"
      epoch_start: 40
      epoch_end: 50
      criteria: {}  # Full dataset fine-tuning with lower LR

  # NOVEL: Clinical validation
  use_clinical_validation: true

  # NOVEL: Uncertainty Quantification
  use_uncertainty_training: true
  uncertainty_dropout: 0.1
  mc_samples: 5                           # Reduced for memory
  use_calibration: true

  # NOVEL: Multi-Task Learning
  use_multi_task_learning: true
  auxiliary_task_weights:
    region_classification: 0.1
    severity_prediction: 0.1
    finding_detection: 0.15
    length_prediction: 0.05

  # NOVEL: Factual Grounding
  use_factual_grounding: true
  grounding_loss_weight: 0.1
  grounding_threshold: 0.15

  # NOVEL: Self-Supervised Pre-training (optional)
  use_pretraining: false
  pretrain_epochs: 10
  contrastive_temperature: 0.07
  mask_ratio: 0.15

  # NOVEL: OOD Detection
  use_ood_detection: true
  ood_threshold: 0.5

  # GPU Temperature Monitoring (RTX 4060)
  enable_temp_monitoring: true            # IMPROVED: Enable monitoring
  max_gpu_temp: 95                        # Stop training at 95C
  warning_gpu_temp: 85                    # Warn at 85C
  pause_gpu_temp: 88                      # Pause at 88C (was 91) - slightly more conservative
  temp_check_interval: 5                  # Check every 5 optimizer steps (was 10) - more frequent
  gpu_cooldown_time: 45                   # 45 second cooldown (was 30)

  # CUBLAS Error Recovery (NEW - prevents random CUDA crashes)
  cublas_retry_enabled: true              # Retry on CUBLAS errors
  cublas_max_retries: 3                   # Number of retries before failing
  cublas_retry_delay: 10                  # Seconds to wait between retries

  # Memory Management (IMPROVED - prevents OOM from memory fragmentation)
  clear_cache_every_steps: 5              # Clear CUDA cache every N optimizer steps (more frequent)
  max_oom_retries: 10                     # Max OOM errors before stopping (increased for long epochs)

  # Checkpointing
  checkpoint_dir: "checkpoints"
  # NOTE: Periodic checkpoints DISABLED - only best_model.pt saved at end of training
  # The trainer tracks the best model (by BLEU-4 + ROUGE-L) in memory and saves it at the end

  # Logging
  log_every: 100
  experiment_name: "xr2text_haqt_arr_improved"

# Data Configuration
data:
  dataset: "itsanmolgupta/mimic-cxr-dataset"
  image_size: 384
  max_length: 256
  target_type: "both"  # findings, impression, both
  num_workers: 2
  pin_memory: true
  prefetch_factor: 2

# Generation Configuration - FAST FOR TRAINING
generation:
  max_length: 200                         # Shorter during training
  min_length: 20
  num_beams: 2                            # FAST: Only 2 beams during training validation
  temperature: 1.0
  top_k: 50
  top_p: 0.92
  repetition_penalty: 1.3
  length_penalty: 1.0
  no_repeat_ngram_size: 3
  early_stopping: true
  do_sample: false

  # DISABLED for speed during training
  num_beam_groups: 1
  diversity_penalty: 0.0

# API Configuration
api:
  host: "0.0.0.0"
  port: 8000
  workers: 1
  reload: false

# Logging Configuration
logging:
  level: "INFO"
  log_dir: "logs"
  log_to_file: true
  log_to_console: true

# Evaluation Configuration - NEW
evaluation:
  # Standard NLG metrics
  use_bleu: true
  use_rouge: true
  use_meteor: true
  use_cider: true

  # IMPROVED: Clinical metrics
  use_clinical_f1: true                   # NEW: Clinical entity F1
  use_radgraph: true                      # NEW: RadGraph score
  use_chexbert: true                      # NEW: CheXbert labeler

  # Uncertainty metrics
  use_calibration_metrics: true
  use_ece: true                           # Expected Calibration Error

# Anatomical Regions (for reference)
# - right_lung: Right lung field (upper, middle, lower lobes)
# - left_lung: Left lung field (upper, lower lobes)
# - heart: Cardiac silhouette
# - mediastinum: Mediastinal structures
# - spine: Thoracic spine
# - diaphragm: Hemidiaphragms (bilateral)
# - costophrenic_angles: CP angles (bilateral)
