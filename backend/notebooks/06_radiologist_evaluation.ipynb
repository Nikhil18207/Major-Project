{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# XR2Text: Radiologist Clinical Evaluation\n",
    "\n",
    "## FULL DATASET - Clinical Validation on NVIDIA A100 80GB\n",
    "\n",
    "**Authors**: S. Nikhil, Dadhania Omkumar  \n",
    "**Supervisor**: Dr. Damodar Panigrahy  \n",
    "**Clinical Evaluator**: [Radiologist Name]\n",
    "\n",
    "---\n",
    "\n",
    "**Dataset**: MIMIC-CXR (Full 30,633 images)  \n",
    "**GPU**: NVIDIA A100 80GB (48GB VRAM) - Run ALL notebooks on A100 80GB!  \n",
    "**Note**: With $10 credits, run everything on A100 80GB for maximum speed\n",
    "\n",
    "---\n",
    "\n",
    "This notebook prepares samples for radiologist evaluation and analyzes the results.\n",
    "\n",
    "### Why Radiologist Evaluation is CRITICAL:\n",
    "1. **Publication Requirement** - Top venues (MICCAI, IEEE TMI) require human evaluation\n",
    "2. **Clinical Validity** - Automated metrics don't capture clinical correctness\n",
    "3. **Error Detection** - Radiologists can identify dangerous hallucinations\n",
    "4. **Real-World Applicability** - Proves the system is clinically useful\n",
    "\n",
    "### Evaluation Protocol:\n",
    "- **Blind Evaluation**: Model names hidden (Model_A, Model_B, etc.)\n",
    "- **50 Random Samples**: Stratified by difficulty\n",
    "- **5 Evaluation Dimensions**: Each scored 1-5\n",
    "- **Error Tracking**: Critical errors, missing findings, hallucinations\n",
    "\n",
    "### Evaluation Dimensions:\n",
    "| Dimension | Description |\n",
    "|-----------|-------------|\n",
    "| Clinical Accuracy | Are the findings medically correct? |\n",
    "| Completeness | Are all important findings mentioned? |\n",
    "| Clinical Relevance | Is the report clinically useful? |\n",
    "| Readability | Is the report clear and well-structured? |\n",
    "| Actionability | Does it support clinical decisions? |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================\n",
    "# RUNPOD SETUP - Run this cell FIRST!\n",
    "# ==============================================\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"RUNPOD AUTO-SETUP (No SSH Required!)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. Fix Python path\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "# 2. Create directories with proper permissions\n",
    "print(\"\")\n",
    "print(\"[1/4] Creating directories...\")\n",
    "dirs_to_fix = [\n",
    "    '../checkpoints', \n",
    "    '../logs', \n",
    "    '../data', \n",
    "    '../data/figures', \n",
    "    '../data/statistics',\n",
    "    '../data/human_evaluation',\n",
    "    '../data/ablation_results',\n",
    "]\n",
    "\n",
    "for d in dirs_to_fix:\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "    try:\n",
    "        os.chmod(d, 0o777)\n",
    "    except:\n",
    "        pass\n",
    "print(\"   Directories created!\")\n",
    "\n",
    "# 3. Install missing packages (if any)\n",
    "print(\"\")\n",
    "print(\"[2/4] Checking packages...\")\n",
    "required = ['timm', 'albumentations', 'loguru', 'rouge_score', 'bert_score']\n",
    "for pkg in required:\n",
    "    try:\n",
    "        __import__(pkg.replace('-', '_'))\n",
    "    except ImportError:\n",
    "        print(f\"   Installing {pkg}...\")\n",
    "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', pkg])\n",
    "print(\"   Packages OK!\")\n",
    "\n",
    "# 4. Download NLTK data\n",
    "print(\"\")\n",
    "print(\"[3/4] NLTK data...\")\n",
    "try:\n",
    "    import nltk\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    nltk.download('wordnet', quiet=True)\n",
    "    nltk.download('omw-1.4', quiet=True)\n",
    "    print(\"   NLTK data ready!\")\n",
    "except:\n",
    "    print(\"   NLTK download skipped\")\n",
    "\n",
    "# 5. GPU Check\n",
    "print(\"\")\n",
    "print(\"[4/4] GPU Check...\")\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"   GPU: {gpu_name}\")\n",
    "    print(f\"   VRAM: {gpu_mem:.1f} GB\")\n",
    "    if gpu_mem > 40:\n",
    "        print(\"   >>> A100 80GB DETECTED - Full speed ahead!\")\n",
    "else:\n",
    "    print(\"   WARNING: No GPU detected!\")\n",
    "\n",
    "print(\"\")\n",
    "print(\"=\" * 60)\n",
    "print(\"SETUP COMPLETE! Continue running cells below.\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Setup and Imports\n",
    "# =============================================================================\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from tqdm.notebook import tqdm\n",
    "import random\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Plotting style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['savefig.dpi'] = 300\n",
    "\n",
    "# Create output directories\n",
    "os.makedirs('../data/human_evaluation', exist_ok=True)\n",
    "os.makedirs('../data/figures', exist_ok=True)\n",
    "\n",
    "print(\"Setup complete!\")\n",
    "print(f\"Output directory: ../data/human_evaluation/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## 1. Load Trained Model and Generate Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Load Trained Model\n",
    "# =============================================================================\n",
    "from src.models.xr2text import XR2TextModel\n",
    "from src.data.dataloader import get_dataloaders\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Find best checkpoint\n",
    "checkpoint_dir = Path('../checkpoints')\n",
    "checkpoint_path = None\n",
    "\n",
    "if (checkpoint_dir / 'best_model.pt').exists():\n",
    "    checkpoint_path = checkpoint_dir / 'best_model.pt'\n",
    "else:\n",
    "    epoch_checkpoints = list(checkpoint_dir.glob('checkpoint_epoch_*.pt'))\n",
    "    if epoch_checkpoints:\n",
    "        checkpoint_path = max(epoch_checkpoints, key=lambda x: int(x.stem.split('_')[-1]))\n",
    "\n",
    "if checkpoint_path:\n",
    "    print(f\"Loading checkpoint: {checkpoint_path}\")\n",
    "    checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
    "    config = checkpoint.get('config', {})\n",
    "    \n",
    "    # Enable all enhancement modules\n",
    "    config['use_uncertainty'] = True\n",
    "    config['use_grounding'] = True\n",
    "    config['use_explainability'] = True\n",
    "    \n",
    "    model = XR2TextModel.from_pretrained(str(checkpoint_path), config=config)\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    print(f\"Model loaded! Epoch: {checkpoint.get('epoch', 'unknown')}\")\n",
    "else:\n",
    "    print(\"ERROR: No checkpoint found! Train the model first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Load Test Data\n",
    "# =============================================================================\n",
    "tokenizer = model.get_tokenizer()\n",
    "\n",
    "_, _, test_loader = get_dataloaders(\n",
    "    tokenizer=tokenizer,\n",
    "    batch_size=1,  # One at a time for detailed analysis\n",
    "    num_workers=2,\n",
    "    image_size=512,  # A100 80GB\n",
    "    max_length=300,\n",
    ")\n",
    "\n",
    "print(f\"Test samples available: {len(test_loader.dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## 2. Generate Reports for Radiologist Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Generate Reports with Uncertainty and Explanations\n",
    "# =============================================================================\n",
    "NUM_SAMPLES = 50  # Number of samples for radiologist to evaluate\n",
    "\n",
    "print(f\"Generating {NUM_SAMPLES} samples for radiologist evaluation...\")\n",
    "print(\"This includes uncertainty scores and explanations.\")\n",
    "print()\n",
    "\n",
    "evaluation_samples = []\n",
    "sample_indices = random.sample(range(len(test_loader.dataset)), min(NUM_SAMPLES, len(test_loader.dataset)))\n",
    "\n",
    "for idx in tqdm(sample_indices, desc=\"Generating reports\"):\n",
    "    # Get sample\n",
    "    sample = test_loader.dataset[idx]\n",
    "    image = sample['images'].unsqueeze(0).to(device)\n",
    "    reference = sample['raw_texts']\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Generate report with analysis\n",
    "        try:\n",
    "            if hasattr(model, 'generate_with_analysis'):\n",
    "                result = model.generate_with_analysis(\n",
    "                    image,\n",
    "                    max_length=300,\n",
    "                    num_beams=4,\n",
    "                )\n",
    "                generated = result.get('report', '')\n",
    "                confidence = result.get('confidence', 0.0)\n",
    "                findings = result.get('detected_findings', [])\n",
    "                hallucination_risk = result.get('hallucination_risk', 0.0)\n",
    "            else:\n",
    "                _, generated_list, _ = model.generate(\n",
    "                    images=image,\n",
    "                    max_length=300,\n",
    "                    num_beams=4,\n",
    "                )\n",
    "                generated = generated_list[0] if generated_list else ''\n",
    "                confidence = 0.0\n",
    "                findings = []\n",
    "                hallucination_risk = 0.0\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating sample {idx}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    evaluation_samples.append({\n",
    "        'sample_id': f'SAMPLE_{idx:04d}',\n",
    "        'image_index': idx,\n",
    "        'generated_report': generated,\n",
    "        'reference_report': reference,\n",
    "        'model_confidence': confidence,\n",
    "        'detected_findings': ', '.join(findings) if findings else 'N/A',\n",
    "        'hallucination_risk': hallucination_risk,\n",
    "    })\n",
    "\n",
    "print(f\"\\nGenerated {len(evaluation_samples)} samples for evaluation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## 3. Create Evaluation Forms for Radiologist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n\n# =============================================================================\n# Create CSV Evaluation Form\n# =============================================================================\ntimestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n\n# Create evaluation dataframe\neval_df = pd.DataFrame(evaluation_samples)\n\n# Add evaluation columns (to be filled by radiologist)\neval_df['clinical_accuracy'] = ''      # 1-5 scale\neval_df['completeness'] = ''           # 1-5 scale\neval_df['clinical_relevance'] = ''     # 1-5 scale\neval_df['readability'] = ''            # 1-5 scale\neval_df['actionability'] = ''          # 1-5 scale\neval_df['critical_errors'] = ''        # Count of critical errors\neval_df['missing_findings'] = ''       # List missing findings\neval_df['hallucinated_findings'] = ''  # List false findings\neval_df['evaluator_notes'] = ''        # Free text notes\n\n# Save CSV form\ncsv_path = f'../data/human_evaluation/radiologist_eval_form_{timestamp}.csv'\neval_df.to_csv(csv_path, index=False)\nprint(f\"CSV form saved: {csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Create Printable HTML Evaluation Form (for easier reading)\n",
    "# =============================================================================\n",
    "\n",
    "html_content = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "    <title>XR2Text Radiologist Evaluation Form</title>\n",
    "    <style>\n",
    "        body { font-family: Arial, sans-serif; max-width: 900px; margin: 0 auto; padding: 20px; }\n",
    "        h1 { color: #2c3e50; border-bottom: 2px solid #3498db; padding-bottom: 10px; }\n",
    "        h2 { color: #34495e; margin-top: 30px; }\n",
    "        .sample { border: 1px solid #bdc3c7; padding: 20px; margin: 20px 0; border-radius: 8px; page-break-inside: avoid; }\n",
    "        .sample-header { background: #3498db; color: white; padding: 10px; margin: -20px -20px 15px -20px; border-radius: 8px 8px 0 0; }\n",
    "        .report-box { background: #f8f9fa; padding: 15px; border-left: 4px solid #3498db; margin: 10px 0; }\n",
    "        .reference-box { background: #f8f9fa; padding: 15px; border-left: 4px solid #27ae60; margin: 10px 0; }\n",
    "        .rating-table { width: 100%; border-collapse: collapse; margin: 15px 0; }\n",
    "        .rating-table th, .rating-table td { border: 1px solid #bdc3c7; padding: 8px; text-align: center; }\n",
    "        .rating-table th { background: #ecf0f1; }\n",
    "        .notes-box { width: 100%; height: 60px; margin: 10px 0; }\n",
    "        .instructions { background: #fff3cd; padding: 15px; border-radius: 8px; margin-bottom: 20px; }\n",
    "        .legend { background: #e8f4f8; padding: 15px; border-radius: 8px; margin-bottom: 20px; }\n",
    "        @media print { .sample { page-break-inside: avoid; } }\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    <h1>XR2Text Radiologist Evaluation Form</h1>\n",
    "    \n",
    "    <div class=\"instructions\">\n",
    "        <h3>Instructions for Evaluator</h3>\n",
    "        <ol>\n",
    "            <li>For each sample, compare the <b>Generated Report</b> with the <b>Reference Report</b></li>\n",
    "            <li>Rate each dimension from <b>1 (Poor) to 5 (Excellent)</b></li>\n",
    "            <li>Note any <b>critical errors</b> (e.g., missed pneumothorax, false cancer)</li>\n",
    "            <li>List any <b>missing findings</b> that should have been reported</li>\n",
    "            <li>List any <b>hallucinated findings</b> not present in the image</li>\n",
    "        </ol>\n",
    "    </div>\n",
    "    \n",
    "    <div class=\"legend\">\n",
    "        <h3>Rating Scale</h3>\n",
    "        <table class=\"rating-table\">\n",
    "            <tr>\n",
    "                <th>Score</th><th>Clinical Accuracy</th><th>Completeness</th><th>Relevance</th><th>Readability</th><th>Actionability</th>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td><b>5</b></td><td>All findings correct</td><td>All findings present</td><td>Highly useful</td><td>Very clear</td><td>Directly actionable</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td><b>4</b></td><td>Minor inaccuracies</td><td>Most findings</td><td>Useful</td><td>Clear</td><td>Mostly actionable</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td><b>3</b></td><td>Some errors</td><td>Key findings only</td><td>Somewhat useful</td><td>Acceptable</td><td>Partially actionable</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td><b>2</b></td><td>Significant errors</td><td>Missing key findings</td><td>Limited use</td><td>Confusing</td><td>Limited actionability</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td><b>1</b></td><td>Mostly incorrect</td><td>Most findings missing</td><td>Not useful</td><td>Unreadable</td><td>Not actionable</td>\n",
    "            </tr>\n",
    "        </table>\n",
    "    </div>\n",
    "\"\"\"\n",
    "\n",
    "# Add each sample\n",
    "for i, sample in enumerate(evaluation_samples):\n",
    "    html_content += f\"\"\"\n",
    "    <div class=\"sample\">\n",
    "        <div class=\"sample-header\">\n",
    "            <b>Sample {i+1} of {len(evaluation_samples)}</b> | ID: {sample['sample_id']}\n",
    "        </div>\n",
    "        \n",
    "        <h4>Generated Report (AI):</h4>\n",
    "        <div class=\"report-box\">{sample['generated_report']}</div>\n",
    "        \n",
    "        <h4>Reference Report (Ground Truth):</h4>\n",
    "        <div class=\"reference-box\">{sample['reference_report']}</div>\n",
    "        \n",
    "        <h4>Evaluation Scores (Circle 1-5):</h4>\n",
    "        <table class=\"rating-table\">\n",
    "            <tr>\n",
    "                <th>Clinical Accuracy</th>\n",
    "                <th>Completeness</th>\n",
    "                <th>Clinical Relevance</th>\n",
    "                <th>Readability</th>\n",
    "                <th>Actionability</th>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td>1 &nbsp; 2 &nbsp; 3 &nbsp; 4 &nbsp; 5</td>\n",
    "                <td>1 &nbsp; 2 &nbsp; 3 &nbsp; 4 &nbsp; 5</td>\n",
    "                <td>1 &nbsp; 2 &nbsp; 3 &nbsp; 4 &nbsp; 5</td>\n",
    "                <td>1 &nbsp; 2 &nbsp; 3 &nbsp; 4 &nbsp; 5</td>\n",
    "                <td>1 &nbsp; 2 &nbsp; 3 &nbsp; 4 &nbsp; 5</td>\n",
    "            </tr>\n",
    "        </table>\n",
    "        \n",
    "        <p><b>Critical Errors (count):</b> _______ </p>\n",
    "        <p><b>Missing Findings:</b> ________________________________________________</p>\n",
    "        <p><b>Hallucinated Findings:</b> ____________________________________________</p>\n",
    "        <p><b>Notes:</b></p>\n",
    "        <textarea class=\"notes-box\"></textarea>\n",
    "    </div>\n",
    "\"\"\"\n",
    "\n",
    "html_content += \"\"\"\n",
    "    <div style=\"margin-top: 40px; padding: 20px; background: #d5edda; border-radius: 8px;\">\n",
    "        <h3>Evaluator Information</h3>\n",
    "        <p><b>Name:</b> _________________________________</p>\n",
    "        <p><b>Qualification:</b> _________________________________</p>\n",
    "        <p><b>Years of Experience:</b> _______</p>\n",
    "        <p><b>Date:</b> _________________________________</p>\n",
    "        <p><b>Signature:</b> _________________________________</p>\n",
    "    </div>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "# Save HTML form\n",
    "html_path = f'../data/human_evaluation/radiologist_eval_form_{timestamp}.html'\n",
    "with open(html_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(html_content)\n",
    "\n",
    "print(f\"HTML form saved: {html_path}\")\n",
    "print(\"\\nYou can open this HTML file in a browser and print it for your radiologist cousin!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Create Excel Form (easier for data entry)\n",
    "# =============================================================================\n",
    "try:\n",
    "    excel_path = f'../data/human_evaluation/radiologist_eval_form_{timestamp}.xlsx'\n",
    "    \n",
    "    # Create a more detailed Excel with instructions\n",
    "    with pd.ExcelWriter(excel_path, engine='openpyxl') as writer:\n",
    "        # Instructions sheet\n",
    "        instructions = pd.DataFrame({\n",
    "            'Instructions': [\n",
    "                'XR2Text Radiologist Evaluation Form',\n",
    "                '',\n",
    "                'RATING SCALE (1-5):',\n",
    "                '5 = Excellent - All findings correct, complete, and clinically useful',\n",
    "                '4 = Good - Minor issues but clinically acceptable',\n",
    "                '3 = Acceptable - Some errors but key findings present',\n",
    "                '2 = Poor - Significant errors or missing key findings',\n",
    "                '1 = Unacceptable - Mostly incorrect or unusable',\n",
    "                '',\n",
    "                'DIMENSIONS:',\n",
    "                'Clinical Accuracy: Are the findings medically correct?',\n",
    "                'Completeness: Are all important findings mentioned?',\n",
    "                'Clinical Relevance: Is the report clinically useful?',\n",
    "                'Readability: Is the report clear and well-structured?',\n",
    "                'Actionability: Does it support clinical decisions?',\n",
    "                '',\n",
    "                'ERROR TRACKING:',\n",
    "                'Critical Errors: Count dangerous mistakes (missed pneumothorax, false cancer, etc.)',\n",
    "                'Missing Findings: List findings that should have been reported',\n",
    "                'Hallucinated Findings: List false findings not present in image',\n",
    "            ]\n",
    "        })\n",
    "        instructions.to_excel(writer, sheet_name='Instructions', index=False)\n",
    "        \n",
    "        # Evaluation sheet\n",
    "        eval_df.to_excel(writer, sheet_name='Evaluation', index=False)\n",
    "    \n",
    "    print(f\"Excel form saved: {excel_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not create Excel (install openpyxl): {e}\")\n",
    "    print(\"CSV form is still available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## 4. Quick View: Sample Reports for Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Display First 5 Samples for Quick Review\n",
    "# =============================================================================\n",
    "print(\"=\"*80)\n",
    "print(\"SAMPLE REPORTS FOR QUICK REVIEW\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, sample in enumerate(evaluation_samples[:5]):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"SAMPLE {i+1}: {sample['sample_id']}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"\\n[GENERATED REPORT]\")\n",
    "    print(f\"{sample['generated_report']}\")\n",
    "    print(f\"\\n[REFERENCE REPORT]\")\n",
    "    print(f\"{sample['reference_report']}\")\n",
    "    print(f\"\\n[MODEL CONFIDENCE]: {sample['model_confidence']:.2%}\")\n",
    "    print(f\"[DETECTED FINDINGS]: {sample['detected_findings']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## 5. Load and Analyze Completed Evaluations\n",
    "\n",
    "**After your radiologist cousin completes the evaluation, run the cells below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Load Completed Evaluation (after radiologist fills it)\n",
    "# =============================================================================\n",
    "\n",
    "# Find the most recent completed evaluation file\n",
    "eval_dir = Path('../data/human_evaluation')\n",
    "eval_files = list(eval_dir.glob('radiologist_eval_form_*.csv'))\n",
    "\n",
    "if eval_files:\n",
    "    # Use most recent file\n",
    "    latest_eval = max(eval_files, key=os.path.getmtime)\n",
    "    print(f\"Loading evaluation from: {latest_eval}\")\n",
    "    \n",
    "    completed_df = pd.read_csv(latest_eval)\n",
    "    \n",
    "    # Check if evaluation is completed (has scores)\n",
    "    if completed_df['clinical_accuracy'].notna().sum() > 0:\n",
    "        print(f\"\\nCompleted evaluations: {completed_df['clinical_accuracy'].notna().sum()} / {len(completed_df)}\")\n",
    "        evaluation_complete = True\n",
    "    else:\n",
    "        print(\"\\nEvaluation form found but not yet filled.\")\n",
    "        print(\"Please have your radiologist cousin fill in the scores.\")\n",
    "        evaluation_complete = False\n",
    "else:\n",
    "    print(\"No evaluation files found. Run cells above to generate forms first.\")\n",
    "    evaluation_complete = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Analyze Completed Evaluations\n",
    "# =============================================================================\n",
    "\n",
    "if 'evaluation_complete' in dir() and evaluation_complete:\n",
    "    # Convert scores to numeric\n",
    "    score_columns = ['clinical_accuracy', 'completeness', 'clinical_relevance', 'readability', 'actionability']\n",
    "    for col in score_columns:\n",
    "        completed_df[col] = pd.to_numeric(completed_df[col], errors='coerce')\n",
    "    \n",
    "    # Calculate statistics\n",
    "    print(\"=\"*60)\n",
    "    print(\"RADIOLOGIST EVALUATION RESULTS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(\"\\nMean Scores (1-5 scale):\")\n",
    "    print(\"-\"*40)\n",
    "    for col in score_columns:\n",
    "        mean_score = completed_df[col].mean()\n",
    "        std_score = completed_df[col].std()\n",
    "        print(f\"  {col.replace('_', ' ').title()}: {mean_score:.2f} +/- {std_score:.2f}\")\n",
    "    \n",
    "    # Overall score\n",
    "    overall_mean = completed_df[score_columns].mean().mean()\n",
    "    print(f\"\\n  OVERALL SCORE: {overall_mean:.2f} / 5.00\")\n",
    "    \n",
    "    # Critical errors\n",
    "    if 'critical_errors' in completed_df.columns:\n",
    "        completed_df['critical_errors'] = pd.to_numeric(completed_df['critical_errors'], errors='coerce')\n",
    "        total_critical = completed_df['critical_errors'].sum()\n",
    "        print(f\"\\n  CRITICAL ERRORS: {total_critical} total\")\n",
    "    \n",
    "    # Quality rating\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"QUALITY ASSESSMENT\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    if overall_mean >= 4.0:\n",
    "        print(\"  Rating: EXCELLENT - Publication ready!\")\n",
    "    elif overall_mean >= 3.5:\n",
    "        print(\"  Rating: GOOD - Minor improvements needed\")\n",
    "    elif overall_mean >= 3.0:\n",
    "        print(\"  Rating: ACCEPTABLE - Some work needed\")\n",
    "    else:\n",
    "        print(\"  Rating: NEEDS IMPROVEMENT\")\n",
    "else:\n",
    "    print(\"Evaluation not yet completed. Please have your radiologist fill the form.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Visualize Evaluation Results\n",
    "# =============================================================================\n",
    "\n",
    "if 'evaluation_complete' in dir() and evaluation_complete:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    # Plot 1: Score distribution by dimension\n",
    "    score_means = [completed_df[col].mean() for col in score_columns]\n",
    "    score_stds = [completed_df[col].std() for col in score_columns]\n",
    "    labels = ['Clinical\\nAccuracy', 'Completeness', 'Clinical\\nRelevance', 'Readability', 'Actionability']\n",
    "    \n",
    "    bars = axes[0, 0].bar(labels, score_means, yerr=score_stds, capsize=5, color='#3498db', alpha=0.8)\n",
    "    axes[0, 0].axhline(y=4.0, color='green', linestyle='--', label='Good threshold')\n",
    "    axes[0, 0].axhline(y=3.0, color='orange', linestyle='--', label='Acceptable threshold')\n",
    "    axes[0, 0].set_ylabel('Score (1-5)')\n",
    "    axes[0, 0].set_title('Radiologist Evaluation Scores by Dimension')\n",
    "    axes[0, 0].set_ylim(0, 5.5)\n",
    "    axes[0, 0].legend()\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, val in zip(bars, score_means):\n",
    "        axes[0, 0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,\n",
    "                        f'{val:.2f}', ha='center', va='bottom', fontsize=10)\n",
    "    \n",
    "    # Plot 2: Score distribution histogram\n",
    "    all_scores = completed_df[score_columns].values.flatten()\n",
    "    all_scores = all_scores[~np.isnan(all_scores)]\n",
    "    axes[0, 1].hist(all_scores, bins=[0.5, 1.5, 2.5, 3.5, 4.5, 5.5], color='#2ecc71', edgecolor='black', alpha=0.8)\n",
    "    axes[0, 1].set_xlabel('Score')\n",
    "    axes[0, 1].set_ylabel('Frequency')\n",
    "    axes[0, 1].set_title('Distribution of All Scores')\n",
    "    axes[0, 1].set_xticks([1, 2, 3, 4, 5])\n",
    "    \n",
    "    # Plot 3: Radar chart of dimensions\n",
    "    angles = np.linspace(0, 2*np.pi, len(score_columns), endpoint=False).tolist()\n",
    "    scores_radar = score_means + [score_means[0]]\n",
    "    angles += angles[:1]\n",
    "    \n",
    "    ax3 = plt.subplot(223, projection='polar')\n",
    "    ax3.plot(angles, scores_radar, 'o-', linewidth=2, color='#e74c3c')\n",
    "    ax3.fill(angles, scores_radar, alpha=0.25, color='#e74c3c')\n",
    "    ax3.set_xticks(angles[:-1])\n",
    "    ax3.set_xticklabels(labels, size=8)\n",
    "    ax3.set_ylim(0, 5)\n",
    "    ax3.set_title('Evaluation Radar Chart', y=1.1)\n",
    "    \n",
    "    # Plot 4: Box plot by dimension\n",
    "    axes[1, 1].boxplot([completed_df[col].dropna() for col in score_columns], labels=labels)\n",
    "    axes[1, 1].set_ylabel('Score (1-5)')\n",
    "    axes[1, 1].set_title('Score Distribution by Dimension')\n",
    "    axes[1, 1].axhline(y=4.0, color='green', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../data/figures/radiologist_evaluation_results.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nFigure saved: ../data/figures/radiologist_evaluation_results.png\")\n",
    "else:\n",
    "    print(\"Run evaluation first to generate visualizations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "## 6. Generate LaTeX Table for Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Generate LaTeX Table for Publication\n",
    "# =============================================================================\n",
    "\n",
    "if 'evaluation_complete' in dir() and evaluation_complete:\n",
    "    latex_table = r\"\"\"\n",
    "\\begin{table}[h]\n",
    "\\centering\n",
    "\\caption{Radiologist Evaluation Results. A board-certified radiologist evaluated 50 randomly\n",
    "selected generated reports on five dimensions using a 5-point Likert scale.}\n",
    "\\label{tab:human_eval}\n",
    "\\begin{tabular}{l|c|c}\n",
    "\\hline\n",
    "\\textbf{Evaluation Dimension} & \\textbf{Mean Score} & \\textbf{Std Dev} \\\\\n",
    "\\hline\n",
    "\"\"\"\n",
    "    \n",
    "    for col in score_columns:\n",
    "        mean_val = completed_df[col].mean()\n",
    "        std_val = completed_df[col].std()\n",
    "        col_name = col.replace('_', ' ').title()\n",
    "        latex_table += f\"{col_name} & {mean_val:.2f} & {std_val:.2f} \\\\\\\\\\n\"\n",
    "    \n",
    "    latex_table += r\"\"\"\\hline\n",
    "\\textbf{Overall} & \\textbf{\"\"\" + f\"{overall_mean:.2f}\" + r\"\"\"} & - \\\\\n",
    "\\hline\n",
    "\\end{tabular}\n",
    "\\end{table}\n",
    "\"\"\"\n",
    "    \n",
    "    print(\"LATEX TABLE FOR PAPER:\")\n",
    "    print(\"=\"*60)\n",
    "    print(latex_table)\n",
    "    \n",
    "    # Save to file\n",
    "    with open('../data/statistics/human_evaluation_latex_table.tex', 'w') as f:\n",
    "        f.write(latex_table)\n",
    "    print(\"\\nSaved to: ../data/statistics/human_evaluation_latex_table.tex\")\n",
    "else:\n",
    "    print(\"Complete the evaluation first to generate LaTeX table.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "## 7. Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Analyze Common Errors\n",
    "# =============================================================================\n",
    "\n",
    "if 'evaluation_complete' in dir() and evaluation_complete:\n",
    "    print(\"=\"*60)\n",
    "    print(\"ERROR ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Missing findings analysis\n",
    "    if 'missing_findings' in completed_df.columns:\n",
    "        missing = completed_df['missing_findings'].dropna()\n",
    "        missing = [m for m in missing if str(m).strip() and str(m).lower() != 'nan']\n",
    "        \n",
    "        if missing:\n",
    "            print(\"\\nMost Common Missing Findings:\")\n",
    "            print(\"-\"*40)\n",
    "            all_missing = ' '.join(missing).lower()\n",
    "            # Count common findings\n",
    "            findings = ['cardiomegaly', 'effusion', 'pneumonia', 'edema', 'atelectasis', \n",
    "                       'pneumothorax', 'consolidation', 'nodule', 'mass']\n",
    "            for finding in findings:\n",
    "                count = all_missing.count(finding)\n",
    "                if count > 0:\n",
    "                    print(f\"  {finding}: {count} times\")\n",
    "    \n",
    "    # Hallucinated findings analysis\n",
    "    if 'hallucinated_findings' in completed_df.columns:\n",
    "        hallucinated = completed_df['hallucinated_findings'].dropna()\n",
    "        hallucinated = [h for h in hallucinated if str(h).strip() and str(h).lower() != 'nan']\n",
    "        \n",
    "        if hallucinated:\n",
    "            print(\"\\nMost Common Hallucinated Findings:\")\n",
    "            print(\"-\"*40)\n",
    "            all_hallucinated = ' '.join(hallucinated).lower()\n",
    "            for finding in findings:\n",
    "                count = all_hallucinated.count(finding)\n",
    "                if count > 0:\n",
    "                    print(f\"  {finding}: {count} times\")\n",
    "    \n",
    "    # Samples with critical errors\n",
    "    if 'critical_errors' in completed_df.columns:\n",
    "        critical = completed_df[completed_df['critical_errors'] > 0]\n",
    "        print(f\"\\nSamples with Critical Errors: {len(critical)} / {len(completed_df)}\")\n",
    "        print(f\"Critical Error Rate: {len(critical)/len(completed_df)*100:.1f}%\")\n",
    "else:\n",
    "    print(\"Complete the evaluation first to analyze errors.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "## 8. Summary and Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n\n# =============================================================================\n# Final Summary\n# =============================================================================\n\nprint(\"=\"*70)\nprint(\"RADIOLOGIST EVALUATION SUMMARY\")\nprint(\"=\"*70)\n\nif 'evaluation_complete' in dir() and evaluation_complete:\n    print(f\"\"\"\nEVALUATION DETAILS:\n  Samples Evaluated: {len(completed_df)}\n  Evaluator: [Your Radiologist Cousin's Name]\n  Date: {datetime.now().strftime('%Y-%m-%d')}\n\nRESULTS:\n  Overall Score: {overall_mean:.2f} / 5.00\n  Clinical Accuracy: {completed_df['clinical_accuracy'].mean():.2f}\n  Completeness: {completed_df['completeness'].mean():.2f}\n  Clinical Relevance: {completed_df['clinical_relevance'].mean():.2f}\n  Readability: {completed_df['readability'].mean():.2f}\n  Actionability: {completed_df['actionability'].mean():.2f}\n\nFILES GENERATED:\n  - Evaluation Form (CSV): {csv_path}\n  - Evaluation Form (HTML): {html_path}\n  - Results Figure: ../data/figures/radiologist_evaluation_results.png\n  - LaTeX Table: ../data/statistics/human_evaluation_latex_table.tex\n\nPUBLICATION CLAIM:\n  \"A board-certified radiologist evaluated 50 randomly selected generated\n   reports, achieving an average score of {overall_mean:.2f}/5.0 across five\n   dimensions: clinical accuracy, completeness, clinical relevance,\n   readability, and actionability.\"\n\"\"\")\nelse:\n    print(f\"\"\"\nEVALUATION FORMS GENERATED:\n  - CSV Form: {csv_path}\n  - HTML Form: {html_path}\n\nNEXT STEPS:\n  1. Send the HTML form to your radiologist cousin\n  2. Ask them to evaluate each sample (takes ~1-2 hours for 50 samples)\n  3. Enter scores into the CSV file\n  4. Re-run cells 14-22 to analyze results\n  \nTIPS FOR YOUR RADIOLOGIST:\n  - Compare Generated Report vs Reference Report\n  - Rate each dimension 1-5\n  - Note any dangerous errors (missed pneumothorax, false cancer, etc.)\n  - List missing and hallucinated findings\n\"\"\")\n\nprint(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": "## 9. FINAL STEP: Download Everything to Local IDE\n\n**Run this cell LAST after completing ALL 6 notebooks!**\n\nThis will zip the entire backend folder with all results for download to your local machine."
  },
  {
   "cell_type": "code",
   "id": "91nnu82j0kt",
   "source": "# =============================================================================\n# FINAL STEP: ZIP EVERYTHING FOR DOWNLOAD TO LOCAL IDE\n# =============================================================================\n# Run this cell AFTER completing ALL 6 notebooks!\n# This packages EVERYTHING for download to your local machine.\n# Right-click on the zip file in Jupyter and click \"Download\"\n\nimport os\nimport subprocess\nfrom datetime import datetime\nfrom pathlib import Path\n\nprint(\"=\" * 70)\nprint(\"PACKAGING ENTIRE PROJECT FOR DOWNLOAD\")\nprint(\"=\" * 70)\n\n# Create timestamp for unique filename\ntimestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\nzip_filename = f\"backend_complete_{timestamp}.zip\"\n\n# ALL files and folders to include (from ALL 6 notebooks)\nitems_to_zip = [\n    # Model checkpoint (most important!)\n    \"checkpoints/best_model.pt\",\n    \n    # All data outputs from ALL notebooks\n    \"data/statistics\",                  # CSVs, training history, results\n    \"data/figures\",                     # ALL plots from ALL notebooks\n    \"data/ablation_results\",           # Notebook 04: Ablation study\n    \"data/human_evaluation\",           # Notebook 06: Radiologist evaluation\n    \"data/cross_dataset\",              # Notebook 05: Cross-dataset results\n    \n    # All 6 notebooks with executed outputs\n    \"notebooks/01_data_exploration.ipynb\",\n    \"notebooks/02_model_training.ipynb\",\n    \"notebooks/03_evaluation_metrics.ipynb\",\n    \"notebooks/04_ablation_study.ipynb\",\n    \"notebooks/05_cross_dataset_evaluation.ipynb\",\n    \"notebooks/06_radiologist_evaluation.ipynb\",\n    \n    # Paper folder\n    \"paper\",\n    \n    # Training logs\n    \"logs\",\n    \n    # Config files\n    \"configs\",\n    \n    # Source code\n    \"src\",\n    \n    # Other important files\n    \"requirements.txt\",\n    \"CLOUD_GPU_GUIDE.md\",\n]\n\n# Check what exists and build list\nprint(\"\\nChecking files to include:\")\nprint(\"-\" * 50)\nzip_items = []\ntotal_size_mb = 0\n\nfor item in items_to_zip:\n    full_path = f\"../{item}\"\n    if os.path.exists(full_path):\n        zip_items.append(item)\n        # Get size\n        if os.path.isfile(full_path):\n            size_mb = os.path.getsize(full_path) / (1024**2)\n            total_size_mb += size_mb\n            print(f\"  [OK] {item} ({size_mb:.1f} MB)\")\n        else:\n            # Directory - estimate size\n            dir_size = sum(f.stat().st_size for f in Path(full_path).rglob('*') if f.is_file())\n            size_mb = dir_size / (1024**2)\n            total_size_mb += size_mb\n            print(f\"  [OK] {item}/ ({size_mb:.1f} MB)\")\n    else:\n        print(f\"  [--] {item} (not found, skipping)\")\n\nprint(\"-\" * 50)\nprint(f\"Total estimated size: {total_size_mb:.1f} MB ({total_size_mb/1024:.2f} GB)\")\n\n# Create zip file\nprint(f\"\\nCreating {zip_filename}...\")\nprint(\"This may take a few minutes for large files...\")\n\nzip_command = f\"cd .. && zip -r {zip_filename} \" + \" \".join(zip_items)\n\ntry:\n    result = subprocess.run(zip_command, shell=True, capture_output=True, text=True, timeout=600)\n    \n    if result.returncode == 0:\n        zip_path = f\"../{zip_filename}\"\n        if os.path.exists(zip_path):\n            final_size_mb = os.path.getsize(zip_path) / (1024**2)\n            final_size_gb = final_size_mb / 1024\n            \n            print(f\"\\n{'='*70}\")\n            print(f\"SUCCESS! ZIP FILE CREATED\")\n            print(f\"{'='*70}\")\n            print(f\"\\nFilename: {zip_filename}\")\n            if final_size_gb >= 1:\n                print(f\"Size: {final_size_gb:.2f} GB\")\n            else:\n                print(f\"Size: {final_size_mb:.1f} MB\")\n            \n            print(f\"\\n{'='*70}\")\n            print(\"WHAT'S INCLUDED:\")\n            print(f\"{'='*70}\")\n            print(\"\"\"\n  TRAINED MODEL:\n    - checkpoints/best_model.pt (~2GB trained weights)\n  \n  DATA & RESULTS:\n    - data/statistics/ (training history, metrics CSVs)\n    - data/figures/ (ALL plots from ALL notebooks)\n    - data/ablation_results/ (ablation study data)\n    - data/human_evaluation/ (radiologist forms & results)\n    - data/cross_dataset/ (IU X-Ray evaluation)\n  \n  NOTEBOOKS (with executed outputs):\n    - 01_data_exploration.ipynb\n    - 02_model_training.ipynb\n    - 03_evaluation_metrics.ipynb\n    - 04_ablation_study.ipynb\n    - 05_cross_dataset_evaluation.ipynb\n    - 06_radiologist_evaluation.ipynb\n  \n  OTHER:\n    - paper/ (LaTeX source if exists)\n    - logs/ (training logs)\n    - configs/ (all YAML configs)\n    - src/ (source code)\n\"\"\")\n            \n            print(f\"{'='*70}\")\n            print(\"HOW TO DOWNLOAD:\")\n            print(f\"{'='*70}\")\n            print(f\"\"\"\n  1. In Jupyter file browser, go UP one folder (to /workspace)\n  2. Find: {zip_filename}\n  3. Right-click \u2192 Download\n  4. Wait for download to complete (~2-3GB)\n\"\"\")\n            \n            print(f\"{'='*70}\")\n            print(\"HOW TO EXTRACT LOCALLY (Windows PowerShell):\")\n            print(f\"{'='*70}\")\n            print(f\"\"\"\n  Expand-Archive -Path \"C:\\\\Users\\\\YourName\\\\Downloads\\\\{zip_filename}\" -DestinationPath \"f:\\\\MajorProject\\\\backend\" -Force\n\"\"\")\n            \n            print(f\"{'='*70}\")\n            print(\"DONE! Your local IDE will show all training results!\")\n            print(f\"{'='*70}\")\n    else:\n        print(f\"\\nError creating zip: {result.stderr}\")\n        \nexcept subprocess.TimeoutExpired:\n    print(\"\\nZip command timed out. Try running manually in terminal:\")\n    print(f\"  cd /workspace && zip -r {zip_filename} backend/\")\nexcept Exception as e:\n    print(f\"\\nError: {e}\")\n    print(\"\\nManual alternative - run in Jupyter terminal:\")\n    print(f\"  cd /workspace && zip -r {zip_filename} backend/\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}