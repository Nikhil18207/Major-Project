{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# XR2Text: Model Training with HAQT-ARR\n\n## CLOUD GPU VERSION - Optimized for RunPod A100 PCIe 80GB VRAM\n\n**Authors**: S. Nikhil, Dadhania Omkumar  \n**Supervisor**: Dr. Damodar Panigrahy\n\n---\n\nThis notebook implements the complete training pipeline for XR2Text:\n\n### Architecture (NOVEL CONTRIBUTIONS):\n1. **HAQT-ARR** - Hierarchical Anatomical Query Tokens with Adaptive Region Routing\n2. **Uncertainty Quantification** - MC Dropout + Temperature Calibration\n3. **Factual Grounding** - Knowledge Graph + Hallucination Detection\n4. **Multi-Task Learning** - Region/Severity/Finding Classification\n\n### Training Configuration (A100 80GB Optimized):\n- **FULL DATASET**: 30,633 images (21,443 train / 3,063 val / 6,127 test)\n- **BioBART-Large** decoder (406M params)\n- **Image Size**: 512x512 (high resolution for detail)\n- **Batch Size**: 28 - A100 has superior memory bandwidth!\n- **Gradient Accumulation**: 2 steps (effective batch = 56)\n- **R-Drop Regularization**: ENABLED for +1-2% metrics\n- **All Encoder Layers Unfrozen** - Full fine-tuning\n- **Curriculum Learning**: 5 stages over 50 epochs\n\n### Expected Results (Full Dataset):\n| Metric | Target | Published SOTA |\n|--------|--------|----------------|\n| BLEU-4 | 0.15+ | 0.128 (ORGAN, ACL 2023) |\n| ROUGE-L | 0.35+ | 0.293 (ORGAN, ACL 2023) |\n| Clinical F1 | 0.80-0.85 | Novel metric |\n\n**Note**: A100 has ~3x better FP16 performance than consumer GPUs - training will be FAST!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ==============================================\n# RUNPOD SETUP - Run this cell FIRST!\n# ==============================================\nimport os\nimport sys\nimport subprocess\n\nprint(\"=\" * 60)\nprint(\"RUNPOD AUTO-SETUP (No SSH Required!)\")\nprint(\"=\" * 60)\n\n# 1. Fix Python path\nsys.path.insert(0, '..')\n\n# 2. Create directories with proper permissions\nprint(\"\")\nprint(\"[1/4] Creating directories...\")\ndirs_to_fix = [\n    '../checkpoints', \n    '../logs', \n    '../data', \n    '../data/figures', \n    '../data/statistics',\n    '../data/human_evaluation',\n    '../data/ablation_results',\n]\n\nfor d in dirs_to_fix:\n    os.makedirs(d, exist_ok=True)\n    try:\n        os.chmod(d, 0o777)\n    except:\n        pass\nprint(\"   Directories created!\")\n\n# 3. Install missing packages (if any)\nprint(\"\")\nprint(\"[2/4] Checking packages...\")\nrequired = ['timm', 'albumentations', 'loguru', 'rouge_score', 'bert_score']\nfor pkg in required:\n    try:\n        __import__(pkg.replace('-', '_'))\n    except ImportError:\n        print(f\"   Installing {pkg}...\")\n        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', pkg])\nprint(\"   Packages OK!\")\n\n# 4. Download NLTK data\nprint(\"\")\nprint(\"[3/4] NLTK data...\")\ntry:\n    import nltk\n    nltk.download('punkt', quiet=True)\n    nltk.download('wordnet', quiet=True)\n    nltk.download('omw-1.4', quiet=True)\n    print(\"   NLTK data ready!\")\nexcept:\n    print(\"   NLTK download skipped\")\n\n# 5. GPU Check\nprint(\"\")\nprint(\"[4/4] GPU Check...\")\nimport torch\nif torch.cuda.is_available():\n    gpu_name = torch.cuda.get_device_name(0)\n    gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n    print(f\"   GPU: {gpu_name}\")\n    print(f\"   VRAM: {gpu_mem:.1f} GB\")\n    if \"A100\" in gpu_name:\n        print(\"   >>> A100 DETECTED - BEST FOR DEEP LEARNING!\")\n        print(\"   >>> Superior tensor cores & memory bandwidth!\")\n    elif gpu_mem > 90:\n        print(\"   >>> RTX 6000 Pro DETECTED (96GB)\")\n    elif gpu_mem > 40:\n        print(\"   >>> A40 DETECTED\")\nelse:\n    print(\"   WARNING: No GPU detected!\")\n\nprint(\"\")\nprint(\"=\" * 60)\nprint(\"SETUP COMPLETE! Continue running cells below.\")\nprint(\"=\" * 60)"
  },
  {
   "cell_type": "code",
   "source": "# ============================================\n# VERIFY TRAINER CHECKPOINT CONFIGURATION\n# Run this cell AFTER RunPod Setup, BEFORE Training\n# Saves ONLY best_model.pt at end of training\n# ============================================\n\ntrainer_path = '../src/training/trainer.py'\n\nwith open(trainer_path, 'r') as f:\n    content = f.read()\n\n# ============================================\n# VERIFY: Check if all patches are correctly applied\n# ============================================\nchecks_passed = 0\ntotal_checks = 3\n\n# CHECK 1: best_model_state initialization\nif \"self.best_model_state = None\" in content:\n    print(\"✓ Check 1 PASSED: best_model_state initialization present\")\n    checks_passed += 1\nelse:\n    print(\"✗ Check 1 FAILED: best_model_state initialization missing\")\n\n# CHECK 2: Store best model in memory (not saving to disk during training)\nif \"Store best model state in memory (NO DISK SAVE during training)\" in content:\n    print(\"✓ Check 2 PASSED: Best model stored in memory (no disk save during training)\")\n    checks_passed += 1\nelse:\n    print(\"✗ Check 2 FAILED: Best model memory storage not configured\")\n\n# CHECK 3: Save best model only at the very end\nif \"Training complete - NOW save the best model to disk\" in content:\n    print(\"✓ Check 3 PASSED: Best model saved only at training end\")\n    checks_passed += 1\nelse:\n    print(\"✗ Check 3 FAILED: End-of-training save not configured\")\n\nprint(\"\")\nprint(\"=\" * 60)\nif checks_passed == total_checks:\n    print(f\"✅ TRAINER CORRECTLY CONFIGURED! ({checks_passed}/{total_checks} checks passed)\")\nelse:\n    print(f\"⚠️ TRAINER MAY NEED UPDATES ({checks_passed}/{total_checks} checks passed)\")\n    print(\"   Please check trainer.py manually or re-download from repo\")\nprint(\"=\" * 60)\nprint(\"\")\nprint(\"CHECKPOINT STRATEGY:\")\nprint(\"  - Epoch 1-49:  NO checkpoints saved (training in memory)\")\nprint(\"  - Epoch 50:    best_model.pt (final best by BLEU-4 + ROUGE-L)\")\nprint(\"\")\nprint(\"Best model selection: highest BLEU-4 + ROUGE-L combined score\")\nprint(\"=\" * 60)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# Training Configuration with HAQT-ARR + ALL NOVEL FEATURES\n# OPTIMIZED FOR RUNPOD A100 PCIe 80GB VRAM - SAFE + FAST\n# Target: BLEU-4 > 0.15, ROUGE-L > 0.35 (competitive with SOTA)\n# =============================================================================\n# OPTIMIZATIONS APPLIED:\n# 1. Fixed curriculum learning criteria (was too restrictive)\n# 2. Added min_lr_ratio to prevent LR dropping to 0\n# 3. Batch size 48 (SAFE - leaves 25GB buffer for memory spikes)\n# 4. No gradient accumulation (not needed)\n# 5. Num workers: 12 (match vCPU count)\n# 6. Persistent workers for faster epoch transitions\n# 7. FIXED: Plain cosine scheduler (no restarts) for curriculum stability\n# 8. FIXED: R-Drop disabled (causes OOM with batch=48)\n# =============================================================================\nfrom datetime import datetime\nimport torch\n\nconfig = {\n    # Model - High resolution for A100\n    'image_size': 512,\n    'encoder_name': 'base',                    # Swin-Base (88M params)\n    'decoder_name': 'biobart-large',           # BioBART-Large (406M params)\n    'use_anatomical_attention': True,          # Enable HAQT-ARR (Novel)\n    \n    # HAQT-ARR specific parameters (NOVEL) - DOUBLED for better performance\n    'num_regions': 7,\n    'num_global_queries': 16,\n    'num_region_queries': 8,\n    'use_spatial_priors': True,\n    'use_adaptive_routing': True,\n    'use_cross_region': True,\n    \n    # Enhancement Modules (10/10 Novelty)\n    'use_uncertainty': True,\n    'use_grounding': True,\n    'use_explainability': True,\n    'use_multitask': True,\n    \n    # Standard parameters\n    'language_dim': 1024,\n    \n    # ==========================================================================\n    # TRAINING - A100 80GB VRAM - SAFE BATCH SIZE\n    # ==========================================================================\n    'epochs': 50,\n    'batch_size': 48,                          # SAFE: ~50-55GB, leaves 25GB buffer!\n    'gradient_accumulation_steps': 1,          # Not needed\n    \n    # LEARNING RATES - REDUCED FOR STABILITY\n    'learning_rate': 5e-5,                     # REDUCED from 1e-4\n    'encoder_lr': 1e-5,                        # REDUCED - pretrained needs lower LR\n    'decoder_lr': 5e-5,                        # REDUCED\n    'projection_lr': 1e-4,                     # Higher for new HAQT-ARR layers\n    \n    'weight_decay': 0.05,                      # INCREASED for regularization\n    'warmup_steps': 1500,                      # INCREASED for stability\n    'max_grad_norm': 0.5,                      # REDUCED for stability\n    \n    # Label smoothing - INCREASED\n    'label_smoothing': 0.15,\n    \n    # ==========================================================================\n    # SCHEDULER - FIXED: Plain cosine (no restarts) for curriculum stability\n    # ==========================================================================\n    'scheduler': 'cosine',                     # FIXED: Plain cosine, no restarts\n    'num_cycles': 1,                           # Not used for plain cosine\n    'min_lr_ratio': 0.1,                       # CRITICAL: Don't let LR drop below 10%\n    \n    # ==========================================================================\n    # NOVEL LOSS FUNCTIONS - MINIMAL WEIGHTS TO FOCUS ON MAIN TASK\n    # ==========================================================================\n    'use_novel_losses': True,\n    'use_anatomical_consistency_loss': True,\n    'use_clinical_entity_loss': False,\n    'use_region_focal_loss': True,\n    'use_cross_modal_loss': False,\n    \n    # MINIMAL auxiliary loss weights\n    'anatomical_loss_weight': 0.005,\n    'clinical_loss_weight': 0.0,\n    'focal_loss_weight': 0.005,\n    'alignment_loss_weight': 0.0,\n    \n    # R-Drop - DISABLED (causes OOM with batch=48)\n    'use_rdrop': False,                        # FIXED: Disabled - causes 2x VRAM\n    'rdrop_alpha': 0.1,                        # Only used if use_rdrop: true\n    \n    # CURRICULUM LEARNING - FIXED CRITERIA\n    'use_curriculum_learning': True,\n    'curriculum_stages': [\n        {'name': 'warmup', 'epoch_start': 0, 'epoch_end': 5,\n         'criteria': {'max_findings': 1, 'max_regions': 2}},  # FIXED: removed severity:normal\n        {'name': 'easy', 'epoch_start': 5, 'epoch_end': 12,\n         'criteria': {'max_findings': 2, 'max_regions': 3}},\n        {'name': 'medium', 'epoch_start': 12, 'epoch_end': 25,\n         'criteria': {'max_findings': 4, 'max_regions': 5}},\n        {'name': 'hard', 'epoch_start': 25, 'epoch_end': 40,\n         'criteria': {}},\n        {'name': 'finetune', 'epoch_start': 40, 'epoch_end': 50,\n         'criteria': {}},\n    ],\n    \n    # Clinical Validation\n    'use_clinical_validation': True,\n    \n    # Uncertainty Quantification\n    'use_uncertainty_training': True,\n    'uncertainty_dropout': 0.1,\n    'mc_samples': 5,\n    'use_calibration': True,\n    \n    # Multi-Task Learning - REDUCED weights\n    'use_multi_task_learning': True,\n    'auxiliary_task_weights': {\n        'region_classification': 0.02,\n        'severity_prediction': 0.02,\n        'finding_detection': 0.05,\n        'length_prediction': 0.01,\n    },\n    \n    # Factual Grounding - REDUCED\n    'use_factual_grounding': True,\n    'grounding_loss_weight': 0.02,\n    'grounding_threshold': 0.15,\n    \n    # OOD Detection\n    'use_ood_detection': True,\n    'ood_threshold': 0.5,\n    \n    # Scheduled Sampling - MORE TEACHER FORCING\n    'use_scheduled_sampling': True,\n    'scheduled_sampling_start': 1.0,\n    'scheduled_sampling_end': 0.8,             # INCREASED - more teacher forcing\n    'scheduled_sampling_warmup': 15,           # INCREASED - longer warmup\n    \n    # Region regularization\n    'use_region_regularization': True,\n    'region_regularization_weight': 0.001,\n    \n    # EMA (Exponential Moving Average) - for stable training\n    'use_ema': True,\n    'ema_decay': 0.9999,\n    \n    # Data - A100 80GB OPTIMIZED\n    'max_length': 300,\n    'num_workers': 12,                         # Matches 12 vCPU\n    'pin_memory': True,\n    'prefetch_factor': 4,\n    'persistent_workers': True,                # Faster epoch transitions\n    \n    # Device\n    'use_amp': True,\n    'gradient_checkpointing': False,           # NOT NEEDED with 80GB VRAM\n    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n    \n    # Experiment\n    'experiment_name': 'xr2text_a100_80gb_' + datetime.now().strftime(\"%Y%m%d_%H%M%S\"),\n    'checkpoint_dir': '../checkpoints',\n    'validate_every': 1,\n    'save_every': 999,\n    'patience': 30,                            # INCREASED\n    'log_dir': '../logs',\n    \n    # Validation\n    'val_fraction': 0.5,\n    \n    # Generation parameters\n    'generation': {\n        'num_beams': 5,\n        'val_num_beams': 3,\n        'min_length': 30,\n        'max_length': 300,\n        'length_penalty': 1.2,\n        'repetition_penalty': 1.2,\n        'no_repeat_ngram_size': 3,\n        'early_stopping': True,\n    },\n    \n    # Error Recovery\n    'cublas_retry_enabled': True,\n    'cublas_max_retries': 3,\n    'cublas_retry_delay': 3,\n    'clear_cache_every_steps': 500,\n    'max_oom_retries': 3,\n    'enable_temp_monitoring': False,\n}\n\n# Create directories\nos.makedirs(config['checkpoint_dir'], exist_ok=True)\nos.makedirs(config['log_dir'], exist_ok=True)\nos.makedirs('../data/figures', exist_ok=True)\nos.makedirs('../data/statistics', exist_ok=True)\n\nprint(\"=\" * 70)\nprint(\"XR2Text Training Config - A100 PCIe 80GB VRAM (FIXED)\")\nprint(\"=\" * 70)\nprint(\"\")\nprint(\"MEMORY SAFE SETTINGS:\")\nprint(f\"  Batch size: {config['batch_size']} (~50-55GB VRAM usage)\")\nprint(f\"  Buffer: ~25GB FREE for memory spikes!\")\nprint(f\"  Gradient checkpointing: OFF (not needed)\")\nprint(f\"  R-Drop: DISABLED (prevents OOM)\")\nprint(\"\")\nprint(\"SCHEDULER FIX:\")\nprint(f\"  Scheduler: {config['scheduler']} (no restarts - stable with curriculum)\")\nprint(f\"  min_lr_ratio: {config['min_lr_ratio']} (LR won't drop below 10%)\")\nprint(\"\")\nprint(\"CONVERGENCE FIXES:\")\nprint(\"  1. Fixed curriculum criteria (removed severity:normal)\")\nprint(\"  2. Plain cosine scheduler (no restarts during curriculum)\")\nprint(\"  3. EMA enabled for stable weights\")\nprint(\"\")\nprint(\">>> STABLE TRAINING - No divergence at curriculum transitions!\")\nprint(\"=\" * 70)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Model and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from src.models.xr2text import XR2TextModel, DEFAULT_CONFIG\nfrom src.models.anatomical_attention import ANATOMICAL_REGIONS\nfrom src.data.dataloader import get_dataloaders\nfrom src.utils.device import setup_cuda_optimizations\n\n# Setup CUDA optimizations for A100\nsetup_cuda_optimizations()\n\n# Create model with HAQT-ARR + ALL ENHANCEMENT MODULES (10/10 Novelty)\nprint(\"Creating XR2Text model with HAQT-ARR + Enhancement Modules...\")\nmodel_config = {\n    'image_size': config['image_size'],                            # 512\n    'use_anatomical_attention': config['use_anatomical_attention'],  # Enable HAQT-ARR\n    'gradient_checkpointing': False,                                 # NOT NEEDED on 80GB\n    \n    # Enhancement Modules (10/10 Novelty)\n    'use_uncertainty': config.get('use_uncertainty', True),\n    'use_grounding': config.get('use_grounding', True),\n    'use_explainability': config.get('use_explainability', True),\n    'use_multitask': config.get('use_multitask', True),\n    \n    'encoder': {\n        'model_name': config['encoder_name'],\n        'pretrained': True,\n        'freeze_layers': 0,  # UNFREEZE ALL LAYERS - A100 has massive compute!\n        'output_dim': 1024,\n        'drop_rate': 0.1,\n        'attn_drop_rate': 0.1,\n    },\n    'projection': {\n        # HAQT-ARR parameters (Novel) - DOUBLED for better performance\n        'language_dim': config['language_dim'],\n        'num_regions': config['num_regions'],\n        'num_global_queries': config['num_global_queries'],          # 16 (doubled)\n        'num_region_queries': config['num_region_queries'],          # 8 (doubled)\n        'use_spatial_priors': config['use_spatial_priors'],\n        'use_adaptive_routing': config['use_adaptive_routing'],\n        'use_cross_region': config['use_cross_region'],\n        'num_cross_region_layers': 3,\n        'feature_size': 16,                                          # 512/32 = 16x16\n        'dropout': 0.1,\n        'num_projection_layers': 3,\n        'num_queries': 64,                                           # Doubled from 32\n        'use_cross_attention': True,\n        'use_residual': True,\n    },\n    'decoder': {\n        'model_name': config['decoder_name'],\n        'max_length': config['max_length'],                          # 300\n        'freeze_embeddings': False,\n        'freeze_layers': 0,\n        'use_cache': True,\n        'dropout': 0.1,\n    }\n}\n\nmodel = XR2TextModel.from_config(model_config)\nmodel = model.to(config['device'])\n\n# Model summary\ntotal_params = sum(p.numel() for p in model.parameters())\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n\nprint(f\"\\n{'='*60}\")\nprint(\"XR2Text Model - A100 PCIe 80GB FULL TRAINING MODE\")\nprint(f\"{'='*60}\")\nprint(f\"Total parameters: {total_params:,}\")\nprint(f\"Trainable parameters: {trainable_params:,}\")\nprint(f\"Frozen parameters: {total_params - trainable_params:,}\")\nprint(f\"\\nAnatomical regions: {model.get_anatomical_regions()}\")\nprint(f\"Total queries: {config['num_global_queries'] + config['num_regions'] * config['num_region_queries']}\")\nprint(f\"\\nEnhancement Modules Enabled:\")\nprint(f\"  - Uncertainty Quantification: {config.get('use_uncertainty', True)}\")\nprint(f\"  - Factual Grounding: {config.get('use_grounding', True)}\")\nprint(f\"  - Explainability: {config.get('use_explainability', True)}\")\nprint(f\"  - Multi-Task Learning: {config.get('use_multitask', True)}\")\nprint(f\"\\nA100 Optimizations:\")\nprint(f\"  - Image Size: 512x512\")\nprint(f\"  - All encoder layers unfrozen: YES\")\nprint(f\"  - Gradient checkpointing: OFF (not needed with 80GB)\")\nprint(f\"  - R-Drop regularization: {config.get('use_rdrop', True)}\")\nprint(f\"  - Num queries: 64 (doubled from 32)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load data with A100 80GB SAFE settings\nprint(\"\\nLoading datasets with A100 80GB VRAM SAFE settings...\")\ntokenizer = model.get_tokenizer()\n\ntrain_loader, val_loader, test_loader = get_dataloaders(\n    tokenizer=tokenizer,\n    batch_size=config['batch_size'],          # 48 - SAFE with 25GB buffer\n    num_workers=config['num_workers'],        # 12 workers (matches 12 vCPUs)\n    image_size=config['image_size'],          # 512 for high resolution\n    max_length=config['max_length'],          # 300 for longer reports\n    train_subset=None,                        # Use full dataset\n    pin_memory=config.get('pin_memory', True),\n    prefetch_factor=config.get('prefetch_factor', 4),\n    persistent_workers=config.get('persistent_workers', True),\n)\n\nprint(f\"\\n{'='*70}\")\nprint(\"DATALOADER CONFIGURATION - A100 PCIe 80GB VRAM (SAFE)\")\nprint(f\"{'='*70}\")\nprint(f\"Train samples: {len(train_loader.dataset)}\")\nprint(f\"Val samples: {len(val_loader.dataset)}\")\nprint(f\"Test samples: {len(test_loader.dataset)}\")\nprint(f\"\\nMEMORY SAFE SETTINGS:\")\nprint(f\"  Batch size: {config['batch_size']} (~50-55GB VRAM)\")\nprint(f\"  Buffer: ~25GB FREE for spikes!\")\nprint(f\"  Train batches per epoch: {len(train_loader)}\")\nprint(f\"\\nSPEED SETTINGS:\")\nprint(f\"  Num workers: {config['num_workers']} (matches 12 vCPU)\")\nprint(f\"  Persistent workers: {config.get('persistent_workers', True)}\")\nprint(f\"  Prefetch factor: {config.get('prefetch_factor', 4)}\")\nprint(f\"  Pin memory: {config.get('pin_memory', True)}\")\nprint(f\"\\nImage size: {config['image_size']}x{config['image_size']}\")\nprint(f\"Max length: {config['max_length']}\")\nprint(f\"\\nSteps per epoch: {len(train_loader)}\")\nprint(f\"Total steps (50 epochs): {len(train_loader) * 50}\")\nprint(f\"\\n>>> NO OOM ERRORS EXPECTED - 25GB buffer maintained!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from torch.optim import AdamW\nfrom torch.cuda.amp import GradScaler, autocast\nfrom src.training.scheduler import get_cosine_with_hard_restarts_schedule_with_warmup\nfrom src.utils.metrics import compute_metrics\n\n# NOVEL: Import novel training components\nfrom src.training.losses import CombinedNovelLoss\nfrom src.training.curriculum import AnatomicalCurriculumScheduler, create_curriculum_dataloader\nfrom src.utils.clinical_validator import ClinicalValidator\n\n# Optimizer with proper weight decay\nno_decay = ['bias', 'LayerNorm.weight', 'layer_norm.weight']\noptimizer_grouped_parameters = [\n    {\n        'params': [p for n, p in model.named_parameters() \n                   if p.requires_grad and not any(nd in n for nd in no_decay)],\n        'weight_decay': config['weight_decay'],\n    },\n    {\n        'params': [p for n, p in model.named_parameters() \n                   if p.requires_grad and any(nd in n for nd in no_decay)],\n        'weight_decay': 0.0,\n    },\n]\n\noptimizer = AdamW(optimizer_grouped_parameters, lr=config['learning_rate'])\n\n# Scheduler - Cosine with Restarts (3 cycles for better convergence)\ntotal_steps = len(train_loader) * config['epochs'] // config['gradient_accumulation_steps']\nscheduler = get_cosine_with_hard_restarts_schedule_with_warmup(\n    optimizer,\n    num_warmup_steps=config['warmup_steps'],\n    num_training_steps=total_steps,\n    num_cycles=3,  # 3 restarts for better convergence\n)\n\n# Mixed precision scaler\nscaler = GradScaler() if config['use_amp'] else None\n\n# NOVEL: Initialize novel loss functions (with REDUCED weights)\nif config.get('use_novel_losses', False):\n    novel_loss = CombinedNovelLoss(\n        use_anatomical_consistency=config.get('use_anatomical_consistency_loss', True),\n        use_clinical_entity=config.get('use_clinical_entity_loss', False),  # DISABLED\n        use_region_focal=config.get('use_region_focal_loss', True),\n        use_cross_modal=config.get('use_cross_modal_loss', False),          # DISABLED\n        anatomical_weight=config.get('anatomical_loss_weight', 0.01),       # Very small\n        clinical_weight=config.get('clinical_loss_weight', 0.0),            # Disabled\n        focal_weight=config.get('focal_loss_weight', 0.01),                 # Very small\n        alignment_weight=config.get('alignment_loss_weight', 0.0),          # Disabled\n    )\n    print(\"Novel loss functions initialized (REDUCED weights for better main task focus)\")\nelse:\n    novel_loss = None\n\n# NOVEL: Initialize curriculum learning scheduler\nif config.get('use_curriculum_learning', False):\n    curriculum_scheduler = AnatomicalCurriculumScheduler()\n    print(\"Curriculum learning scheduler initialized\")\nelse:\n    curriculum_scheduler = None\n\n# NOVEL: Initialize clinical validator\nif config.get('use_clinical_validation', False):\n    clinical_validator = ClinicalValidator()\n    print(\"Clinical validator initialized\")\nelse:\n    clinical_validator = None\n\nprint(f\"\\n{'='*60}\")\nprint(\"TRAINING SETUP - RTX 6000 Pro 96GB OPTIMIZED\")\nprint(f\"{'='*60}\")\nprint(f\"Total optimization steps: {total_steps}\")\nprint(f\"Warmup steps: {config['warmup_steps']} (increased for stability)\")\nprint(f\"Scheduler: Cosine with 3 restarts\")\nprint(f\"Mixed precision (AMP): {config['use_amp']}\")\nprint(f\"\\nLearning Rates (stable for better convergence):\")\nprint(f\"  - Base: {config['learning_rate']} (reduced)\")\nprint(f\"  - Encoder: {config['encoder_lr']} (pretrained, lower LR)\")\nprint(f\"  - Decoder: {config['decoder_lr']}\")\nprint(f\"  - Projection: {config['projection_lr']} (new HAQT-ARR layers)\")\nprint(f\"\\nNovel Components:\")\nprint(f\"  - Novel losses: {config.get('use_novel_losses', False)} (reduced weights)\")\nprint(f\"  - Curriculum learning: {config.get('use_curriculum_learning', False)}\")\nprint(f\"  - Clinical validation: {config.get('use_clinical_validation', False)}\")\nprint(f\"  - R-Drop regularization: {config.get('use_rdrop', True)} (alpha={config.get('rdrop_alpha', 0.3)})\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training history\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'val_loss': [],\n",
    "    'bleu_1': [],\n",
    "    'bleu_2': [],\n",
    "    'bleu_3': [],\n",
    "    'bleu_4': [],\n",
    "    'rouge_1': [],\n",
    "    'rouge_2': [],\n",
    "    'rouge_l': [],\n",
    "    'learning_rate': [],\n",
    "}\n",
    "\n",
    "best_metric = 0.0\n",
    "patience_counter = 0\n",
    "patience = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# MAIN TRAINING LOOP - A100 PCIe 80GB OPTIMIZED WITH AUTO-RESUME\n# =============================================================================\nfrom src.training.trainer import XR2TextTrainer\nimport torch\nimport gc\nfrom pathlib import Path\n\n# ============================================\n# FINAL PERMISSION FIX (before training)\n# ============================================\nprint(\"Ensuring all directories have write permissions...\")\ndirs_to_fix = [\n    config['checkpoint_dir'],\n    config['log_dir'],\n    '../data',\n    '../data/figures',\n    '../data/statistics'\n]\nfor d in dirs_to_fix:\n    p = Path(d)\n    p.mkdir(parents=True, exist_ok=True)\n    try:\n        os.chmod(p, 0o777)\n    except:\n        pass\nprint(\"Directories ready!\")\n\n# ============================================\n# AUTO-RESUME FROM CHECKPOINT\n# ============================================\ncheckpoint_dir = Path(config['checkpoint_dir'])\n\ndef find_best_checkpoint(checkpoint_dir):\n    \"\"\"Find the best checkpoint to resume from.\n    \n    PRIORITY ORDER:\n    1. Latest checkpoint_epoch_*.pt (resume from most recent training progress)\n    2. best_model.pt (fallback if no epoch checkpoints exist)\n    \"\"\"\n    checkpoint_dir = Path(checkpoint_dir)\n    if not checkpoint_dir.exists():\n        return None, 0\n    \n    # PRIORITY 1: Find latest epoch checkpoint\n    epoch_checkpoints = list(checkpoint_dir.glob(\"checkpoint_epoch_*.pt\"))\n    if epoch_checkpoints:\n        def get_epoch(p):\n            try:\n                return int(p.stem.split('_')[-1])\n            except:\n                return 0\n        latest = max(epoch_checkpoints, key=get_epoch)\n        # Verify checkpoint is not corrupted (should be > 1GB)\n        if latest.stat().st_size > 1e9:\n            ckpt = torch.load(latest, map_location='cpu')\n            latest_epoch = ckpt.get('epoch', get_epoch(latest))\n            print(f\"   Found epoch checkpoint: {latest.name} (epoch {latest_epoch})\")\n            return str(latest), latest_epoch + 1\n        else:\n            print(f\"   Skipping corrupted checkpoint: {latest.name}\")\n            latest.unlink()  # Remove corrupted file\n    \n    # PRIORITY 2: Fallback to best_model.pt\n    best_model = checkpoint_dir / \"best_model.pt\"\n    if best_model.exists():\n        # Verify checkpoint is not corrupted\n        if best_model.stat().st_size > 1e9:\n            ckpt = torch.load(best_model, map_location='cpu')\n            best_epoch = ckpt.get('epoch', 0)\n            print(f\"   Found best_model.pt (from epoch {best_epoch})\")\n            return str(best_model), best_epoch + 1\n        else:\n            print(f\"   Skipping corrupted best_model.pt\")\n            best_model.unlink()\n    \n    return None, 0\n\n# Auto-detect checkpoint\ncheckpoint_path, resume_epoch = find_best_checkpoint(checkpoint_dir)\n\nprint(\"=\" * 70)\nprint(\"XR2Text Training - RUNPOD A100 PCIe 80GB\")\nprint(\"=\" * 70)\n\n# Memory cleanup before training\nprint(\"\\nClearing GPU memory...\")\ngc.collect()\ntorch.cuda.empty_cache()\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"GPU Memory - Allocated: {torch.cuda.memory_allocated()/1024**3:.2f} GB\")\n    print(f\"GPU Memory - Total: {torch.cuda.get_device_properties(0).total_memory/1024**3:.1f} GB\")\n\n# Create trainer with A100 optimized config\ntrainer = XR2TextTrainer(\n    model=model,\n    train_loader=train_loader,\n    val_loader=val_loader,\n    config=config,\n)\n\n# AUTO-RESUME: Load checkpoint if found\nif checkpoint_path:\n    print(f\"\\n>>> CHECKPOINT FOUND: {checkpoint_path}\")\n    print(f\">>> Resuming from epoch {resume_epoch}\")\n    trainer.load_checkpoint(checkpoint_path)\nelse:\n    print(\"\\n>>> No checkpoint found. Starting fresh training from epoch 1\")\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"A100 PCIe 80GB TRAINING CONFIGURATION:\")\nprint(f\"  Batch size: {config['batch_size']} (28x RTX 4060!)\")\nprint(f\"  Gradient accumulation: {config['gradient_accumulation_steps']}\")\nprint(f\"  Effective batch: {config['batch_size'] * config['gradient_accumulation_steps']}\")\nprint(f\"  Image size: {config['image_size']}x{config['image_size']}\")\nprint(f\"  Learning rate: {config['learning_rate']} (stable)\")\nprint(f\"  R-Drop: {config.get('use_rdrop', True)} (alpha={config.get('rdrop_alpha', 0.3)})\")\nprint(f\"  Validate every: {config.get('validate_every', 1)} epochs\")\nprint(f\"  Val fraction: {config.get('val_fraction', 0.5)} (50% for accuracy)\")\nprint(\"=\" * 70 + \"\\n\")\n\n# Run training\nfinal_metrics = trainer.train()\n\n# Extract history from trainer for visualization\nhistory = {\n    'train_loss': trainer.metrics_tracker.get_history('train_loss'),\n    'val_loss': trainer.metrics_tracker.get_history('val_loss'),\n    'bleu_1': trainer.metrics_tracker.get_history('bleu_1'),\n    'bleu_2': trainer.metrics_tracker.get_history('bleu_2'),\n    'bleu_3': trainer.metrics_tracker.get_history('bleu_3'),\n    'bleu_4': trainer.metrics_tracker.get_history('bleu_4'),\n    'rouge_1': trainer.metrics_tracker.get_history('rouge_1'),\n    'rouge_2': trainer.metrics_tracker.get_history('rouge_2'),\n    'rouge_l': trainer.metrics_tracker.get_history('rouge_l'),\n    'learning_rate': [trainer.scheduler.get_last_lr()[0]] * (trainer.current_epoch + 1),\n}\n\n# Add clinical validation metrics if enabled\nif config.get('use_clinical_validation', False):\n    history['clinical_accuracy'] = trainer.metrics_tracker.get_history('clinical_accuracy')\n    history['clinical_f1'] = trainer.metrics_tracker.get_history('clinical_f1')\n    history['critical_errors'] = trainer.metrics_tracker.get_history('critical_errors')\n\n# Save training history\nhistory_df = pd.DataFrame(history)\nhistory_df['epoch'] = range(1, len(history_df) + 1)\nos.makedirs('../data/statistics', exist_ok=True)\nhistory_df.to_csv('../data/statistics/training_history.csv', index=False)\n\n# Store predictions and references for sample display\npredictions = []\nreferences = []\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"TRAINING COMPLETE!\")\nprint(\"=\" * 70)\nprint(f\"\\nFinal Metrics:\")\nfor key, value in final_metrics.items():\n    if isinstance(value, float):\n        print(f\"  {key}: {value:.4f}\")\n    else:\n        print(f\"  {key}: {value}\")\n\n# Final memory cleanup\ngc.collect()\ntorch.cuda.empty_cache()\nprint(f\"\\nFinal GPU Memory - Allocated: {torch.cuda.memory_allocated()/1024**3:.2f} GB\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROBUST FIX - handles any array length mismatch\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Get all the metrics\n",
    "val_loss = trainer.metrics_tracker.get_history('val_loss')\n",
    "bleu_1 = trainer.metrics_tracker.get_history('bleu_1')\n",
    "bleu_2 = trainer.metrics_tracker.get_history('bleu_2')\n",
    "bleu_3 = trainer.metrics_tracker.get_history('bleu_3')\n",
    "bleu_4 = trainer.metrics_tracker.get_history('bleu_4')\n",
    "rouge_1 = trainer.metrics_tracker.get_history('rouge_1')\n",
    "rouge_2 = trainer.metrics_tracker.get_history('rouge_2')\n",
    "rouge_l = trainer.metrics_tracker.get_history('rouge_l')\n",
    "train_loss = trainer.metrics_tracker.get_history('train_loss')\n",
    "\n",
    "# Debug: Print lengths\n",
    "print(\"Array lengths:\")\n",
    "print(f\"  val_loss: {len(val_loss)}\")\n",
    "print(f\"  bleu_4: {len(bleu_4)}\")\n",
    "print(f\"  rouge_l: {len(rouge_l)}\")\n",
    "print(f\"  train_loss: {len(train_loss)}\")\n",
    "\n",
    "# Find minimum length among validation metrics\n",
    "min_len = min(len(val_loss), len(bleu_4), len(rouge_l))\n",
    "print(f\"\\nUsing {min_len} epochs\")\n",
    "\n",
    "# Create DataFrame with matching lengths\n",
    "history_df = pd.DataFrame({\n",
    "    'epoch': list(range(2, 2 + min_len * 2, 2))[:min_len],\n",
    "    'val_loss': val_loss[:min_len],\n",
    "    'bleu_1': bleu_1[:min_len],\n",
    "    'bleu_2': bleu_2[:min_len],\n",
    "    'bleu_3': bleu_3[:min_len],\n",
    "    'bleu_4': bleu_4[:min_len],\n",
    "    'rouge_1': rouge_1[:min_len],\n",
    "    'rouge_2': rouge_2[:min_len],\n",
    "    'rouge_l': rouge_l[:min_len],\n",
    "})\n",
    "\n",
    "# Add train_loss if available (sample every 2nd)\n",
    "if train_loss:\n",
    "    sampled_train = train_loss[1::2][:min_len]\n",
    "    if len(sampled_train) == min_len:\n",
    "        history_df['train_loss'] = sampled_train\n",
    "\n",
    "# Save\n",
    "os.makedirs('../data/statistics', exist_ok=True)\n",
    "history_df.to_csv('../data/statistics/training_history.csv', index=False)\n",
    "\n",
    "print(f\"\\n✅ Saved {len(history_df)} epochs!\")\n",
    "print(\"\\nLast 5 rows:\")\n",
    "print(history_df.tail())\n",
    "print(f\"\\nBest BLEU-4: {history_df['bleu_4'].max():.4f}\")\n",
    "print(f\"Best ROUGE-L: {history_df['rouge_l'].max():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Curves Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 NOVEL: Enhanced Curriculum Learning Analysis\n",
    "\n",
    "This section provides detailed analysis of our curriculum learning strategy,\n",
    "showing how it affects training dynamics and final performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================\n# NOVEL: ENHANCED CURRICULUM LEARNING ANALYSIS (5 STAGES, 50 EPOCHS)\n# RTX 6000 Pro 96GB - Gentler Progression for Better Metrics\n# ============================================\nfrom src.training.curriculum import AnatomicalCurriculumScheduler\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nprint(\"=\" * 80)\nprint(\"NOVEL: CURRICULUM LEARNING ANALYSIS (5 STAGES, 50 EPOCHS)\")\nprint(\"RTX 6000 Pro 96GB - Optimized for Better BLEU/ROUGE\")\nprint(\"=\" * 80)\n\n# Initialize curriculum scheduler\ncurriculum = AnatomicalCurriculumScheduler()\n\n# Display curriculum stages (RTX 6000 Pro - Gentler Progression)\nprint(\"\\n1. CURRICULUM STAGES (5-Stage Progressive Training)\")\nprint(\"-\" * 60)\nprint(f\"\\n{'Stage':<20} {'Epochs':<15} {'Description':<40}\")\nprint(\"-\" * 80)\n\n# RTX 6000 Pro optimized stages\nstage_descriptions = {\n    'warmup': 'Warmup with easy cases only (epochs 0-5)',\n    'easy': 'Normal X-rays, simple findings (epochs 5-12)',\n    'medium': 'Single anatomical region findings (epochs 12-25)',\n    'hard': 'Multiple regions, moderate complexity (epochs 25-40)',\n    'finetune': 'Full dataset fine-tuning (epochs 40-50)',\n}\n\nrtx6000_stages = [\n    {'name': 'warmup', 'epoch_start': 0, 'epoch_end': 5},\n    {'name': 'easy', 'epoch_start': 5, 'epoch_end': 12},\n    {'name': 'medium', 'epoch_start': 12, 'epoch_end': 25},\n    {'name': 'hard', 'epoch_start': 25, 'epoch_end': 40},\n    {'name': 'finetune', 'epoch_start': 40, 'epoch_end': 50},\n]\n\nfor stage in rtx6000_stages:\n    name = stage['name']\n    epoch_range = f\"{stage['epoch_start']}-{stage['epoch_end']}\"\n    desc = stage_descriptions.get(name, 'Full dataset')\n    print(f\"{name:<20} {epoch_range:<15} {desc:<40}\")\n\n# Sample difficulty scoring demo\nprint(\"\\n2. SAMPLE DIFFICULTY SCORING\")\nprint(\"-\" * 60)\n\nsample_reports = [\n    \"Lungs are clear. Heart size is normal. No acute cardiopulmonary process.\",\n    \"Mild cardiomegaly. Lungs are clear bilaterally.\",\n    \"Bilateral pleural effusions. Cardiomegaly. Pulmonary edema.\",\n    \"Large right pneumothorax. Left lung consolidation. Cardiomegaly.\",\n]\n\nprint(\"\\nSample Reports with Difficulty Scores:\")\nfor i, report in enumerate(sample_reports):\n    scores = curriculum.difficulty_scorer(report)\n    total_difficulty = scores.get('num_findings', 0) + scores.get('severity_score', 0)\n    print(f\"\\n[Sample {i+1}] Difficulty: {total_difficulty:.1f}\")\n    print(f\"   Report: {report[:60]}...\")\n    print(f\"   Findings: {scores.get('num_findings', 0)}, Regions: {scores.get('num_regions', 0)}\")\n\n# Load and analyze training history\nprint(\"\\n3. CURRICULUM LEARNING IMPACT\")\nprint(\"-\" * 60)\n\nhistory_path = '../data/statistics/training_history.csv'\nif os.path.exists(history_path):\n    print(\"\\n\" + \"=\" * 60)\n    print(\"CURRICULUM LEARNING RESULTS (Real Data)\")\n    print(\"=\" * 60)\n\n    df = pd.read_csv(history_path)\n\n    print(\"\\nPerformance at Curriculum Stage Transitions:\")\n    print(\"-\" * 60)\n\n    # RTX 6000 Pro 5-stage curriculum: warmup(0-5), easy(5-12), medium(12-25), hard(25-40), finetune(40-50)\n    stage_info = [\n        (5, 'End of Stage 1 (Warmup)'),\n        (12, 'End of Stage 2 (Easy Cases)'),\n        (25, 'End of Stage 3 (Medium Cases)'),\n        (40, 'End of Stage 4 (Hard Cases)'),\n        (50, 'End of Stage 5 (Fine-tuning)'),\n    ]\n\n    for target_epoch, stage_name in stage_info:\n        mask = df['epoch'] <= target_epoch\n        if mask.any():\n            row = df[mask].iloc[-1]\n            print(f\"\\nEpoch {int(row['epoch'])} - {stage_name}:\")\n            print(f\"  BLEU-4:  {row['bleu_4']:.4f}\")\n            print(f\"  ROUGE-L: {row['rouge_l']:.4f}\")\n            print(f\"  Val Loss: {row['val_loss']:.4f}\")\n\n    # Plot curriculum impact\n    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n    # RTX 6000 Pro stage transitions at epochs 5, 12, 25, 40\n    stage_transitions = [5, 12, 25, 40]\n\n    # BLEU-4 progression with stage markers\n    axes[0].plot(df['epoch'], df['bleu_4'], linewidth=2, color='blue', marker='o', markersize=3)\n    for trans in stage_transitions:\n        axes[0].axvline(x=trans, color='red', linestyle='--', alpha=0.7)\n    axes[0].set_xlabel('Epoch')\n    axes[0].set_ylabel('BLEU-4')\n    axes[0].set_title('BLEU-4 Progression with 5-Stage Curriculum (RTX 6000 Pro)')\n    axes[0].grid(True, alpha=0.3)\n\n    # Loss progression with stage markers\n    axes[1].plot(df['epoch'], df['val_loss'], linewidth=2, color='orange', marker='o', markersize=3)\n    for trans in stage_transitions:\n        axes[1].axvline(x=trans, color='red', linestyle='--', alpha=0.7)\n    axes[1].set_xlabel('Epoch')\n    axes[1].set_ylabel('Validation Loss')\n    axes[1].set_title('Loss Progression with 5-Stage Curriculum (RTX 6000 Pro)')\n    axes[1].grid(True, alpha=0.3)\n\n    plt.tight_layout()\n    os.makedirs('../data/figures', exist_ok=True)\n    plt.savefig('../data/figures/curriculum_impact.png', dpi=300, bbox_inches='tight')\n    plt.show()\n\n    print(\"\\n\" + \"=\" * 60)\n    print(\"Curriculum learning analysis complete!\")\n    print(\"Figure saved: ../data/figures/curriculum_impact.png\")\n    print(\"=\" * 60)\nelse:\n    print(\"\\nTraining history not found yet.\")\n    print(\"Run this cell again after training completes.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# FIXED: TRAINING CURVES VISUALIZATION\n",
    "# ============================================\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load history from CSV\n",
    "history_path = \"../data/statistics/training_history.csv\"\n",
    "\n",
    "if os.path.exists(history_path):\n",
    "    print(\"Loading training history from CSV...\")\n",
    "    df = pd.read_csv(history_path)\n",
    "    print(f\"Loaded {len(df)} epochs of data\")\n",
    "\n",
    "    # Check if we have data\n",
    "    if len(df) > 0 and 'bleu_4' in df.columns:\n",
    "        # Create figure with subplots\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "        axes = axes.flatten()\n",
    "\n",
    "        # Plot 1: Validation Loss\n",
    "        axes[0].plot(df['epoch'], df['val_loss'], label='Val Loss', color='orange', linewidth=2, marker='o', markersize=4)\n",
    "        axes[0].set_xlabel('Epoch')\n",
    "        axes[0].set_ylabel('Loss')\n",
    "        axes[0].set_title('Validation Loss')\n",
    "        axes[0].legend()\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "        # Plot 2: BLEU Scores\n",
    "        axes[1].plot(df['epoch'], df['bleu_1'], label='BLEU-1', linewidth=2)\n",
    "        axes[1].plot(df['epoch'], df['bleu_2'], label='BLEU-2', linewidth=2)\n",
    "        axes[1].plot(df['epoch'], df['bleu_3'], label='BLEU-3', linewidth=2)\n",
    "        axes[1].plot(df['epoch'], df['bleu_4'], label='BLEU-4', linewidth=2, marker='o', markersize=4)\n",
    "        axes[1].set_xlabel('Epoch')\n",
    "        axes[1].set_ylabel('Score')\n",
    "        axes[1].set_title('BLEU Scores')\n",
    "        axes[1].legend()\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "        # Plot 3: ROUGE Scores\n",
    "        axes[2].plot(df['epoch'], df['rouge_1'], label='ROUGE-1', linewidth=2)\n",
    "        axes[2].plot(df['epoch'], df['rouge_2'], label='ROUGE-2', linewidth=2)\n",
    "        axes[2].plot(df['epoch'], df['rouge_l'], label='ROUGE-L', linewidth=2, marker='o', markersize=4)\n",
    "        axes[2].set_xlabel('Epoch')\n",
    "        axes[2].set_ylabel('Score')\n",
    "        axes[2].set_title('ROUGE Scores')\n",
    "        axes[2].legend()\n",
    "        axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "        # Plot 4: Combined BLEU-4 and ROUGE-L\n",
    "        axes[3].plot(df['epoch'], df['bleu_4'], label='BLEU-4', linewidth=2, color='blue', marker='o', markersize=4)\n",
    "        axes[3].plot(df['epoch'], df['rouge_l'], label='ROUGE-L', linewidth=2, color='green', marker='s', markersize=4)\n",
    "        axes[3].set_xlabel('Epoch')\n",
    "        axes[3].set_ylabel('Score')\n",
    "        axes[3].set_title('BLEU-4 vs ROUGE-L Comparison')\n",
    "        axes[3].legend()\n",
    "        axes[3].grid(True, alpha=0.3)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        os.makedirs('../data/figures', exist_ok=True)\n",
    "        plt.savefig('../data/figures/training_curves.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "        print(\"\\nTraining curves saved to ../data/figures/training_curves.png\")\n",
    "\n",
    "        # Print summary\n",
    "        print(\"\\n\" + \"=\" * 50)\n",
    "        print(\"TRAINING SUMMARY\")\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"Best BLEU-4:  {df['bleu_4'].max():.4f} (Epoch {df.loc[df['bleu_4'].idxmax(), 'epoch']:.0f})\")\n",
    "        print(f\"Best ROUGE-L: {df['rouge_l'].max():.4f} (Epoch {df.loc[df['rouge_l'].idxmax(), 'epoch']:.0f})\")\n",
    "        print(f\"Final Val Loss: {df['val_loss'].iloc[-1]:.4f}\")\n",
    "    else:\n",
    "        print(\"No valid data in CSV file\")\n",
    "else:\n",
    "    print(\"Training history CSV not found!\")\n",
    "    print(\"Expected at:\", history_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Sample Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sample predictions vs ground truth\n",
    "print(\"Sample Predictions vs Ground Truth:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Check if predictions and references exist\n",
    "if 'predictions' not in dir() or not predictions:\n",
    "    predictions = []\n",
    "if 'references' not in dir() or not references:\n",
    "    references = []\n",
    "\n",
    "if len(predictions) > 0 and len(references) > 0:\n",
    "    for i in range(min(5, len(predictions))):\n",
    "        print(f\"\\n--- Sample {i+1} ---\")\n",
    "        print(f\"\\nGround Truth:\")\n",
    "        print(references[i][:500] + \"...\" if len(references[i]) > 500 else references[i])\n",
    "        print(f\"\\nGenerated:\")\n",
    "        print(predictions[i][:500] + \"...\" if len(predictions[i]) > 500 else predictions[i])\n",
    "        print(\"-\" * 80)\n",
    "else:\n",
    "    print(\"\\n⚠️ No predictions available yet!\")\n",
    "    print(\"   Predictions will be available after training completes (cell 11).\")\n",
    "    print(\"   Or run evaluation on test set in notebook 03_evaluation.ipynb.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Final Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# FIXED: FINAL RESULTS SUMMARY\n",
    "# ============================================\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "history_path = \"../data/statistics/training_history.csv\"\n",
    "\n",
    "if os.path.exists(history_path):\n",
    "    df = pd.read_csv(history_path)\n",
    "\n",
    "    # Find best epoch by combined BLEU-4 + ROUGE-L score\n",
    "    df['combined_score'] = df['bleu_4'] + df['rouge_l']\n",
    "    best_idx = df['combined_score'].idxmax()\n",
    "    best_row = df.loc[best_idx]\n",
    "    final_row = df.iloc[-1]\n",
    "\n",
    "    print(\"=\" * 60)\n",
    "    print(\"TRAINING RESULTS SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # FIXED: Use actual epoch value from the dataframe\n",
    "    print(f\"\\nBest Epoch: {int(best_row['epoch'])} (by BLEU-4 + ROUGE-L)\")\n",
    "\n",
    "    print(f\"\\nBest Metrics (Epoch {int(best_row['epoch'])}):\")\n",
    "    print(f\"  BLEU-1:  {best_row['bleu_1']:.4f}\")\n",
    "    print(f\"  BLEU-2:  {best_row['bleu_2']:.4f}\")\n",
    "    print(f\"  BLEU-3:  {best_row['bleu_3']:.4f}\")\n",
    "    print(f\"  BLEU-4:  {best_row['bleu_4']:.4f}\")\n",
    "    print(f\"  ROUGE-1: {best_row['rouge_1']:.4f}\")\n",
    "    print(f\"  ROUGE-2: {best_row['rouge_2']:.4f}\")\n",
    "    print(f\"  ROUGE-L: {best_row['rouge_l']:.4f}\")\n",
    "\n",
    "    print(f\"\\nFinal Metrics (Epoch {int(final_row['epoch'])}):\")\n",
    "    print(f\"  Val Loss: {final_row['val_loss']:.4f}\")\n",
    "    print(f\"  BLEU-4:   {final_row['bleu_4']:.4f}\")\n",
    "    print(f\"  ROUGE-L:  {final_row['rouge_l']:.4f}\")\n",
    "\n",
    "    # Save best results to CSV\n",
    "    results_table = pd.DataFrame({\n",
    "        'Metric': ['BLEU-1', 'BLEU-2', 'BLEU-3', 'BLEU-4', 'ROUGE-1', 'ROUGE-2', 'ROUGE-L'],\n",
    "        'Best Score': [\n",
    "            best_row['bleu_1'],\n",
    "            best_row['bleu_2'],\n",
    "            best_row['bleu_3'],\n",
    "            best_row['bleu_4'],\n",
    "            best_row['rouge_1'],\n",
    "            best_row['rouge_2'],\n",
    "            best_row['rouge_l'],\n",
    "        ],\n",
    "        'Final Score': [\n",
    "            final_row['bleu_1'],\n",
    "            final_row['bleu_2'],\n",
    "            final_row['bleu_3'],\n",
    "            final_row['bleu_4'],\n",
    "            final_row['rouge_1'],\n",
    "            final_row['rouge_2'],\n",
    "            final_row['rouge_l'],\n",
    "        ]\n",
    "    })\n",
    "\n",
    "    os.makedirs('../data/statistics', exist_ok=True)\n",
    "    results_table.to_csv('../data/statistics/best_results.csv', index=False)\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Results saved to ../data/statistics/best_results.csv\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Display table\n",
    "    print(\"\\nResults Table:\")\n",
    "    print(results_table.to_string(index=False))\n",
    "\n",
    "else:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"TRAINING RESULTS SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"\\nNo training results available yet!\")\n",
    "    print(\"Run training first (cell 11) to see results.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 8. NOVEL: Enhanced Analysis with New Features\n",
    "\n",
    "##This section demonstrates the new enhancement modules for comprehensive report analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# NOVEL: Enhanced Analysis Demo\n",
    "# ============================================\n",
    "# This demonstrates the new enhancement modules\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"NOVEL ENHANCEMENT MODULES DEMO\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Check if model has enhancement modules\n",
    "if hasattr(model, 'generate_with_analysis'):\n",
    "    print(\"\\n✅ Model has enhanced analysis capabilities!\")\n",
    "    print(\"\\nAvailable analysis features:\")\n",
    "    print(\"  1. Uncertainty Quantification\")\n",
    "    print(\"     - Overall confidence score (0-1)\")\n",
    "    print(\"     - Per-finding confidence scores\")\n",
    "    print(\"     - Calibrated uncertainty estimates\")\n",
    "    print(\"\\n  2. Factual Grounding\")\n",
    "    print(\"     - Detected medical findings\")\n",
    "    print(\"     - Potential hallucinations flagged\")\n",
    "    print(\"     - Knowledge graph validation\")\n",
    "    print(\"\\n  3. Explainability\")\n",
    "    print(\"     - Evidence regions highlighted\")\n",
    "    print(\"     - Clinical reasoning chains\")\n",
    "    print(\"     - Attention visualizations\")\n",
    "    print(\"\\n  4. Multi-Task Outputs\")\n",
    "    print(\"     - Region classification\")\n",
    "    print(\"     - Severity prediction\")\n",
    "    print(\"     - Finding detection\")\n",
    "    \n",
    "    # Demo analysis on a sample if test data is available\n",
    "    print(\"\\n\" + \"-\" * 50)\n",
    "    print(\"Running Enhanced Analysis on Sample...\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    try:\n",
    "        # Get a sample from test loader\n",
    "        sample_batch = next(iter(test_loader))\n",
    "        sample_image = sample_batch['images'][0:1].to(config['device'])\n",
    "        \n",
    "        # Run enhanced analysis\n",
    "        with torch.no_grad():\n",
    "            analysis = model.generate_with_analysis(\n",
    "                sample_image,\n",
    "                max_length=config['generation']['max_length'],\n",
    "                num_beams=config['generation']['num_beams'],\n",
    "            )\n",
    "        \n",
    "        print(f\"\\n📝 Generated Report:\")\n",
    "        print(f\"   {analysis.get('report', 'N/A')[:200]}...\")\n",
    "        \n",
    "        print(f\"\\n📊 Uncertainty Analysis:\")\n",
    "        print(f\"   Overall Confidence: {analysis.get('confidence', 0):.2%}\")\n",
    "        if 'finding_confidences' in analysis:\n",
    "            print(f\"   Finding Confidences: {len(analysis['finding_confidences'])} findings analyzed\")\n",
    "        \n",
    "        print(f\"\\n🔍 Factual Grounding:\")\n",
    "        if 'detected_findings' in analysis:\n",
    "            print(f\"   Detected Findings: {analysis['detected_findings'][:5]}\")\n",
    "        if 'potential_hallucinations' in analysis:\n",
    "            print(f\"   Potential Hallucinations: {len(analysis.get('potential_hallucinations', []))}\")\n",
    "        \n",
    "        print(f\"\\n💡 Explainability:\")\n",
    "        if 'evidence_regions' in analysis:\n",
    "            print(f\"   Evidence Regions: {len(analysis['evidence_regions'])} regions identified\")\n",
    "        if 'reasoning' in analysis:\n",
    "            print(f\"   Clinical Reasoning: Available\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"   Demo skipped (requires trained model): {e}\")\n",
    "        \n",
    "else:\n",
    "    print(\"\\n⚠️  Enhancement modules not loaded in current model.\")\n",
    "    print(\"   Ensure use_uncertainty, use_grounding, use_explainability, use_multitask are True.\")\n",
    "    print(\"   Re-initialize model with updated config to enable these features.\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Enhanced Analysis Demo Complete\")\n",
    "print(\"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (swin)",
   "language": "python",
   "name": "swin"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}