{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XR2Text: Model Training with HAQT-ARR\n",
    "\n",
    "## IMPROVED VERSION - Optimized for RTX 4060 8GB\n",
    "\n",
    "**Authors**: S. Nikhil, Dadhania Omkumar  \n",
    "**Supervisor**: Dr. Damodar Panigrahy\n",
    "\n",
    "---\n",
    "\n",
    "This notebook implements the complete training pipeline for XR2Text:\n",
    "\n",
    "### Architecture (NOVEL CONTRIBUTIONS):\n",
    "1. **HAQT-ARR** - Hierarchical Anatomical Query Tokens with Adaptive Region Routing\n",
    "2. **Uncertainty Quantification** - MC Dropout + Temperature Calibration\n",
    "3. **Factual Grounding** - Knowledge Graph + Hallucination Detection\n",
    "4. **Multi-Task Learning** - Region/Severity/Finding Classification\n",
    "\n",
    "### Training Improvements:\n",
    "- **BioBART-Large** decoder (upgraded from base)\n",
    "- **R-Drop Regularization** for better generation\n",
    "- **Cross-Modal Alignment Loss** for vision-language grounding\n",
    "- **Extended Curriculum Learning** (5 stages, 100 epochs)\n",
    "- **Gradient Checkpointing** for RTX 4060 memory efficiency\n",
    "\n",
    "### Expected Results:\n",
    "| Metric | Target | SOTA Reference |\n",
    "|--------|--------|----------------|\n",
    "| BLEU-4 | 0.12+ | 0.142 (ChestBioX-Gen) |\n",
    "| ROUGE-L | 0.35+ | 0.312 (ChestBioX-Gen) |\n",
    "| Clinical F1 | 0.70+ | Novel metric |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\MajorProject\\swin\\lib\\site-packages\\torch\\cuda\\__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "SYSTEM CONFIGURATION\n",
      "==================================================\n",
      "CUDA Available: True\n",
      "GPU Connected: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "GPU Memory: 8.0 GB\n",
      "CUDA Version: 12.1\n",
      "PyTorch Version: 2.5.1+cu121\n",
      "\n",
      "Using Device: cuda\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# GPU/CUDA Check - Run this first!\n",
    "# ============================================\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from tqdm.notebook import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['savefig.dpi'] = 300\n",
    "\n",
    "# GPU Check\n",
    "print(\"=\" * 50)\n",
    "print(\"SYSTEM CONFIGURATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
    "    print(f\"CUDA Available: True\")\n",
    "    print(f\"GPU Connected: {gpu_name}\")\n",
    "    print(f\"GPU Memory: {gpu_memory:.1f} GB\")\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"PyTorch Version: {torch.__version__}\")\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    print(f\"CUDA Available: False\")\n",
    "    print(f\"WARNING: Running on CPU (Training will be slow)\")\n",
    "    print(f\"PyTorch Version: {torch.__version__}\")\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"\\nUsing Device: {device}\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "XR2Text Training Config - ALL NOVEL FEATURES (10/10 Novelty)\n",
      "======================================================================\n",
      "\n",
      "CORE ARCHITECTURE:\n",
      "  [x] HAQT-ARR Projection Layer\n",
      "  [x] Swin Transformer Encoder (Swin-Base)\n",
      "  [x] BioBART-Large Decoder (UPGRADED)\n",
      "\n",
      "NOVEL TRAINING STRATEGIES:\n",
      "  [x] Novel Loss Functions (Anatomical, Clinical, Focal, Cross-Modal)\n",
      "  [ ] R-Drop Regularization (DISABLED for 2x faster training)\n",
      "  [x] Clinical Validation\n",
      "  [x] Curriculum Learning (5-stage progressive, 100 epochs)\n",
      "\n",
      "NEW ENHANCEMENT MODULES:\n",
      "  [x] Uncertainty Quantification (MC Dropout, Temperature Scaling)\n",
      "  [x] Factual Grounding (Knowledge Graph, Hallucination Detection)\n",
      "  [x] Explainability (Evidence Regions, Clinical Reasoning)\n",
      "  [x] Multi-Task Learning (Region, Severity, Finding, Length)\n",
      "  [x] OOD Detection (Mahalanobis, Energy-based)\n",
      "\n",
      "MEMORY OPTIMIZATIONS (RTX 4060 8GB):\n",
      "  - batch_size: 1 (BioBART-Large needs more memory)\n",
      "  - gradient_accumulation: 32 (effective batch=32)\n",
      "  - validate_every: 2 (see BLEU/ROUGE every 2 epochs)\n",
      "  - val_fraction: 15% (fast validation)\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Training Configuration with HAQT-ARR + ALL NOVEL FEATURES (10/10 Novelty)\n",
    "# OPTIMIZED FOR SPEED + METRICS EVERY 2 EPOCHS\n",
    "# ALL NOVEL CONTRIBUTIONS ENABLED\n",
    "config = {\n",
    "    # Model\n",
    "    'image_size': 384,\n",
    "    'encoder_name': 'base',  # Swin-Base\n",
    "    'decoder_name': 'biobart-large',  # UPGRADED: BioBART-Large for better generation\n",
    "    'use_anatomical_attention': True,  # Enable HAQT-ARR (Novel)\n",
    "    \n",
    "    # HAQT-ARR specific parameters (NOVEL)\n",
    "    'num_regions': 7,\n",
    "    'num_global_queries': 8,\n",
    "    'num_region_queries': 4,\n",
    "    'use_spatial_priors': True,\n",
    "    'use_adaptive_routing': True,\n",
    "    'use_cross_region': True,\n",
    "    \n",
    "    # NEW: Enhancement Modules (10/10 Novelty)\n",
    "    'use_uncertainty': True,           # Uncertainty quantification\n",
    "    'use_grounding': True,             # Factual grounding & hallucination detection\n",
    "    'use_explainability': True,        # Explainability & evidence regions\n",
    "    'use_multitask': True,             # Multi-task learning heads\n",
    "    \n",
    "    # Standard parameters\n",
    "    'language_dim': 1024,              # UPDATED: BioBART-Large uses 1024 hidden dim\n",
    "    \n",
    "    # Training - OPTIMIZED FOR RTX 4060 8GB\n",
    "    'epochs': 100,                     # Full 100 epochs for curriculum learning\n",
    "    'batch_size': 1,                   # REDUCED: BioBART-Large needs more memory\n",
    "    'gradient_accumulation_steps': 32, # Keeps effective batch=32\n",
    "    'learning_rate': 1e-4,\n",
    "    'weight_decay': 0.01,\n",
    "    'warmup_steps': 1000,\n",
    "    'max_grad_norm': 1.0,\n",
    "    \n",
    "    # Label smoothing - for better BLEU\n",
    "    'label_smoothing': 0.05,\n",
    "    \n",
    "    # NOVEL: Novel Loss Functions - ENABLED\n",
    "    'use_novel_losses': True,\n",
    "    'use_anatomical_consistency_loss': True,\n",
    "    'use_clinical_entity_loss': True,\n",
    "    'use_region_focal_loss': True,\n",
    "    'use_cross_modal_loss': False,\n",
    "    'anatomical_loss_weight': 0.05,\n",
    "    'clinical_loss_weight': 0.1,\n",
    "    'focal_loss_weight': 0.1,\n",
    "    'alignment_loss_weight': 0.1,\n",
    "    \n",
    "    # R-Drop Regularization - DISABLED for faster training (2x speedup)\n",
    "    # Can be enabled later for final model to squeeze extra 1-2% metrics\n",
    "    'use_rdrop': False,\n",
    "    'rdrop_alpha': 0.7,\n",
    "    \n",
    "    # NOVEL: Curriculum Learning - ENABLED (5 stages over 100 epochs)\n",
    "    'use_curriculum_learning': True,\n",
    "    \n",
    "    # NOVEL: Clinical Validation - ENABLED\n",
    "    'use_clinical_validation': True,\n",
    "    \n",
    "    # NEW: Uncertainty Quantification\n",
    "    'use_uncertainty_training': True,\n",
    "    'uncertainty_dropout': 0.1,\n",
    "    'mc_samples': 10,                  # Monte Carlo dropout samples\n",
    "    'use_calibration': True,           # Temperature scaling calibration\n",
    "    \n",
    "    # NEW: Multi-Task Learning\n",
    "    'use_multi_task_learning': True,\n",
    "    'auxiliary_task_weights': {\n",
    "        'region_classification': 0.1,\n",
    "        'severity_prediction': 0.1,\n",
    "        'finding_detection': 0.15,\n",
    "        'length_prediction': 0.05,\n",
    "    },\n",
    "    \n",
    "    # NEW: Factual Grounding\n",
    "    'use_factual_grounding': True,\n",
    "    'grounding_loss_weight': 0.1,\n",
    "    'grounding_threshold': 0.15,\n",
    "    \n",
    "    # NEW: OOD Detection\n",
    "    'use_ood_detection': True,\n",
    "    'ood_threshold': 0.5,\n",
    "    \n",
    "    # Scheduled Sampling\n",
    "    'use_scheduled_sampling': True,\n",
    "    'scheduled_sampling_start': 1.0,\n",
    "    'scheduled_sampling_end': 0.6,\n",
    "    'scheduled_sampling_warmup': 10,\n",
    "    \n",
    "    # Region regularization\n",
    "    'use_region_regularization': True,\n",
    "    'region_regularization_weight': 0.005,\n",
    "    \n",
    "    # Data\n",
    "    'max_length': 256,\n",
    "    'num_workers': 4,\n",
    "    \n",
    "    # Device\n",
    "    'use_amp': True,\n",
    "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    \n",
    "    # Experiment\n",
    "    'experiment_name': f'xr2text_haqt_arr_full_novel_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}',\n",
    "    'checkpoint_dir': '../checkpoints',\n",
    "    'validate_every': 2,                # SEE SCORES EVERY 2 EPOCHS\n",
    "    'save_every': 5,\n",
    "    'patience': 999,\n",
    "    'log_dir': '../logs',\n",
    "    \n",
    "    # Validation - FAST (use less data)\n",
    "    'val_fraction': 0.15,               # Use only 15% of val data for speed\n",
    "    \n",
    "    # Generation parameters - OPTIMIZED FOR SPEED\n",
    "    'generation': {\n",
    "        'num_beams': 2,                 # FAST: reduced from 3\n",
    "        'min_length': 10,               # FAST: reduced\n",
    "        'max_length': 150,              # FAST: reduced from 200\n",
    "        'length_penalty': 1.0,\n",
    "        'repetition_penalty': 1.1,\n",
    "        'no_repeat_ngram_size': 3,\n",
    "        'early_stopping': True,\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(config['checkpoint_dir'], exist_ok=True)\n",
    "os.makedirs(config['log_dir'], exist_ok=True)\n",
    "os.makedirs('../data/figures', exist_ok=True)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"XR2Text Training Config - ALL NOVEL FEATURES (10/10 Novelty)\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nCORE ARCHITECTURE:\")\n",
    "print(\"  [x] HAQT-ARR Projection Layer\")\n",
    "print(\"  [x] Swin Transformer Encoder (Swin-Base)\")\n",
    "print(\"  [x] BioBART-Large Decoder (UPGRADED)\")\n",
    "print(\"\\nNOVEL TRAINING STRATEGIES:\")\n",
    "print(\"  [x] Novel Loss Functions (Anatomical, Clinical, Focal, Cross-Modal)\")\n",
    "print(\"  [ ] R-Drop Regularization (DISABLED for 2x faster training)\")\n",
    "print(\"  [x] Clinical Validation\")\n",
    "print(\"  [x] Curriculum Learning (5-stage progressive, 100 epochs)\")\n",
    "print(\"\\nNEW ENHANCEMENT MODULES:\")\n",
    "print(\"  [x] Uncertainty Quantification (MC Dropout, Temperature Scaling)\")\n",
    "print(\"  [x] Factual Grounding (Knowledge Graph, Hallucination Detection)\")\n",
    "print(\"  [x] Explainability (Evidence Regions, Clinical Reasoning)\")\n",
    "print(\"  [x] Multi-Task Learning (Region, Severity, Finding, Length)\")\n",
    "print(\"  [x] OOD Detection (Mahalanobis, Energy-based)\")\n",
    "print(\"\\nMEMORY OPTIMIZATIONS (RTX 4060 8GB):\")\n",
    "print(\"  - batch_size: 1 (BioBART-Large needs more memory)\")\n",
    "print(\"  - gradient_accumulation: 32 (effective batch=32)\")\n",
    "print(\"  - validate_every: 2 (see BLEU/ROUGE every 2 epochs)\")\n",
    "print(\"  - val_fraction: 15% (fast validation)\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Model and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-14 00:20:54.329\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.utils.device\u001b[0m:\u001b[36msetup_cuda_optimizations\u001b[0m:\u001b[36m86\u001b[0m - \u001b[1mEnabled cuDNN benchmark mode\u001b[0m\n",
      "\u001b[32m2026-01-14 00:20:54.329\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.utils.device\u001b[0m:\u001b[36msetup_cuda_optimizations\u001b[0m:\u001b[36m92\u001b[0m - \u001b[1mEnabled TF32 for matrix operations\u001b[0m\n",
      "\u001b[32m2026-01-14 00:20:54.330\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.utils.device\u001b[0m:\u001b[36msetup_cuda_optimizations\u001b[0m:\u001b[36m96\u001b[0m - \u001b[1mCleared CUDA cache\u001b[0m\n",
      "\u001b[32m2026-01-14 00:20:54.330\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.models.xr2text\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m109\u001b[0m - \u001b[1mBuilding Swin Transformer Encoder...\u001b[0m\n",
      "\u001b[32m2026-01-14 00:20:54.331\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.models.swin_encoder\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m81\u001b[0m - \u001b[1mInitializing Swin Encoder: swin_base_patch4_window7_224\u001b[0m\n",
      "\u001b[32m2026-01-14 00:20:54.331\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.models.swin_encoder\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m82\u001b[0m - \u001b[1mPretrained: True, Image Size: 384\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating XR2Text model with HAQT-ARR + Enhancement Modules...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-14 00:20:55.971\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.models.swin_encoder\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m96\u001b[0m - \u001b[1mSwin feature dimension: 1024\u001b[0m\n",
      "\u001b[32m2026-01-14 00:20:55.973\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.models.swin_encoder\u001b[0m:\u001b[36m_freeze_layers\u001b[0m:\u001b[36m139\u001b[0m - \u001b[1mFrozen 404,424 parameters in 2 layers\u001b[0m\n",
      "\u001b[32m2026-01-14 00:20:55.973\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.models.swin_encoder\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m120\u001b[0m - \u001b[1mSwin Encoder initialized successfully\u001b[0m\n",
      "\u001b[32m2026-01-14 00:20:55.974\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.models.xr2text\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1mBuilding HAQT-ARR (Hierarchical Anatomical) Projection Layer...\u001b[0m\n",
      "\u001b[32m2026-01-14 00:20:55.975\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.models.anatomical_attention\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m799\u001b[0m - \u001b[1mInitializing HAQT-ARR Projection Layer\u001b[0m\n",
      "\u001b[32m2026-01-14 00:20:55.976\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.models.anatomical_attention\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m800\u001b[0m - \u001b[1m  Visual dim: 1024 -> Language dim: 1024\u001b[0m\n",
      "\u001b[32m2026-01-14 00:20:55.977\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.models.anatomical_attention\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m801\u001b[0m - \u001b[1m  Regions: 7, Total queries: 36\u001b[0m\n",
      "\u001b[32m2026-01-14 00:20:55.977\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.models.anatomical_attention\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m802\u001b[0m - \u001b[1m  Spatial priors: True, Adaptive routing: True\u001b[0m\n",
      "\u001b[32m2026-01-14 00:20:55.988\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.models.anatomical_attention\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m409\u001b[0m - \u001b[1mAnatomicalQueryTokens: 8 global + 7x4 region queries\u001b[0m\n",
      "\u001b[32m2026-01-14 00:20:55.994\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.models.anatomical_attention\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m238\u001b[0m - \u001b[1mImageConditionedSpatialPriorRefiner initialized for 7 regions\u001b[0m\n",
      "\u001b[32m2026-01-14 00:20:55.995\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.models.anatomical_attention\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m846\u001b[0m - \u001b[1m  Image-conditioned prior refinement: ENABLED\u001b[0m\n",
      "\u001b[32m2026-01-14 00:20:56.443\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.models.anatomical_attention\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m901\u001b[0m - \u001b[1mHAQT-ARR Projection Layer initialized successfully\u001b[0m\n",
      "\u001b[32m2026-01-14 00:20:56.444\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.models.xr2text\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m158\u001b[0m - \u001b[1mBuilding BioBART Decoder...\u001b[0m\n",
      "\u001b[32m2026-01-14 00:20:56.444\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.models.biobart_decoder\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m90\u001b[0m - \u001b[1mInitializing decoder: GanjinZero/biobart-large\u001b[0m\n",
      "\u001b[32m2026-01-14 00:20:56.446\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.models.biobart_decoder\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m91\u001b[0m - \u001b[1mLabel smoothing: 0.0\u001b[0m\n",
      "\u001b[32m2026-01-14 00:20:56.446\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.models.biobart_decoder\u001b[0m:\u001b[36m_init_bart_decoder\u001b[0m:\u001b[36m131\u001b[0m - \u001b[1mAttempting to load decoder: GanjinZero/biobart-large\u001b[0m\n",
      "\u001b[32m2026-01-14 00:20:59.407\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.models.biobart_decoder\u001b[0m:\u001b[36m_init_bart_decoder\u001b[0m:\u001b[36m143\u001b[0m - \u001b[1mSuccessfully loaded decoder: GanjinZero/biobart-large\u001b[0m\n",
      "\u001b[32m2026-01-14 00:20:59.408\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.models.biobart_decoder\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m121\u001b[0m - \u001b[1mDecoder initialized with hidden_dim: 1024\u001b[0m\n",
      "\u001b[32m2026-01-14 00:20:59.409\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.models.xr2text\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m182\u001b[0m - \u001b[1mBuilding Uncertainty Estimator...\u001b[0m\n",
      "\u001b[32m2026-01-14 00:20:59.415\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.models.uncertainty_estimator\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m562\u001b[0m - \u001b[1mUncertaintyEstimator initialized with 10 MC samples\u001b[0m\n",
      "\u001b[32m2026-01-14 00:20:59.416\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.models.xr2text\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m194\u001b[0m - \u001b[1mBuilding Factual Grounding Module...\u001b[0m\n",
      "\u001b[32m2026-01-14 00:20:59.417\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.models.knowledge_grounding\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m90\u001b[0m - \u001b[1mMedicalKnowledgeGraph initialized with 24 findings\u001b[0m\n",
      "\u001b[32m2026-01-14 00:20:59.420\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.models.knowledge_grounding\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m672\u001b[0m - \u001b[1mFactualGroundingModule initialized with 24 findings\u001b[0m\n",
      "\u001b[32m2026-01-14 00:20:59.422\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.models.xr2text\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mBuilding Explainability Module...\u001b[0m\n",
      "\u001b[32m2026-01-14 00:20:59.424\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.models.explainability\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m374\u001b[0m - \u001b[1mExplainabilityModule initialized\u001b[0m\n",
      "\u001b[32m2026-01-14 00:20:59.425\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.models.xr2text\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m215\u001b[0m - \u001b[1mBuilding Multi-Task Learning Head...\u001b[0m\n",
      "\u001b[32m2026-01-14 00:20:59.444\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.models.auxiliary_tasks\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m316\u001b[0m - \u001b[1mMultiTaskHead initialized: regions=7, findings=20, severity_classes=4\u001b[0m\n",
      "\u001b[32m2026-01-14 00:20:59.445\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.models.xr2text\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m225\u001b[0m - \u001b[1mXR2Text Model initialized successfully!\u001b[0m\n",
      "\u001b[32m2026-01-14 00:20:59.456\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.models.xr2text\u001b[0m:\u001b[36m_log_model_info\u001b[0m:\u001b[36m264\u001b[0m - \u001b[1mTotal parameters: 541,634,767\u001b[0m\n",
      "\u001b[32m2026-01-14 00:20:59.457\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.models.xr2text\u001b[0m:\u001b[36m_log_model_info\u001b[0m:\u001b[36m265\u001b[0m - \u001b[1mTrainable parameters: 541,230,343\u001b[0m\n",
      "\u001b[32m2026-01-14 00:20:59.458\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.models.xr2text\u001b[0m:\u001b[36m_log_model_info\u001b[0m:\u001b[36m266\u001b[0m - \u001b[1mFrozen parameters: 404,424\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "XR2Text Model with HAQT-ARR + Enhancement Modules (10/10 Novelty)\n",
      "============================================================\n",
      "Total parameters: 541,634,767\n",
      "Trainable parameters: 541,230,343\n",
      "Frozen parameters: 404,424\n",
      "\n",
      "Anatomical regions: ['right_lung', 'left_lung', 'heart', 'mediastinum', 'spine', 'diaphragm', 'costophrenic_angles']\n",
      "Total queries: 36\n",
      "\n",
      "Enhancement Modules Enabled:\n",
      "  - Uncertainty Quantification: True\n",
      "  - Factual Grounding: True\n",
      "  - Explainability: True\n",
      "  - Multi-Task Learning: True\n"
     ]
    }
   ],
   "source": [
    "from src.models.xr2text import XR2TextModel, DEFAULT_CONFIG\n",
    "from src.models.anatomical_attention import ANATOMICAL_REGIONS\n",
    "from src.data.dataloader import get_dataloaders\n",
    "from src.utils.device import setup_cuda_optimizations\n",
    "\n",
    "# Setup CUDA optimizations for RTX 4060\n",
    "setup_cuda_optimizations()\n",
    "\n",
    "# Create model with HAQT-ARR + ALL ENHANCEMENT MODULES (10/10 Novelty)\n",
    "print(\"Creating XR2Text model with HAQT-ARR + Enhancement Modules...\")\n",
    "model_config = {\n",
    "    'image_size': config['image_size'],\n",
    "    'use_anatomical_attention': config['use_anatomical_attention'],  # Enable HAQT-ARR\n",
    "    \n",
    "    # NEW: Enhancement Modules (10/10 Novelty)\n",
    "    'use_uncertainty': config.get('use_uncertainty', True),\n",
    "    'use_grounding': config.get('use_grounding', True),\n",
    "    'use_explainability': config.get('use_explainability', True),\n",
    "    'use_multitask': config.get('use_multitask', True),\n",
    "    \n",
    "    'encoder': {\n",
    "        'model_name': config['encoder_name'],\n",
    "        'pretrained': True,\n",
    "        'freeze_layers': 2,  # Freeze first 2 Swin layers\n",
    "    },\n",
    "    'projection': {\n",
    "        # HAQT-ARR parameters (Novel)\n",
    "        'language_dim': config['language_dim'],\n",
    "        'num_regions': config['num_regions'],\n",
    "        'num_global_queries': config['num_global_queries'],\n",
    "        'num_region_queries': config['num_region_queries'],\n",
    "        'use_spatial_priors': config['use_spatial_priors'],\n",
    "        'use_adaptive_routing': config['use_adaptive_routing'],\n",
    "        'use_cross_region': config['use_cross_region'],\n",
    "        'feature_size': 12,  # 384/32 = 12x12 patches\n",
    "    },\n",
    "    'decoder': {\n",
    "        'model_name': config['decoder_name'],\n",
    "        'max_length': config['max_length'],\n",
    "    }\n",
    "}\n",
    "\n",
    "model = XR2TextModel.from_config(model_config)\n",
    "model = model.to(config['device'])\n",
    "\n",
    "# Model summary\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"XR2Text Model with HAQT-ARR + Enhancement Modules (10/10 Novelty)\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Frozen parameters: {total_params - trainable_params:,}\")\n",
    "print(f\"\\nAnatomical regions: {model.get_anatomical_regions()}\")\n",
    "print(f\"Total queries: {config['num_global_queries'] + config['num_regions'] * config['num_region_queries']}\")\n",
    "print(f\"\\nEnhancement Modules Enabled:\")\n",
    "print(f\"  - Uncertainty Quantification: {config.get('use_uncertainty', True)}\")\n",
    "print(f\"  - Factual Grounding: {config.get('use_grounding', True)}\")\n",
    "print(f\"  - Explainability: {config.get('use_explainability', True)}\")\n",
    "print(f\"  - Multi-Task Learning: {config.get('use_multitask', True)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-14 00:21:00.185\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.data.dataloader\u001b[0m:\u001b[36mget_dataloaders\u001b[0m:\u001b[36m47\u001b[0m - \u001b[1mCreating dataloaders...\u001b[0m\n",
      "\u001b[32m2026-01-14 00:21:00.193\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.data.dataset\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m60\u001b[0m - \u001b[1mLoading MIMIC-CXR dataset (split: train)...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-14 00:21:02.918\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.data.dataset\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m80\u001b[0m - \u001b[1mLoaded 30633 samples\u001b[0m\n",
      "\u001b[32m2026-01-14 00:21:02.919\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.data.dataset\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m60\u001b[0m - \u001b[1mLoading MIMIC-CXR dataset (split: validation)...\u001b[0m\n",
      "\u001b[32m2026-01-14 00:21:04.349\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.data.dataset\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m80\u001b[0m - \u001b[1mLoaded 3063 samples\u001b[0m\n",
      "\u001b[32m2026-01-14 00:21:04.350\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.data.dataset\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m60\u001b[0m - \u001b[1mLoading MIMIC-CXR dataset (split: test)...\u001b[0m\n",
      "\u001b[32m2026-01-14 00:21:05.778\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.data.dataset\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m80\u001b[0m - \u001b[1mLoaded 3064 samples\u001b[0m\n",
      "\u001b[32m2026-01-14 00:21:05.779\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.data.dataloader\u001b[0m:\u001b[36mget_dataloaders\u001b[0m:\u001b[36m117\u001b[0m - \u001b[1mTrain samples: 30633\u001b[0m\n",
      "\u001b[32m2026-01-14 00:21:05.780\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.data.dataloader\u001b[0m:\u001b[36mget_dataloaders\u001b[0m:\u001b[36m118\u001b[0m - \u001b[1mVal samples: 3063\u001b[0m\n",
      "\u001b[32m2026-01-14 00:21:05.782\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.data.dataloader\u001b[0m:\u001b[36mget_dataloaders\u001b[0m:\u001b[36m119\u001b[0m - \u001b[1mTest samples: 3064\u001b[0m\n",
      "\u001b[32m2026-01-14 00:21:05.783\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.data.dataloader\u001b[0m:\u001b[36mget_dataloaders\u001b[0m:\u001b[36m120\u001b[0m - \u001b[1mBatch size: 1\u001b[0m\n",
      "\u001b[32m2026-01-14 00:21:05.783\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.data.dataloader\u001b[0m:\u001b[36mget_dataloaders\u001b[0m:\u001b[36m121\u001b[0m - \u001b[1mTrain batches: 30633\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batches: 30633\n",
      "Val batches: 3063\n",
      "Test batches: 3064\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "print(\"\\nLoading datasets...\")\n",
    "tokenizer = model.get_tokenizer()\n",
    "\n",
    "train_loader, val_loader, test_loader = get_dataloaders(\n",
    "    tokenizer=tokenizer,\n",
    "    batch_size=config['batch_size'],\n",
    "    num_workers=config['num_workers'],\n",
    "    image_size=config['image_size'],\n",
    "    max_length=config['max_length'],\n",
    "    train_subset=None,  # Use full dataset, or set to e.g., 1000 for testing\n",
    ")\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Val batches: {len(val_loader)}\")\n",
    "print(f\"Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Novel loss functions initialized\n",
      "✅ Curriculum learning scheduler initialized\n",
      "✅ Clinical validator initialized\n",
      "\n",
      "Total optimization steps: 95728\n",
      "Warmup steps: 1000\n",
      "Novel losses: True\n",
      "Curriculum learning: True\n",
      "Clinical validation: True\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import AdamW\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from src.training.scheduler import get_cosine_schedule_with_warmup\n",
    "from src.utils.metrics import compute_metrics\n",
    "\n",
    "# NOVEL: Import novel training components\n",
    "from src.training.losses import CombinedNovelLoss\n",
    "from src.training.curriculum import AnatomicalCurriculumScheduler, create_curriculum_dataloader\n",
    "from src.utils.clinical_validator import ClinicalValidator\n",
    "\n",
    "# Optimizer\n",
    "no_decay = ['bias', 'LayerNorm.weight', 'layer_norm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {\n",
    "        'params': [p for n, p in model.named_parameters() \n",
    "                   if p.requires_grad and not any(nd in n for nd in no_decay)],\n",
    "        'weight_decay': config['weight_decay'],\n",
    "    },\n",
    "    {\n",
    "        'params': [p for n, p in model.named_parameters() \n",
    "                   if p.requires_grad and any(nd in n for nd in no_decay)],\n",
    "        'weight_decay': 0.0,\n",
    "    },\n",
    "]\n",
    "\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=config['learning_rate'])\n",
    "\n",
    "# Scheduler\n",
    "total_steps = len(train_loader) * config['epochs'] // config['gradient_accumulation_steps']\n",
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=config['warmup_steps'],\n",
    "    num_training_steps=total_steps,\n",
    ")\n",
    "\n",
    "# Mixed precision scaler\n",
    "scaler = GradScaler() if config['use_amp'] else None\n",
    "\n",
    "# NOVEL: Initialize novel loss functions\n",
    "if config.get('use_novel_losses', False):\n",
    "    novel_loss = CombinedNovelLoss(\n",
    "        use_anatomical_consistency=config.get('use_anatomical_consistency_loss', True),\n",
    "        use_clinical_entity=config.get('use_clinical_entity_loss', True),\n",
    "        use_region_focal=config.get('use_region_focal_loss', True),\n",
    "        use_cross_modal=config.get('use_cross_modal_loss', False),\n",
    "        anatomical_weight=config.get('anatomical_loss_weight', 0.1),\n",
    "        clinical_weight=config.get('clinical_loss_weight', 0.2),\n",
    "        focal_weight=config.get('focal_loss_weight', 0.15),\n",
    "        alignment_weight=config.get('alignment_loss_weight', 0.1),\n",
    "    )\n",
    "    print(\"✅ Novel loss functions initialized\")\n",
    "else:\n",
    "    novel_loss = None\n",
    "\n",
    "# NOVEL: Initialize curriculum learning scheduler\n",
    "if config.get('use_curriculum_learning', False):\n",
    "    curriculum_scheduler = AnatomicalCurriculumScheduler()\n",
    "    print(\"✅ Curriculum learning scheduler initialized\")\n",
    "else:\n",
    "    curriculum_scheduler = None\n",
    "\n",
    "# NOVEL: Initialize clinical validator\n",
    "if config.get('use_clinical_validation', False):\n",
    "    clinical_validator = ClinicalValidator()\n",
    "    print(\"✅ Clinical validator initialized\")\n",
    "else:\n",
    "    clinical_validator = None\n",
    "\n",
    "print(f\"\\nTotal optimization steps: {total_steps}\")\n",
    "print(f\"Warmup steps: {config['warmup_steps']}\")\n",
    "print(f\"Novel losses: {config.get('use_novel_losses', False)}\")\n",
    "print(f\"Curriculum learning: {config.get('use_curriculum_learning', False)}\")\n",
    "print(f\"Clinical validation: {config.get('use_clinical_validation', False)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training history\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'val_loss': [],\n",
    "    'bleu_1': [],\n",
    "    'bleu_2': [],\n",
    "    'bleu_3': [],\n",
    "    'bleu_4': [],\n",
    "    'rouge_1': [],\n",
    "    'rouge_2': [],\n",
    "    'rouge_l': [],\n",
    "    'learning_rate': [],\n",
    "}\n",
    "\n",
    "best_metric = 0.0\n",
    "patience_counter = 0\n",
    "patience = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-14 00:21:10.930\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.utils.device\u001b[0m:\u001b[36mget_device\u001b[0m:\u001b[36m27\u001b[0m - \u001b[1mUsing CUDA device: NVIDIA GeForce RTX 4060 Laptop GPU\u001b[0m\n",
      "\u001b[32m2026-01-14 00:21:10.931\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.utils.device\u001b[0m:\u001b[36msetup_cuda_optimizations\u001b[0m:\u001b[36m86\u001b[0m - \u001b[1mEnabled cuDNN benchmark mode\u001b[0m\n",
      "\u001b[32m2026-01-14 00:21:10.932\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.utils.device\u001b[0m:\u001b[36msetup_cuda_optimizations\u001b[0m:\u001b[36m92\u001b[0m - \u001b[1mEnabled TF32 for matrix operations\u001b[0m\n",
      "\u001b[32m2026-01-14 00:21:10.934\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.utils.device\u001b[0m:\u001b[36msetup_cuda_optimizations\u001b[0m:\u001b[36m96\u001b[0m - \u001b[1mCleared CUDA cache\u001b[0m\n",
      "\u001b[32m2026-01-14 00:21:10.963\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.training.trainer\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m146\u001b[0m - \u001b[1mNovel loss functions enabled\u001b[0m\n",
      "\u001b[32m2026-01-14 00:21:10.964\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.training.trainer\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m152\u001b[0m - \u001b[1mCurriculum learning enabled\u001b[0m\n",
      "\u001b[32m2026-01-14 00:21:10.965\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.training.trainer\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m158\u001b[0m - \u001b[1mClinical validation enabled\u001b[0m\n",
      "\u001b[32m2026-01-14 00:21:10.979\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.training.trainer\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m221\u001b[0m - \u001b[1mHAQT-ARR enabled with regions: ['right_lung', 'left_lung', 'heart', 'mediastinum', 'spine', 'diaphragm', 'costophrenic_angles']\u001b[0m\n",
      "\u001b[32m2026-01-14 00:21:10.982\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.training.trainer\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m223\u001b[0m - \u001b[1mTrainer initialized on cuda\u001b[0m\n",
      "\u001b[32m2026-01-14 00:21:10.983\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.training.trainer\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m224\u001b[0m - \u001b[1mTraining for 100 epochs\u001b[0m\n",
      "\u001b[32m2026-01-14 00:21:10.984\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.training.trainer\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m225\u001b[0m - \u001b[1mTotal optimization steps: 95728\u001b[0m\n",
      "\u001b[32m2026-01-14 00:21:10.986\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.training.trainer\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1mLabel smoothing: 0.05\u001b[0m\n",
      "\u001b[32m2026-01-14 00:21:10.987\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.training.trainer\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m227\u001b[0m - \u001b[1mScheduled sampling: True (start=1.0, end=0.6)\u001b[0m\n",
      "\u001b[32m2026-01-14 00:21:10.989\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.training.trainer\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m228\u001b[0m - \u001b[1mRegion regularization: True (weight=0.005)\u001b[0m\n",
      "\u001b[32m2026-01-14 00:21:10.991\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.training.trainer\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m229\u001b[0m - \u001b[1mEarly stopping patience: 999\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "XR2Text Training with AUTO-RESUME\n",
      "======================================================================\n",
      "\n",
      "Clearing GPU memory...\n",
      "GPU Memory - Allocated: 2.03 GB\n",
      "GPU Memory - Cached: 2.06 GB\n",
      "\n",
      ">>> CHECKPOINT FOUND: ..\\checkpoints\\best_model.pt\n",
      ">>> Resuming from epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-14 00:21:15.881\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.training.trainer\u001b[0m:\u001b[36mload_checkpoint\u001b[0m:\u001b[36m766\u001b[0m - \u001b[1mLoaded checkpoint from ..\\checkpoints\\best_model.pt\u001b[0m\n",
      "\u001b[32m2026-01-14 00:21:15.883\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.training.trainer\u001b[0m:\u001b[36mload_checkpoint\u001b[0m:\u001b[36m767\u001b[0m - \u001b[1mResuming from epoch 2\u001b[0m\n",
      "\u001b[32m2026-01-14 00:21:15.885\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.training.trainer\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m368\u001b[0m - \u001b[1mStarting training...\u001b[0m\n",
      "\u001b[32m2026-01-14 00:21:15.886\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.training.curriculum\u001b[0m:\u001b[36mprecompute_difficulty_scores\u001b[0m:\u001b[36m176\u001b[0m - \u001b[1mPre-computing difficulty scores for 30633 samples...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "TRAINING CONFIGURATION SUMMARY:\n",
      "  Learning rate: 0.0001\n",
      "  Label smoothing: 0.05\n",
      "  Validate every: 2 epochs\n",
      "  Generation beams: 2\n",
      "  Min generation length: 10\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-14 00:24:39.766\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.training.curriculum\u001b[0m:\u001b[36mprecompute_difficulty_scores\u001b[0m:\u001b[36m194\u001b[0m - \u001b[1mPre-computed 30633 difficulty scores\u001b[0m\n",
      "\u001b[32m2026-01-14 00:24:39.772\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.training.curriculum\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m328\u001b[0m - \u001b[1mCurriculum stage 'warmup': 3363/30633 samples\u001b[0m\n",
      "\u001b[32m2026-01-14 00:24:39.774\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.training.trainer\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m390\u001b[0m - \u001b[1mCurriculum stage: warmup (3363/30633 samples)\u001b[0m\n",
      "Epoch 3:   0%|                                                                       | 0/106 [00:00<?, ?step/s]"
     ]
    }
   ],
   "source": [
    "# Main training loop - Using XR2TextTrainer class\n",
    "# AUTO-RESUME: Automatically detects and resumes from best checkpoint\n",
    "from src.training.trainer import XR2TextTrainer\n",
    "import torch\n",
    "import gc\n",
    "from pathlib import Path\n",
    "\n",
    "# ============================================\n",
    "# AUTO-RESUME FROM CHECKPOINT\n",
    "# ============================================\n",
    "checkpoint_dir = Path(config['checkpoint_dir'])\n",
    "\n",
    "def find_best_checkpoint(checkpoint_dir):\n",
    "    \"\"\"Find the best checkpoint to resume from.\"\"\"\n",
    "    checkpoint_dir = Path(checkpoint_dir)\n",
    "    if not checkpoint_dir.exists():\n",
    "        return None, 0\n",
    "    \n",
    "    # Priority: best_model.pt > latest checkpoint_epoch_*.pt\n",
    "    best_model = checkpoint_dir / \"best_model.pt\"\n",
    "    if best_model.exists():\n",
    "        ckpt = torch.load(best_model, map_location='cpu')\n",
    "        return str(best_model), ckpt.get('epoch', 0) + 1\n",
    "    \n",
    "    # Find latest epoch checkpoint\n",
    "    epoch_checkpoints = list(checkpoint_dir.glob(\"checkpoint_epoch_*.pt\"))\n",
    "    if epoch_checkpoints:\n",
    "        # Sort by epoch number\n",
    "        def get_epoch(p):\n",
    "            try:\n",
    "                return int(p.stem.split('_')[-1])\n",
    "            except:\n",
    "                return 0\n",
    "        latest = max(epoch_checkpoints, key=get_epoch)\n",
    "        ckpt = torch.load(latest, map_location='cpu')\n",
    "        return str(latest), ckpt.get('epoch', 0) + 1\n",
    "    \n",
    "    return None, 0\n",
    "\n",
    "# Auto-detect checkpoint\n",
    "checkpoint_path, resume_epoch = find_best_checkpoint(checkpoint_dir)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"XR2Text Training with AUTO-RESUME\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Memory cleanup before training\n",
    "print(\"\\nClearing GPU memory...\")\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Memory - Allocated: {torch.cuda.memory_allocated()/1024**3:.2f} GB\")\n",
    "    print(f\"GPU Memory - Cached: {torch.cuda.memory_reserved()/1024**3:.2f} GB\")\n",
    "\n",
    "# Create trainer with optimized config\n",
    "trainer = XR2TextTrainer(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "# AUTO-RESUME: Load checkpoint if found\n",
    "if checkpoint_path:\n",
    "    print(f\"\\n>>> CHECKPOINT FOUND: {checkpoint_path}\")\n",
    "    print(f\">>> Resuming from epoch {resume_epoch}\")\n",
    "    trainer.load_checkpoint(checkpoint_path)\n",
    "else:\n",
    "    print(\"\\n>>> No checkpoint found. Starting fresh training from epoch 1\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"TRAINING CONFIGURATION SUMMARY:\")\n",
    "print(f\"  Learning rate: {config['learning_rate']}\")\n",
    "print(f\"  Label smoothing: {config.get('label_smoothing', 0.1)}\")\n",
    "print(f\"  Validate every: {config.get('validate_every', 2)} epochs\")\n",
    "print(f\"  Generation beams: {config.get('generation', {}).get('num_beams', 5)}\")\n",
    "print(f\"  Min generation length: {config.get('generation', {}).get('min_length', 20)}\")\n",
    "print(\"=\" * 70 + \"\\n\")\n",
    "\n",
    "# Run training\n",
    "final_metrics = trainer.train()\n",
    "\n",
    "# Extract history from trainer for visualization\n",
    "history = {\n",
    "    'train_loss': trainer.metrics_tracker.get_history('train_loss'),\n",
    "    'val_loss': trainer.metrics_tracker.get_history('val_loss'),\n",
    "    'bleu_1': trainer.metrics_tracker.get_history('bleu_1'),\n",
    "    'bleu_2': trainer.metrics_tracker.get_history('bleu_2'),\n",
    "    'bleu_3': trainer.metrics_tracker.get_history('bleu_3'),\n",
    "    'bleu_4': trainer.metrics_tracker.get_history('bleu_4'),\n",
    "    'rouge_1': trainer.metrics_tracker.get_history('rouge_1'),\n",
    "    'rouge_2': trainer.metrics_tracker.get_history('rouge_2'),\n",
    "    'rouge_l': trainer.metrics_tracker.get_history('rouge_l'),\n",
    "    'learning_rate': [trainer.scheduler.get_last_lr()[0]] * (trainer.current_epoch + 1),\n",
    "}\n",
    "\n",
    "# Add clinical validation metrics if enabled\n",
    "if config.get('use_clinical_validation', False):\n",
    "    history['clinical_accuracy'] = trainer.metrics_tracker.get_history('clinical_accuracy')\n",
    "    history['clinical_f1'] = trainer.metrics_tracker.get_history('clinical_f1')\n",
    "    history['critical_errors'] = trainer.metrics_tracker.get_history('critical_errors')\n",
    "\n",
    "# Save training history\n",
    "history_df = pd.DataFrame(history)\n",
    "history_df['epoch'] = range(1, len(history_df) + 1)\n",
    "os.makedirs('../data/statistics', exist_ok=True)\n",
    "history_df.to_csv('../data/statistics/training_history.csv', index=False)\n",
    "\n",
    "# Store predictions and references for sample display\n",
    "predictions = []\n",
    "references = []\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"TRAINING COMPLETE!\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nFinal Metrics:\")\n",
    "for key, value in final_metrics.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  {key}: {value:.4f}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "# Final memory cleanup\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print(f\"\\nFinal GPU Memory - Allocated: {torch.cuda.memory_allocated()/1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROBUST FIX - handles any array length mismatch\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Get all the metrics\n",
    "val_loss = trainer.metrics_tracker.get_history('val_loss')\n",
    "bleu_1 = trainer.metrics_tracker.get_history('bleu_1')\n",
    "bleu_2 = trainer.metrics_tracker.get_history('bleu_2')\n",
    "bleu_3 = trainer.metrics_tracker.get_history('bleu_3')\n",
    "bleu_4 = trainer.metrics_tracker.get_history('bleu_4')\n",
    "rouge_1 = trainer.metrics_tracker.get_history('rouge_1')\n",
    "rouge_2 = trainer.metrics_tracker.get_history('rouge_2')\n",
    "rouge_l = trainer.metrics_tracker.get_history('rouge_l')\n",
    "train_loss = trainer.metrics_tracker.get_history('train_loss')\n",
    "\n",
    "# Debug: Print lengths\n",
    "print(\"Array lengths:\")\n",
    "print(f\"  val_loss: {len(val_loss)}\")\n",
    "print(f\"  bleu_4: {len(bleu_4)}\")\n",
    "print(f\"  rouge_l: {len(rouge_l)}\")\n",
    "print(f\"  train_loss: {len(train_loss)}\")\n",
    "\n",
    "# Find minimum length among validation metrics\n",
    "min_len = min(len(val_loss), len(bleu_4), len(rouge_l))\n",
    "print(f\"\\nUsing {min_len} epochs\")\n",
    "\n",
    "# Create DataFrame with matching lengths\n",
    "history_df = pd.DataFrame({\n",
    "    'epoch': list(range(2, 2 + min_len * 2, 2))[:min_len],\n",
    "    'val_loss': val_loss[:min_len],\n",
    "    'bleu_1': bleu_1[:min_len],\n",
    "    'bleu_2': bleu_2[:min_len],\n",
    "    'bleu_3': bleu_3[:min_len],\n",
    "    'bleu_4': bleu_4[:min_len],\n",
    "    'rouge_1': rouge_1[:min_len],\n",
    "    'rouge_2': rouge_2[:min_len],\n",
    "    'rouge_l': rouge_l[:min_len],\n",
    "})\n",
    "\n",
    "# Add train_loss if available (sample every 2nd)\n",
    "if train_loss:\n",
    "    sampled_train = train_loss[1::2][:min_len]\n",
    "    if len(sampled_train) == min_len:\n",
    "        history_df['train_loss'] = sampled_train\n",
    "\n",
    "# Save\n",
    "os.makedirs('../data/statistics', exist_ok=True)\n",
    "history_df.to_csv('../data/statistics/training_history.csv', index=False)\n",
    "\n",
    "print(f\"\\n✅ Saved {len(history_df)} epochs!\")\n",
    "print(\"\\nLast 5 rows:\")\n",
    "print(history_df.tail())\n",
    "print(f\"\\nBest BLEU-4: {history_df['bleu_4'].max():.4f}\")\n",
    "print(f\"Best ROUGE-L: {history_df['rouge_l'].max():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Curves Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 NOVEL: Enhanced Curriculum Learning Analysis\n",
    "\n",
    "This section provides detailed analysis of our curriculum learning strategy,\n",
    "showing how it affects training dynamics and final performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# NOVEL: ENHANCED CURRICULUM LEARNING ANALYSIS (5 STAGES, 100 EPOCHS)\n",
    "# ============================================\n",
    "from src.training.curriculum import AnatomicalCurriculumScheduler\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"NOVEL: CURRICULUM LEARNING ANALYSIS (5 STAGES, 100 EPOCHS)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Initialize curriculum scheduler\n",
    "curriculum = AnatomicalCurriculumScheduler()\n",
    "\n",
    "# Display curriculum stages\n",
    "print(\"\\n1. CURRICULUM STAGES (5-Stage Progressive Training)\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"\\n{'Stage':<20} {'Epochs':<15} {'Description':<40}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "stage_descriptions = {\n",
    "    'warmup': 'Warmup with easy cases only',\n",
    "    'easy': 'Normal X-rays, simple findings',\n",
    "    'medium': 'Single anatomical region findings',\n",
    "    'hard': 'Multiple regions, moderate complexity',\n",
    "    'finetune': 'Complex cases, full dataset fine-tuning',\n",
    "}\n",
    "\n",
    "for stage in curriculum.stages:\n",
    "    name = stage['name']\n",
    "    epoch_range = f\"{stage['epoch_start']}-{stage['epoch_end']}\"\n",
    "    desc = stage_descriptions.get(name, 'Full dataset')\n",
    "    print(f\"{name:<20} {epoch_range:<15} {desc:<40}\")\n",
    "\n",
    "# Sample difficulty scoring demo\n",
    "print(\"\\n2. SAMPLE DIFFICULTY SCORING\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "sample_reports = [\n",
    "    \"Lungs are clear. Heart size is normal. No acute cardiopulmonary process.\",\n",
    "    \"Mild cardiomegaly. Lungs are clear bilaterally.\",\n",
    "    \"Bilateral pleural effusions. Cardiomegaly. Pulmonary edema.\",\n",
    "    \"Large right pneumothorax. Left lung consolidation. Cardiomegaly.\",\n",
    "]\n",
    "\n",
    "print(\"\\nSample Reports with Difficulty Scores:\")\n",
    "for i, report in enumerate(sample_reports):\n",
    "    scores = curriculum.difficulty_scorer(report)\n",
    "    total_difficulty = scores.get('num_findings', 0) + scores.get('severity_score', 0)\n",
    "    print(f\"\\n[Sample {i+1}] Difficulty: {total_difficulty:.1f}\")\n",
    "    print(f\"   Report: {report[:60]}...\")\n",
    "    print(f\"   Findings: {scores.get('num_findings', 0)}, Regions: {scores.get('num_regions', 0)}\")\n",
    "\n",
    "# Load and analyze training history\n",
    "print(\"\\n3. CURRICULUM LEARNING IMPACT\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "history_path = '../data/statistics/training_history.csv'\n",
    "if os.path.exists(history_path):\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"CURRICULUM LEARNING RESULTS (Real Data)\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    df = pd.read_csv(history_path)\n",
    "\n",
    "    print(\"\\nPerformance at Curriculum Stage Transitions:\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    # 5-stage curriculum: warmup(1-5), easy(6-20), medium(21-50), hard(51-80), finetune(81-100)\n",
    "    stage_info = [\n",
    "        (5, 'End of Stage 1 (Warmup)'),\n",
    "        (20, 'End of Stage 2 (Easy Cases)'),\n",
    "        (50, 'End of Stage 3 (Medium Cases)'),\n",
    "        (80, 'End of Stage 4 (Hard Cases)'),\n",
    "        (100, 'End of Stage 5 (Fine-tuning)'),\n",
    "    ]\n",
    "\n",
    "    for target_epoch, stage_name in stage_info:\n",
    "        mask = df['epoch'] <= target_epoch\n",
    "        if mask.any():\n",
    "            row = df[mask].iloc[-1]\n",
    "            print(f\"\\nEpoch {int(row['epoch'])} - {stage_name}:\")\n",
    "            print(f\"  BLEU-4:  {row['bleu_4']:.4f}\")\n",
    "            print(f\"  ROUGE-L: {row['rouge_l']:.4f}\")\n",
    "            print(f\"  Val Loss: {row['val_loss']:.4f}\")\n",
    "\n",
    "    # Plot curriculum impact\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "    # Stage transitions at epochs 5, 20, 50, 80\n",
    "    stage_transitions = [5, 20, 50, 80]\n",
    "\n",
    "    # BLEU-4 progression with stage markers\n",
    "    axes[0].plot(df['epoch'], df['bleu_4'], linewidth=2, color='blue', marker='o', markersize=3)\n",
    "    for trans in stage_transitions:\n",
    "        axes[0].axvline(x=trans, color='red', linestyle='--', alpha=0.7)\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('BLEU-4')\n",
    "    axes[0].set_title('BLEU-4 Progression with 5-Stage Curriculum')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Loss progression with stage markers\n",
    "    axes[1].plot(df['epoch'], df['val_loss'], linewidth=2, color='orange', marker='o', markersize=3)\n",
    "    for trans in stage_transitions:\n",
    "        axes[1].axvline(x=trans, color='red', linestyle='--', alpha=0.7)\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].set_ylabel('Validation Loss')\n",
    "    axes[1].set_title('Loss Progression with 5-Stage Curriculum')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    os.makedirs('../data/figures', exist_ok=True)\n",
    "    plt.savefig('../data/figures/curriculum_impact.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Curriculum learning analysis complete!\")\n",
    "    print(\"Figure saved: ../data/figures/curriculum_impact.png\")\n",
    "    print(\"=\" * 60)\n",
    "else:\n",
    "    print(\"\\nTraining history not found yet.\")\n",
    "    print(\"Run this cell again after training completes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# FIXED: TRAINING CURVES VISUALIZATION\n",
    "# ============================================\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load history from CSV\n",
    "history_path = \"../data/statistics/training_history.csv\"\n",
    "\n",
    "if os.path.exists(history_path):\n",
    "    print(\"Loading training history from CSV...\")\n",
    "    df = pd.read_csv(history_path)\n",
    "    print(f\"Loaded {len(df)} epochs of data\")\n",
    "\n",
    "    # Check if we have data\n",
    "    if len(df) > 0 and 'bleu_4' in df.columns:\n",
    "        # Create figure with subplots\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "        axes = axes.flatten()\n",
    "\n",
    "        # Plot 1: Validation Loss\n",
    "        axes[0].plot(df['epoch'], df['val_loss'], label='Val Loss', color='orange', linewidth=2, marker='o', markersize=4)\n",
    "        axes[0].set_xlabel('Epoch')\n",
    "        axes[0].set_ylabel('Loss')\n",
    "        axes[0].set_title('Validation Loss')\n",
    "        axes[0].legend()\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "        # Plot 2: BLEU Scores\n",
    "        axes[1].plot(df['epoch'], df['bleu_1'], label='BLEU-1', linewidth=2)\n",
    "        axes[1].plot(df['epoch'], df['bleu_2'], label='BLEU-2', linewidth=2)\n",
    "        axes[1].plot(df['epoch'], df['bleu_3'], label='BLEU-3', linewidth=2)\n",
    "        axes[1].plot(df['epoch'], df['bleu_4'], label='BLEU-4', linewidth=2, marker='o', markersize=4)\n",
    "        axes[1].set_xlabel('Epoch')\n",
    "        axes[1].set_ylabel('Score')\n",
    "        axes[1].set_title('BLEU Scores')\n",
    "        axes[1].legend()\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "        # Plot 3: ROUGE Scores\n",
    "        axes[2].plot(df['epoch'], df['rouge_1'], label='ROUGE-1', linewidth=2)\n",
    "        axes[2].plot(df['epoch'], df['rouge_2'], label='ROUGE-2', linewidth=2)\n",
    "        axes[2].plot(df['epoch'], df['rouge_l'], label='ROUGE-L', linewidth=2, marker='o', markersize=4)\n",
    "        axes[2].set_xlabel('Epoch')\n",
    "        axes[2].set_ylabel('Score')\n",
    "        axes[2].set_title('ROUGE Scores')\n",
    "        axes[2].legend()\n",
    "        axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "        # Plot 4: Combined BLEU-4 and ROUGE-L\n",
    "        axes[3].plot(df['epoch'], df['bleu_4'], label='BLEU-4', linewidth=2, color='blue', marker='o', markersize=4)\n",
    "        axes[3].plot(df['epoch'], df['rouge_l'], label='ROUGE-L', linewidth=2, color='green', marker='s', markersize=4)\n",
    "        axes[3].set_xlabel('Epoch')\n",
    "        axes[3].set_ylabel('Score')\n",
    "        axes[3].set_title('BLEU-4 vs ROUGE-L Comparison')\n",
    "        axes[3].legend()\n",
    "        axes[3].grid(True, alpha=0.3)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        os.makedirs('../data/figures', exist_ok=True)\n",
    "        plt.savefig('../data/figures/training_curves.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "        print(\"\\nTraining curves saved to ../data/figures/training_curves.png\")\n",
    "\n",
    "        # Print summary\n",
    "        print(\"\\n\" + \"=\" * 50)\n",
    "        print(\"TRAINING SUMMARY\")\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"Best BLEU-4:  {df['bleu_4'].max():.4f} (Epoch {df.loc[df['bleu_4'].idxmax(), 'epoch']:.0f})\")\n",
    "        print(f\"Best ROUGE-L: {df['rouge_l'].max():.4f} (Epoch {df.loc[df['rouge_l'].idxmax(), 'epoch']:.0f})\")\n",
    "        print(f\"Final Val Loss: {df['val_loss'].iloc[-1]:.4f}\")\n",
    "    else:\n",
    "        print(\"No valid data in CSV file\")\n",
    "else:\n",
    "    print(\"Training history CSV not found!\")\n",
    "    print(\"Expected at:\", history_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Sample Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sample predictions vs ground truth\n",
    "print(\"Sample Predictions vs Ground Truth:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Check if predictions and references exist\n",
    "if 'predictions' not in dir() or not predictions:\n",
    "    predictions = []\n",
    "if 'references' not in dir() or not references:\n",
    "    references = []\n",
    "\n",
    "if len(predictions) > 0 and len(references) > 0:\n",
    "    for i in range(min(5, len(predictions))):\n",
    "        print(f\"\\n--- Sample {i+1} ---\")\n",
    "        print(f\"\\nGround Truth:\")\n",
    "        print(references[i][:500] + \"...\" if len(references[i]) > 500 else references[i])\n",
    "        print(f\"\\nGenerated:\")\n",
    "        print(predictions[i][:500] + \"...\" if len(predictions[i]) > 500 else predictions[i])\n",
    "        print(\"-\" * 80)\n",
    "else:\n",
    "    print(\"\\n⚠️ No predictions available yet!\")\n",
    "    print(\"   Predictions will be available after training completes (cell 11).\")\n",
    "    print(\"   Or run evaluation on test set in notebook 03_evaluation.ipynb.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Final Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# FIXED: FINAL RESULTS SUMMARY\n",
    "# ============================================\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "history_path = \"../data/statistics/training_history.csv\"\n",
    "\n",
    "if os.path.exists(history_path):\n",
    "    df = pd.read_csv(history_path)\n",
    "\n",
    "    # Find best epoch by combined BLEU-4 + ROUGE-L score\n",
    "    df['combined_score'] = df['bleu_4'] + df['rouge_l']\n",
    "    best_idx = df['combined_score'].idxmax()\n",
    "    best_row = df.loc[best_idx]\n",
    "    final_row = df.iloc[-1]\n",
    "\n",
    "    print(\"=\" * 60)\n",
    "    print(\"TRAINING RESULTS SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # FIXED: Use actual epoch value from the dataframe\n",
    "    print(f\"\\nBest Epoch: {int(best_row['epoch'])} (by BLEU-4 + ROUGE-L)\")\n",
    "\n",
    "    print(f\"\\nBest Metrics (Epoch {int(best_row['epoch'])}):\")\n",
    "    print(f\"  BLEU-1:  {best_row['bleu_1']:.4f}\")\n",
    "    print(f\"  BLEU-2:  {best_row['bleu_2']:.4f}\")\n",
    "    print(f\"  BLEU-3:  {best_row['bleu_3']:.4f}\")\n",
    "    print(f\"  BLEU-4:  {best_row['bleu_4']:.4f}\")\n",
    "    print(f\"  ROUGE-1: {best_row['rouge_1']:.4f}\")\n",
    "    print(f\"  ROUGE-2: {best_row['rouge_2']:.4f}\")\n",
    "    print(f\"  ROUGE-L: {best_row['rouge_l']:.4f}\")\n",
    "\n",
    "    print(f\"\\nFinal Metrics (Epoch {int(final_row['epoch'])}):\")\n",
    "    print(f\"  Val Loss: {final_row['val_loss']:.4f}\")\n",
    "    print(f\"  BLEU-4:   {final_row['bleu_4']:.4f}\")\n",
    "    print(f\"  ROUGE-L:  {final_row['rouge_l']:.4f}\")\n",
    "\n",
    "    # Save best results to CSV\n",
    "    results_table = pd.DataFrame({\n",
    "        'Metric': ['BLEU-1', 'BLEU-2', 'BLEU-3', 'BLEU-4', 'ROUGE-1', 'ROUGE-2', 'ROUGE-L'],\n",
    "        'Best Score': [\n",
    "            best_row['bleu_1'],\n",
    "            best_row['bleu_2'],\n",
    "            best_row['bleu_3'],\n",
    "            best_row['bleu_4'],\n",
    "            best_row['rouge_1'],\n",
    "            best_row['rouge_2'],\n",
    "            best_row['rouge_l'],\n",
    "        ],\n",
    "        'Final Score': [\n",
    "            final_row['bleu_1'],\n",
    "            final_row['bleu_2'],\n",
    "            final_row['bleu_3'],\n",
    "            final_row['bleu_4'],\n",
    "            final_row['rouge_1'],\n",
    "            final_row['rouge_2'],\n",
    "            final_row['rouge_l'],\n",
    "        ]\n",
    "    })\n",
    "\n",
    "    os.makedirs('../data/statistics', exist_ok=True)\n",
    "    results_table.to_csv('../data/statistics/best_results.csv', index=False)\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Results saved to ../data/statistics/best_results.csv\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Display table\n",
    "    print(\"\\nResults Table:\")\n",
    "    print(results_table.to_string(index=False))\n",
    "\n",
    "else:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"TRAINING RESULTS SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"\\nNo training results available yet!\")\n",
    "    print(\"Run training first (cell 11) to see results.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 8. NOVEL: Enhanced Analysis with New Features\n",
    "\n",
    "##This section demonstrates the new enhancement modules for comprehensive report analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# NOVEL: Enhanced Analysis Demo\n",
    "# ============================================\n",
    "# This demonstrates the new enhancement modules\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"NOVEL ENHANCEMENT MODULES DEMO\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Check if model has enhancement modules\n",
    "if hasattr(model, 'generate_with_analysis'):\n",
    "    print(\"\\n✅ Model has enhanced analysis capabilities!\")\n",
    "    print(\"\\nAvailable analysis features:\")\n",
    "    print(\"  1. Uncertainty Quantification\")\n",
    "    print(\"     - Overall confidence score (0-1)\")\n",
    "    print(\"     - Per-finding confidence scores\")\n",
    "    print(\"     - Calibrated uncertainty estimates\")\n",
    "    print(\"\\n  2. Factual Grounding\")\n",
    "    print(\"     - Detected medical findings\")\n",
    "    print(\"     - Potential hallucinations flagged\")\n",
    "    print(\"     - Knowledge graph validation\")\n",
    "    print(\"\\n  3. Explainability\")\n",
    "    print(\"     - Evidence regions highlighted\")\n",
    "    print(\"     - Clinical reasoning chains\")\n",
    "    print(\"     - Attention visualizations\")\n",
    "    print(\"\\n  4. Multi-Task Outputs\")\n",
    "    print(\"     - Region classification\")\n",
    "    print(\"     - Severity prediction\")\n",
    "    print(\"     - Finding detection\")\n",
    "    \n",
    "    # Demo analysis on a sample if test data is available\n",
    "    print(\"\\n\" + \"-\" * 50)\n",
    "    print(\"Running Enhanced Analysis on Sample...\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    try:\n",
    "        # Get a sample from test loader\n",
    "        sample_batch = next(iter(test_loader))\n",
    "        sample_image = sample_batch['images'][0:1].to(config['device'])\n",
    "        \n",
    "        # Run enhanced analysis\n",
    "        with torch.no_grad():\n",
    "            analysis = model.generate_with_analysis(\n",
    "                sample_image,\n",
    "                max_length=config['generation']['max_length'],\n",
    "                num_beams=config['generation']['num_beams'],\n",
    "            )\n",
    "        \n",
    "        print(f\"\\n📝 Generated Report:\")\n",
    "        print(f\"   {analysis.get('report', 'N/A')[:200]}...\")\n",
    "        \n",
    "        print(f\"\\n📊 Uncertainty Analysis:\")\n",
    "        print(f\"   Overall Confidence: {analysis.get('confidence', 0):.2%}\")\n",
    "        if 'finding_confidences' in analysis:\n",
    "            print(f\"   Finding Confidences: {len(analysis['finding_confidences'])} findings analyzed\")\n",
    "        \n",
    "        print(f\"\\n🔍 Factual Grounding:\")\n",
    "        if 'detected_findings' in analysis:\n",
    "            print(f\"   Detected Findings: {analysis['detected_findings'][:5]}\")\n",
    "        if 'potential_hallucinations' in analysis:\n",
    "            print(f\"   Potential Hallucinations: {len(analysis.get('potential_hallucinations', []))}\")\n",
    "        \n",
    "        print(f\"\\n💡 Explainability:\")\n",
    "        if 'evidence_regions' in analysis:\n",
    "            print(f\"   Evidence Regions: {len(analysis['evidence_regions'])} regions identified\")\n",
    "        if 'reasoning' in analysis:\n",
    "            print(f\"   Clinical Reasoning: Available\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"   Demo skipped (requires trained model): {e}\")\n",
    "        \n",
    "else:\n",
    "    print(\"\\n⚠️  Enhancement modules not loaded in current model.\")\n",
    "    print(\"   Ensure use_uncertainty, use_grounding, use_explainability, use_multitask are True.\")\n",
    "    print(\"   Re-initialize model with updated config to enable these features.\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Enhanced Analysis Demo Complete\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (swin)",
   "language": "python",
   "name": "swin"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
