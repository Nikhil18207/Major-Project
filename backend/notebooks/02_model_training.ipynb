{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XR2Text: Model Training with HAQT-ARR\n",
    "\n",
    "This notebook implements the complete training pipeline for the XR2Text model featuring our novel **HAQT-ARR (Hierarchical Anatomical Query Tokens with Adaptive Region Routing)** projection layer.\n",
    "\n",
    "## Novel Contribution: HAQT-ARR\n",
    "\n",
    "Our key innovation is the HAQT-ARR projection layer that bridges vision and language with anatomical awareness:\n",
    "\n",
    "1. **Hierarchical Anatomical Query Tokens**: Region-specific learnable queries for 7 anatomical regions\n",
    "2. **Spatial Prior Injection**: Learnable 2D Gaussian priors for anatomical locations\n",
    "3. **Adaptive Region Routing**: Dynamic weighting of anatomical region importance\n",
    "4. **Cross-Region Interaction**: Transformer layers modeling inter-region dependencies\n",
    "\n",
    "## Architecture\n",
    "```\n",
    "Input Image (384√ó384) ‚Üí Swin Transformer ‚Üí HAQT-ARR Projection ‚Üí BioBART Decoder ‚Üí Report\n",
    "```\n",
    "\n",
    "**Authors**: S. Nikhil, Dadhania Omkumar  \n",
    "**Supervisor**: Dr. Damodar Panigrahy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\MajorProject\\swin\\lib\\site-packages\\torch\\cuda\\__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "SYSTEM CONFIGURATION\n",
      "==================================================\n",
      "CUDA Available: True\n",
      "GPU Connected: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "GPU Memory: 8.0 GB\n",
      "CUDA Version: 12.1\n",
      "PyTorch Version: 2.5.1+cu121\n",
      "\n",
      "Using Device: cuda\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# GPU/CUDA Check - Run this first!\n",
    "# ============================================\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from tqdm.notebook import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['savefig.dpi'] = 300\n",
    "\n",
    "# GPU Check\n",
    "print(\"=\" * 50)\n",
    "print(\"SYSTEM CONFIGURATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
    "    print(f\"CUDA Available: True\")\n",
    "    print(f\"GPU Connected: {gpu_name}\")\n",
    "    print(f\"GPU Memory: {gpu_memory:.1f} GB\")\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"PyTorch Version: {torch.__version__}\")\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    print(f\"CUDA Available: False\")\n",
    "    print(f\"WARNING: Running on CPU (Training will be slow)\")\n",
    "    print(f\"PyTorch Version: {torch.__version__}\")\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"\\nUsing Device: {device}\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Training Configuration with HAQT-ARR + NOVEL FEATURES\n# OPTIMIZED FOR SPEED + METRICS EVERY 2 EPOCHS\n# ALL NOVEL CONTRIBUTIONS ENABLED\nconfig = {\n    # Model\n    'image_size': 384,\n    'encoder_name': 'base',  # Swin-Base\n    'decoder_name': 'biobart',\n    'use_anatomical_attention': True,  # Enable HAQT-ARR (Novel)\n    \n    # HAQT-ARR specific parameters (NOVEL)\n    'num_regions': 7,\n    'num_global_queries': 8,\n    'num_region_queries': 4,\n    'use_spatial_priors': True,\n    'use_adaptive_routing': True,\n    'use_cross_region': True,\n    \n    # Standard parameters\n    'language_dim': 768,\n    \n    # Training - OPTIMIZED FOR SPEED + OOM PREVENTION\n    'epochs': 50,\n    'batch_size': 2,                   # Reduced to prevent OOM\n    'gradient_accumulation_steps': 16, # Keeps effective batch=32\n    'learning_rate': 1e-4,\n    'weight_decay': 0.01,\n    'warmup_steps': 1000,\n    'max_grad_norm': 1.0,\n    \n    # Label smoothing - for better BLEU\n    'label_smoothing': 0.05,\n    \n    # NOVEL: Novel Loss Functions - ENABLED\n    'use_novel_losses': True,\n    'use_anatomical_consistency_loss': True,\n    'use_clinical_entity_loss': True,\n    'use_region_focal_loss': True,\n    'use_cross_modal_loss': False,\n    'anatomical_loss_weight': 0.05,\n    'clinical_loss_weight': 0.1,\n    'focal_loss_weight': 0.1,\n    'alignment_loss_weight': 0.1,\n    \n    # NOVEL: Curriculum Learning - ENABLED\n    'use_curriculum_learning': True,\n    \n    # NOVEL: Clinical Validation - ENABLED\n    'use_clinical_validation': True,\n    \n    # Scheduled Sampling\n    'use_scheduled_sampling': True,\n    'scheduled_sampling_start': 1.0,\n    'scheduled_sampling_end': 0.6,\n    'scheduled_sampling_warmup': 10,\n    \n    # Region regularization\n    'use_region_regularization': True,\n    'region_regularization_weight': 0.005,\n    \n    # Data\n    'max_length': 256,\n    'num_workers': 4,\n    \n    # Device\n    'use_amp': True,\n    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n    \n    # Experiment\n    'experiment_name': f'xr2text_haqt_arr_novel_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}',\n    'checkpoint_dir': '../checkpoints',\n    'validate_every': 2,                # SEE SCORES EVERY 2 EPOCHS\n    'save_every': 5,\n    'patience': 999,\n    'log_dir': '../logs',\n    \n    # Validation - FAST (use less data)\n    'val_fraction': 0.15,               # Use only 15% of val data for speed\n    \n    # Generation parameters - OPTIMIZED FOR SPEED\n    'generation': {\n        'num_beams': 2,                 # FAST: reduced from 3\n        'min_length': 10,               # FAST: reduced\n        'max_length': 150,              # FAST: reduced from 200\n        'length_penalty': 1.0,\n        'repetition_penalty': 1.1,\n        'no_repeat_ngram_size': 3,\n        'early_stopping': True,\n    }\n}\n\n# Create directories\nos.makedirs(config['checkpoint_dir'], exist_ok=True)\nos.makedirs(config['log_dir'], exist_ok=True)\nos.makedirs('../data/figures', exist_ok=True)\n\nprint(\"FAST Training Config - Scores Every 2 Epochs:\")\nprint(\"=\" * 60)\nprint(\"\\nNOVEL CONTRIBUTIONS (ENABLED):\")\nprint(\"  ‚úì HAQT-ARR Projection Layer\")\nprint(\"  ‚úì Novel Loss Functions\")\nprint(\"  ‚úì Clinical Validation\")\nprint(\"  ‚úì Curriculum Learning\")\nprint(\"\\nSPEED OPTIMIZATIONS:\")\nprint(\"  - validate_every: 2 (see BLEU/ROUGE every 2 epochs)\")\nprint(\"  - val_fraction: 15% (fast validation)\")\nprint(\"  - num_beams: 2 (fast generation)\")\nprint(\"  - max_length: 150 (shorter generation)\")\nprint(\"  - num_workers: 4\")\nprint(\"\\nOOM PREVENTION:\")\nprint(\"  - batch_size: 2\")\nprint(\"  - gradient_accumulation_steps: 16\")\nprint(\"=\" * 60)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Model and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-07 12:32:18.069\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.utils.device\u001b[0m:\u001b[36msetup_cuda_optimizations\u001b[0m:\u001b[36m86\u001b[0m - \u001b[1mEnabled cuDNN benchmark mode\u001b[0m\n",
      "\u001b[32m2026-01-07 12:32:18.072\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.utils.device\u001b[0m:\u001b[36msetup_cuda_optimizations\u001b[0m:\u001b[36m92\u001b[0m - \u001b[1mEnabled TF32 for matrix operations\u001b[0m\n",
      "\u001b[32m2026-01-07 12:32:18.072\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.utils.device\u001b[0m:\u001b[36msetup_cuda_optimizations\u001b[0m:\u001b[36m96\u001b[0m - \u001b[1mCleared CUDA cache\u001b[0m\n",
      "\u001b[32m2026-01-07 12:32:18.076\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.models.xr2text\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m92\u001b[0m - \u001b[1mBuilding Swin Transformer Encoder...\u001b[0m\n",
      "\u001b[32m2026-01-07 12:32:18.077\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.models.swin_encoder\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m81\u001b[0m - \u001b[1mInitializing Swin Encoder: swin_base_patch4_window7_224\u001b[0m\n",
      "\u001b[32m2026-01-07 12:32:18.080\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.models.swin_encoder\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m82\u001b[0m - \u001b[1mPretrained: True, Image Size: 384\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating XR2Text model with HAQT-ARR projection layer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-07 12:32:21.478\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.models.swin_encoder\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m96\u001b[0m - \u001b[1mSwin feature dimension: 1024\u001b[0m\n",
      "\u001b[32m2026-01-07 12:32:21.483\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.models.swin_encoder\u001b[0m:\u001b[36m_freeze_layers\u001b[0m:\u001b[36m136\u001b[0m - \u001b[1mFrozen 404,424 parameters in 2 layers\u001b[0m\n",
      "\u001b[32m2026-01-07 12:32:21.485\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.models.swin_encoder\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m117\u001b[0m - \u001b[1mSwin Encoder initialized successfully\u001b[0m\n",
      "\u001b[32m2026-01-07 12:32:21.487\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.models.xr2text\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m110\u001b[0m - \u001b[1mBuilding HAQT-ARR (Hierarchical Anatomical) Projection Layer...\u001b[0m\n",
      "\u001b[32m2026-01-07 12:32:21.489\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.models.anatomical_attention\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m601\u001b[0m - \u001b[1mInitializing HAQT-ARR Projection Layer\u001b[0m\n",
      "\u001b[32m2026-01-07 12:32:21.490\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.models.anatomical_attention\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m602\u001b[0m - \u001b[1m  Visual dim: 1024 -> Language dim: 768\u001b[0m\n",
      "\u001b[32m2026-01-07 12:32:21.492\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.models.anatomical_attention\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m603\u001b[0m - \u001b[1m  Regions: 7, Total queries: 36\u001b[0m\n",
      "\u001b[32m2026-01-07 12:32:21.494\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.models.anatomical_attention\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m604\u001b[0m - \u001b[1m  Spatial priors: True, Adaptive routing: True\u001b[0m\n",
      "\u001b[32m2026-01-07 12:32:21.522\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.models.anatomical_attention\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m219\u001b[0m - \u001b[1mAnatomicalQueryTokens: 8 global + 7x4 region queries\u001b[0m\n",
      "\u001b[32m2026-01-07 12:32:22.147\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.models.anatomical_attention\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m692\u001b[0m - \u001b[1mHAQT-ARR Projection Layer initialized successfully\u001b[0m\n",
      "\u001b[32m2026-01-07 12:32:22.148\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.models.xr2text\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m141\u001b[0m - \u001b[1mBuilding BioBART Decoder...\u001b[0m\n",
      "\u001b[32m2026-01-07 12:32:22.150\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.models.biobart_decoder\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m85\u001b[0m - \u001b[1mInitializing decoder: GanjinZero/biobart-base\u001b[0m\n",
      "\u001b[32m2026-01-07 12:32:22.152\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.models.biobart_decoder\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m86\u001b[0m - \u001b[1mLabel smoothing: 0.0\u001b[0m\n",
      "\u001b[32m2026-01-07 12:32:25.004\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.models.biobart_decoder\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m116\u001b[0m - \u001b[1mDecoder initialized with hidden_dim: 768\u001b[0m\n",
      "\u001b[32m2026-01-07 12:32:25.006\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.models.xr2text\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mXR2Text Model initialized successfully!\u001b[0m\n",
      "\u001b[32m2026-01-07 12:32:25.016\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.models.xr2text\u001b[0m:\u001b[36m_log_model_info\u001b[0m:\u001b[36m173\u001b[0m - \u001b[1mTotal parameters: 251,441,388\u001b[0m\n",
      "\u001b[32m2026-01-07 12:32:25.018\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.models.xr2text\u001b[0m:\u001b[36m_log_model_info\u001b[0m:\u001b[36m174\u001b[0m - \u001b[1mTrainable parameters: 251,036,964\u001b[0m\n",
      "\u001b[32m2026-01-07 12:32:25.019\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.models.xr2text\u001b[0m:\u001b[36m_log_model_info\u001b[0m:\u001b[36m175\u001b[0m - \u001b[1mFrozen parameters: 404,424\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "XR2Text Model with HAQT-ARR (Novel)\n",
      "==================================================\n",
      "Total parameters: 251,441,388\n",
      "Trainable parameters: 251,036,964\n",
      "Frozen parameters: 404,424\n",
      "\n",
      "Anatomical regions: ['right_lung', 'left_lung', 'heart', 'mediastinum', 'spine', 'diaphragm', 'costophrenic_angles']\n",
      "Total queries: 36\n"
     ]
    }
   ],
   "source": [
    "from src.models.xr2text import XR2TextModel, DEFAULT_CONFIG\n",
    "from src.models.anatomical_attention import ANATOMICAL_REGIONS\n",
    "from src.data.dataloader import get_dataloaders\n",
    "from src.utils.device import setup_cuda_optimizations\n",
    "\n",
    "# Setup CUDA optimizations for RTX 4060\n",
    "setup_cuda_optimizations()\n",
    "\n",
    "# Create model with HAQT-ARR (Novel Architecture)\n",
    "print(\"Creating XR2Text model with HAQT-ARR projection layer...\")\n",
    "model_config = {\n",
    "    'image_size': config['image_size'],\n",
    "    'use_anatomical_attention': config['use_anatomical_attention'],  # Enable HAQT-ARR\n",
    "    'encoder': {\n",
    "        'model_name': config['encoder_name'],\n",
    "        'pretrained': True,\n",
    "        'freeze_layers': 2,  # Freeze first 2 Swin layers\n",
    "    },\n",
    "    'projection': {\n",
    "        # HAQT-ARR parameters (Novel)\n",
    "        'language_dim': config['language_dim'],\n",
    "        'num_regions': config['num_regions'],\n",
    "        'num_global_queries': config['num_global_queries'],\n",
    "        'num_region_queries': config['num_region_queries'],\n",
    "        'use_spatial_priors': config['use_spatial_priors'],\n",
    "        'use_adaptive_routing': config['use_adaptive_routing'],\n",
    "        'use_cross_region': config['use_cross_region'],\n",
    "        'feature_size': 12,  # 384/32 = 12x12 patches\n",
    "    },\n",
    "    'decoder': {\n",
    "        'model_name': config['decoder_name'],\n",
    "        'max_length': config['max_length'],\n",
    "    }\n",
    "}\n",
    "\n",
    "model = XR2TextModel.from_config(model_config)\n",
    "model = model.to(config['device'])\n",
    "\n",
    "# Model summary\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"XR2Text Model with HAQT-ARR (Novel)\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Frozen parameters: {total_params - trainable_params:,}\")\n",
    "print(f\"\\nAnatomical regions: {model.get_anatomical_regions()}\")\n",
    "print(f\"Total queries: {config['num_global_queries'] + config['num_regions'] * config['num_region_queries']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-07 12:32:25.725\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.data.dataloader\u001b[0m:\u001b[36mget_dataloaders\u001b[0m:\u001b[36m47\u001b[0m - \u001b[1mCreating dataloaders...\u001b[0m\n",
      "\u001b[32m2026-01-07 12:32:25.750\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.data.dataset\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mLoading MIMIC-CXR dataset (split: train)...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-07 12:32:29.593\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.data.dataset\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m76\u001b[0m - \u001b[1mLoaded 30633 samples\u001b[0m\n",
      "\u001b[32m2026-01-07 12:32:29.597\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.data.dataset\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mLoading MIMIC-CXR dataset (split: validation)...\u001b[0m\n",
      "\u001b[32m2026-01-07 12:32:31.272\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.data.dataset\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m76\u001b[0m - \u001b[1mLoaded 3063 samples\u001b[0m\n",
      "\u001b[32m2026-01-07 12:32:31.275\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.data.dataset\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mLoading MIMIC-CXR dataset (split: test)...\u001b[0m\n",
      "\u001b[32m2026-01-07 12:32:32.888\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.data.dataset\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m76\u001b[0m - \u001b[1mLoaded 3064 samples\u001b[0m\n",
      "\u001b[32m2026-01-07 12:32:32.901\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.data.dataloader\u001b[0m:\u001b[36mget_dataloaders\u001b[0m:\u001b[36m117\u001b[0m - \u001b[1mTrain samples: 30633\u001b[0m\n",
      "\u001b[32m2026-01-07 12:32:32.903\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.data.dataloader\u001b[0m:\u001b[36mget_dataloaders\u001b[0m:\u001b[36m118\u001b[0m - \u001b[1mVal samples: 3063\u001b[0m\n",
      "\u001b[32m2026-01-07 12:32:32.906\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.data.dataloader\u001b[0m:\u001b[36mget_dataloaders\u001b[0m:\u001b[36m119\u001b[0m - \u001b[1mTest samples: 3064\u001b[0m\n",
      "\u001b[32m2026-01-07 12:32:32.907\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.data.dataloader\u001b[0m:\u001b[36mget_dataloaders\u001b[0m:\u001b[36m120\u001b[0m - \u001b[1mBatch size: 4\u001b[0m\n",
      "\u001b[32m2026-01-07 12:32:32.909\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.data.dataloader\u001b[0m:\u001b[36mget_dataloaders\u001b[0m:\u001b[36m121\u001b[0m - \u001b[1mTrain batches: 7658\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batches: 7658\n",
      "Val batches: 766\n",
      "Test batches: 766\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "print(\"\\nLoading datasets...\")\n",
    "tokenizer = model.get_tokenizer()\n",
    "\n",
    "train_loader, val_loader, test_loader = get_dataloaders(\n",
    "    tokenizer=tokenizer,\n",
    "    batch_size=config['batch_size'],\n",
    "    num_workers=config['num_workers'],\n",
    "    image_size=config['image_size'],\n",
    "    max_length=config['max_length'],\n",
    "    train_subset=None,  # Use full dataset, or set to e.g., 1000 for testing\n",
    ")\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Val batches: {len(val_loader)}\")\n",
    "print(f\"Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Novel loss functions initialized\n",
      "‚úÖ Curriculum learning scheduler initialized\n",
      "‚úÖ Clinical validator initialized\n",
      "\n",
      "Total optimization steps: 47862\n",
      "Warmup steps: 500\n",
      "Novel losses: True\n",
      "Curriculum learning: True\n",
      "Clinical validation: True\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import AdamW\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from src.training.scheduler import get_cosine_schedule_with_warmup\n",
    "from src.utils.metrics import compute_metrics\n",
    "\n",
    "# NOVEL: Import novel training components\n",
    "from src.training.losses import CombinedNovelLoss\n",
    "from src.training.curriculum import AnatomicalCurriculumScheduler, create_curriculum_dataloader\n",
    "from src.utils.clinical_validator import ClinicalValidator\n",
    "\n",
    "# Optimizer\n",
    "no_decay = ['bias', 'LayerNorm.weight', 'layer_norm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {\n",
    "        'params': [p for n, p in model.named_parameters() \n",
    "                   if p.requires_grad and not any(nd in n for nd in no_decay)],\n",
    "        'weight_decay': config['weight_decay'],\n",
    "    },\n",
    "    {\n",
    "        'params': [p for n, p in model.named_parameters() \n",
    "                   if p.requires_grad and any(nd in n for nd in no_decay)],\n",
    "        'weight_decay': 0.0,\n",
    "    },\n",
    "]\n",
    "\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=config['learning_rate'])\n",
    "\n",
    "# Scheduler\n",
    "total_steps = len(train_loader) * config['epochs'] // config['gradient_accumulation_steps']\n",
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=config['warmup_steps'],\n",
    "    num_training_steps=total_steps,\n",
    ")\n",
    "\n",
    "# Mixed precision scaler\n",
    "scaler = GradScaler() if config['use_amp'] else None\n",
    "\n",
    "# NOVEL: Initialize novel loss functions\n",
    "if config.get('use_novel_losses', False):\n",
    "    novel_loss = CombinedNovelLoss(\n",
    "        use_anatomical_consistency=config.get('use_anatomical_consistency_loss', True),\n",
    "        use_clinical_entity=config.get('use_clinical_entity_loss', True),\n",
    "        use_region_focal=config.get('use_region_focal_loss', True),\n",
    "        use_cross_modal=config.get('use_cross_modal_loss', False),\n",
    "        anatomical_weight=config.get('anatomical_loss_weight', 0.1),\n",
    "        clinical_weight=config.get('clinical_loss_weight', 0.2),\n",
    "        focal_weight=config.get('focal_loss_weight', 0.15),\n",
    "        alignment_weight=config.get('alignment_loss_weight', 0.1),\n",
    "    )\n",
    "    print(\"‚úÖ Novel loss functions initialized\")\n",
    "else:\n",
    "    novel_loss = None\n",
    "\n",
    "# NOVEL: Initialize curriculum learning scheduler\n",
    "if config.get('use_curriculum_learning', False):\n",
    "    curriculum_scheduler = AnatomicalCurriculumScheduler()\n",
    "    print(\"‚úÖ Curriculum learning scheduler initialized\")\n",
    "else:\n",
    "    curriculum_scheduler = None\n",
    "\n",
    "# NOVEL: Initialize clinical validator\n",
    "if config.get('use_clinical_validation', False):\n",
    "    clinical_validator = ClinicalValidator()\n",
    "    print(\"‚úÖ Clinical validator initialized\")\n",
    "else:\n",
    "    clinical_validator = None\n",
    "\n",
    "print(f\"\\nTotal optimization steps: {total_steps}\")\n",
    "print(f\"Warmup steps: {config['warmup_steps']}\")\n",
    "print(f\"Novel losses: {config.get('use_novel_losses', False)}\")\n",
    "print(f\"Curriculum learning: {config.get('use_curriculum_learning', False)}\")\n",
    "print(f\"Clinical validation: {config.get('use_clinical_validation', False)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training history\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'val_loss': [],\n",
    "    'bleu_1': [],\n",
    "    'bleu_2': [],\n",
    "    'bleu_3': [],\n",
    "    'bleu_4': [],\n",
    "    'rouge_1': [],\n",
    "    'rouge_2': [],\n",
    "    'rouge_l': [],\n",
    "    'learning_rate': [],\n",
    "}\n",
    "\n",
    "best_metric = 0.0\n",
    "patience_counter = 0\n",
    "patience = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Main training loop - Using XR2TextTrainer class\n# OPTIMIZED FOR BETTER BLEU-4 AND ROUGE METRICS\nfrom src.training.trainer import XR2TextTrainer\nimport torch\nimport gc\n\n# ============================================\n# TRAINING OPTIONS - SET THESE\n# ============================================\nRESUME_FROM_CHECKPOINT = True                          # Set to True to resume training\nCHECKPOINT_PATH = \"../checkpoints/checkpoint_epoch_10.pt\"  # Checkpoint to resume from\n\nprint(\"=\" * 70)\nprint(\"XR2Text Training with OPTIMIZED Configuration\")\nprint(\"=\" * 70)\n\n# Memory cleanup before training\nprint(\"\\nClearing GPU memory...\")\ngc.collect()\ntorch.cuda.empty_cache()\nif torch.cuda.is_available():\n    print(f\"GPU Memory - Allocated: {torch.cuda.memory_allocated()/1024**3:.2f} GB\")\n    print(f\"GPU Memory - Cached: {torch.cuda.memory_reserved()/1024**3:.2f} GB\")\n\n# Create trainer with optimized config\ntrainer = XR2TextTrainer(\n    model=model,\n    train_loader=train_loader,\n    val_loader=val_loader,\n    config=config,\n)\n\n# Resume from checkpoint if specified\nif RESUME_FROM_CHECKPOINT:\n    print(f\"\\nLoading checkpoint: {CHECKPOINT_PATH}\")\n    trainer.load_checkpoint(CHECKPOINT_PATH)\n    print(f\"Resuming from epoch {trainer.current_epoch}\")\nelse:\n    print(\"\\nStarting fresh training from epoch 1\")\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"TRAINING CONFIGURATION SUMMARY:\")\nprint(f\"  Learning rate: {config['learning_rate']}\")\nprint(f\"  Label smoothing: {config.get('label_smoothing', 0.1)}\")\nprint(f\"  Validate every: {config.get('validate_every', 2)} epochs\")\nprint(f\"  Generation beams: {config.get('generation', {}).get('num_beams', 5)}\")\nprint(f\"  Min generation length: {config.get('generation', {}).get('min_length', 20)}\")\nprint(\"=\" * 70 + \"\\n\")\n\n# Run training\nfinal_metrics = trainer.train()\n\n# Extract history from trainer for visualization\nhistory = {\n    'train_loss': trainer.metrics_tracker.get_history('train_loss'),\n    'val_loss': trainer.metrics_tracker.get_history('val_loss'),\n    'bleu_1': trainer.metrics_tracker.get_history('bleu_1'),\n    'bleu_2': trainer.metrics_tracker.get_history('bleu_2'),\n    'bleu_3': trainer.metrics_tracker.get_history('bleu_3'),\n    'bleu_4': trainer.metrics_tracker.get_history('bleu_4'),\n    'rouge_1': trainer.metrics_tracker.get_history('rouge_1'),\n    'rouge_2': trainer.metrics_tracker.get_history('rouge_2'),\n    'rouge_l': trainer.metrics_tracker.get_history('rouge_l'),\n    'learning_rate': [trainer.scheduler.get_last_lr()[0]] * (trainer.current_epoch + 1),\n}\n\n# Add clinical validation metrics if enabled\nif config.get('use_clinical_validation', False):\n    history['clinical_accuracy'] = trainer.metrics_tracker.get_history('clinical_accuracy')\n    history['clinical_f1'] = trainer.metrics_tracker.get_history('clinical_f1')\n    history['critical_errors'] = trainer.metrics_tracker.get_history('critical_errors')\n\n# Save training history\nhistory_df = pd.DataFrame(history)\nhistory_df['epoch'] = range(1, len(history_df) + 1)\nos.makedirs('../data/statistics', exist_ok=True)\nhistory_df.to_csv('../data/statistics/training_history.csv', index=False)\n\n# Store predictions and references for sample display\npredictions = []\nreferences = []\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"TRAINING COMPLETE!\")\nprint(\"=\" * 70)\nprint(f\"\\nFinal Metrics:\")\nfor key, value in final_metrics.items():\n    if isinstance(value, float):\n        print(f\"  {key}: {value:.4f}\")\n    else:\n        print(f\"  {key}: {value}\")\n\n# Final memory cleanup\ngc.collect()\ntorch.cuda.empty_cache()\nprint(f\"\\nFinal GPU Memory - Allocated: {torch.cuda.memory_allocated()/1024**3:.2f} GB\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Curves Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 NOVEL: Enhanced Curriculum Learning Analysis\n",
    "\n",
    "This section provides detailed analysis of our curriculum learning strategy,\n",
    "showing how it affects training dynamics and final performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "NOVEL: CURRICULUM LEARNING ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "1. CURRICULUM STAGES\n",
      "------------------------------------------------------------\n",
      "\n",
      "Stage                Epochs          Description                             \n",
      "--------------------------------------------------------------------------------\n",
      "normal_cases         0-5             Normal X-rays, simple findings (e.g., \"lungs are clear\")\n",
      "single_region        5-15            Single anatomical region findings (e.g., cardiomegaly)\n",
      "multi_region         15-30           Multiple regions, moderate complexity   \n",
      "complex_cases        30-50           Complex cases with multiple severe findings\n",
      "\n",
      "2. SAMPLE DIFFICULTY SCORING\n",
      "------------------------------------------------------------\n",
      "\n",
      "Sample Reports with Difficulty Scores:\n",
      "\n",
      "[Sample 1] Difficulty: 0.0\n",
      "   Report: Lungs are clear. Heart size is normal. No acute cardiopulmonary proces...\n",
      "   Findings: 0, Regions: 3\n",
      "\n",
      "[Sample 2] Difficulty: 1.0\n",
      "   Report: Mild cardiomegaly. Lungs are clear bilaterally....\n",
      "   Findings: 1, Regions: 1\n",
      "\n",
      "[Sample 3] Difficulty: 3.0\n",
      "   Report: Bilateral pleural effusions. Cardiomegaly. Pulmonary edema....\n",
      "   Findings: 3, Regions: 2\n",
      "\n",
      "[Sample 4] Difficulty: 4.0\n",
      "   Report: Large right pneumothorax. Left lung consolidation. Cardiomegaly. Bilat...\n",
      "   Findings: 4, Regions: 1\n",
      "\n",
      "3. CURRICULUM LEARNING IMPACT\n",
      "------------------------------------------------------------\n",
      "\n",
      "‚ö†Ô∏è  Training history not found yet.\n",
      "   Run this cell again after training completes.\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# ENHANCED CURRICULUM LEARNING ANALYSIS\n",
    "# ============================================\n",
    "from src.training.curriculum import AnatomicalCurriculumScheduler\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"NOVEL: CURRICULUM LEARNING ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Initialize curriculum scheduler\n",
    "curriculum = AnatomicalCurriculumScheduler()\n",
    "\n",
    "# Display curriculum stages\n",
    "print(\"\\n1. CURRICULUM STAGES\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"\\n{'Stage':<20} {'Epochs':<15} {'Description':<40}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "stage_descriptions = {\n",
    "    'normal_cases': 'Normal X-rays, simple findings (e.g., \"lungs are clear\")',\n",
    "    'single_region': 'Single anatomical region findings (e.g., cardiomegaly)',\n",
    "    'multi_region': 'Multiple regions, moderate complexity',\n",
    "    'complex_cases': 'Complex cases with multiple severe findings',\n",
    "}\n",
    "\n",
    "for stage in curriculum.stages:\n",
    "    name = stage['name']\n",
    "    epoch_range = f\"{stage['epoch_start']}-{stage['epoch_end']}\"\n",
    "    desc = stage_descriptions.get(name, 'Full dataset')\n",
    "    print(f\"{name:<20} {epoch_range:<15} {desc:<40}\")\n",
    "\n",
    "# Curriculum difficulty scoring\n",
    "print(\"\\n2. SAMPLE DIFFICULTY SCORING\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "sample_reports = [\n",
    "    \"Lungs are clear. Heart size is normal. No acute cardiopulmonary process.\",\n",
    "    \"Mild cardiomegaly. Lungs are clear bilaterally.\",\n",
    "    \"Bilateral pleural effusions. Cardiomegaly. Pulmonary edema.\",\n",
    "    \"Large right pneumothorax. Left lung consolidation. Cardiomegaly. Bilateral effusions. ETT in place.\",\n",
    "]\n",
    "\n",
    "print(\"\\nSample Reports with Difficulty Scores:\")\n",
    "for i, report in enumerate(sample_reports):\n",
    "    scores = curriculum.difficulty_scorer(report)\n",
    "    total_difficulty = scores.get('num_findings', 0) + scores.get('severity_score', 0)\n",
    "    print(f\"\\n[Sample {i+1}] Difficulty: {total_difficulty:.1f}\")\n",
    "    print(f\"   Report: {report[:70]}...\")\n",
    "    print(f\"   Findings: {scores.get('num_findings', 0)}, Regions: {scores.get('num_regions', 0)}\")\n",
    "\n",
    "# Simulated curriculum learning results\n",
    "print(\"\\n3. CURRICULUM LEARNING IMPACT\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# ============================================\n",
    "# NOTE: Curriculum learning benefits will be measured\n",
    "# after training completes. The above shows the CONCEPT.\n",
    "# Real performance comparison will be added post-training.\n",
    "# ============================================\n",
    "\n",
    "# NOTE: Curriculum learning benefits will be measured after training.\n",
    "# Real performance data will be added post-training.\n",
    "\n",
    "# ============================================\n",
    "# POST-TRAINING: Curriculum Learning Analysis\n",
    "# This will show real results after training completes\n",
    "# ============================================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Check if training history exists\n",
    "history_path = '../data/statistics/training_history.csv'\n",
    "if os.path.exists(history_path):\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"CURRICULUM LEARNING RESULTS (Real Data)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Load training history\n",
    "    df = pd.read_csv(history_path)\n",
    "    \n",
    "    # Analyze curriculum stage transitions\n",
    "    stage_transitions = [5, 15, 30]  # Epochs where curriculum changes\n",
    "    \n",
    "    print(\"\\nPerformance at Curriculum Stage Transitions:\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for i, epoch in enumerate([1, 5, 15, 30, 50]):\n",
    "        if epoch <= len(df):\n",
    "            row = df.iloc[epoch-1]\n",
    "            stage = ['Stage 1 (Normal)', 'Stage 1‚Üí2', 'Stage 2‚Üí3', 'Stage 3‚Üí4', 'Final'][i]\n",
    "            print(f\"Epoch {epoch} ({stage}):\")\n",
    "            print(f\"  BLEU-4: {row['bleu_4']:.4f}\")\n",
    "            print(f\"  ROUGE-L: {row['rouge_l']:.4f}\")\n",
    "            print(f\"  Loss: {row['val_loss']:.4f}\")\n",
    "            print()\n",
    "    \n",
    "    # Plot curriculum impact\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # BLEU-4 progression\n",
    "    axes[0].plot(df['epoch'], df['bleu_4'], linewidth=2, color='blue')\n",
    "    for trans in stage_transitions:\n",
    "        if trans <= len(df):\n",
    "            axes[0].axvline(x=trans, color='red', linestyle='--', alpha=0.5)\n",
    "            axes[0].text(trans, axes[0].get_ylim()[1]*0.9, 'Stage\\nChange', \n",
    "                        ha='center', fontsize=8)\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('BLEU-4')\n",
    "    axes[0].set_title('BLEU-4 Progression with Curriculum Stages')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Loss progression\n",
    "    axes[1].plot(df['epoch'], df['val_loss'], linewidth=2, color='orange')\n",
    "    for trans in stage_transitions:\n",
    "        if trans <= len(df):\n",
    "            axes[1].axvline(x=trans, color='red', linestyle='--', alpha=0.5)\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].set_ylabel('Validation Loss')\n",
    "    axes[1].set_title('Loss Progression with Curriculum Stages')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../data/figures/curriculum_impact.png', dpi=300)\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"‚úÖ Curriculum learning analysis complete!\")\n",
    "    print(\"   Figure saved: ../data/figures/curriculum_impact.png\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  Training history not found yet.\")\n",
    "    print(\"   Run this cell again after training completes.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================\n# TRAINING CURVES VISUALIZATION\n# ============================================\n# This works with both freshly trained history AND loaded history from CSV!\n\nimport os\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Load history from CSV if not already in memory (i.e., training was skipped)\nhistory_path = \"../data/statistics/training_history.csv\"\n\n# Initialize history if it doesn't exist\nif 'history' not in dir() or not history:\n    history = {}\n\nif not history.get('train_loss') or len(history.get('train_loss', [])) == 0:\n    if os.path.exists(history_path):\n        print(\"üìÇ Loading training history from saved CSV...\")\n        history_df = pd.read_csv(history_path)\n        history = history_df.to_dict(orient='list')\n        if 'epoch' in history:\n            del history['epoch']\n        print(f\"   Loaded {len(history.get('train_loss', []))} epochs of history\")\n    else:\n        print(\"‚ö†Ô∏è No training history found! Run training first (cell 11).\")\n        history = {}\nelse:\n    # Save training history if it came from training\n    history_df = pd.DataFrame(history)\n    history_df['epoch'] = range(1, len(history_df) + 1)\n    os.makedirs('../data/statistics', exist_ok=True)\n    history_df.to_csv(history_path, index=False)\n    print(\"üíæ Training history saved to CSV\")\n\n# Check if we have history to plot\nif history.get('train_loss') and len(history['train_loss']) > 0:\n    # Plot training curves with NOVEL features\n    num_plots = 6 if config.get('use_novel_losses', False) or config.get('use_clinical_validation', False) else 4\n    fig, axes = plt.subplots(2, 3, figsize=(18, 10)) if num_plots > 4 else plt.subplots(2, 2, figsize=(14, 10))\n    axes = axes.flatten()\n\n    plot_idx = 0\n\n    # Loss curves\n    axes[plot_idx].plot(history['train_loss'], label='Train Loss', color='blue', linewidth=2)\n    axes[plot_idx].plot(history['val_loss'], label='Val Loss', color='orange', linewidth=2)\n    axes[plot_idx].set_xlabel('Epoch')\n    axes[plot_idx].set_ylabel('Loss')\n    axes[plot_idx].set_title('Training and Validation Loss')\n    axes[plot_idx].legend()\n    axes[plot_idx].grid(True, alpha=0.3)\n    plot_idx += 1\n\n    # BLEU scores\n    axes[plot_idx].plot(history['bleu_1'], label='BLEU-1', linewidth=2)\n    axes[plot_idx].plot(history['bleu_2'], label='BLEU-2', linewidth=2)\n    axes[plot_idx].plot(history['bleu_3'], label='BLEU-3', linewidth=2)\n    axes[plot_idx].plot(history['bleu_4'], label='BLEU-4', linewidth=2)\n    axes[plot_idx].set_xlabel('Epoch')\n    axes[plot_idx].set_ylabel('Score')\n    axes[plot_idx].set_title('BLEU Scores')\n    axes[plot_idx].legend()\n    axes[plot_idx].grid(True, alpha=0.3)\n    plot_idx += 1\n\n    # ROUGE scores\n    axes[plot_idx].plot(history['rouge_1'], label='ROUGE-1', linewidth=2)\n    axes[plot_idx].plot(history['rouge_2'], label='ROUGE-2', linewidth=2)\n    axes[plot_idx].plot(history['rouge_l'], label='ROUGE-L', linewidth=2)\n    axes[plot_idx].set_xlabel('Epoch')\n    axes[plot_idx].set_ylabel('Score')\n    axes[plot_idx].set_title('ROUGE Scores')\n    axes[plot_idx].legend()\n    axes[plot_idx].grid(True, alpha=0.3)\n    plot_idx += 1\n\n    # NOVEL: Novel loss components\n    if config.get('use_novel_losses', False) and 'anatomical_consistency_loss' in history:\n        axes[plot_idx].plot(history['anatomical_consistency_loss'], label='Anatomical Consistency', linewidth=2, color='purple')\n        axes[plot_idx].plot(history['clinical_entity_loss'], label='Clinical Entity', linewidth=2, color='red')\n        axes[plot_idx].plot(history['region_focal_loss'], label='Region Focal', linewidth=2, color='green')\n        axes[plot_idx].set_xlabel('Epoch')\n        axes[plot_idx].set_ylabel('Loss')\n        axes[plot_idx].set_title('Novel Loss Components (NOVEL)')\n        axes[plot_idx].legend()\n        axes[plot_idx].grid(True, alpha=0.3)\n        plot_idx += 1\n\n    # NOVEL: Clinical validation metrics\n    if config.get('use_clinical_validation', False) and 'clinical_accuracy' in history:\n        ax_twin = axes[plot_idx].twinx()\n        axes[plot_idx].plot(history['clinical_accuracy'], label='Clinical Accuracy', linewidth=2, color='blue')\n        axes[plot_idx].plot(history['clinical_f1'], label='Clinical F1', linewidth=2, color='orange')\n        axes[plot_idx].set_xlabel('Epoch')\n        axes[plot_idx].set_ylabel('Score', color='black')\n        axes[plot_idx].set_title('Clinical Validation Metrics (NOVEL)')\n        axes[plot_idx].legend(loc='upper left')\n        axes[plot_idx].grid(True, alpha=0.3)\n        \n        # Critical errors on secondary axis\n        ax_twin.plot(history['critical_errors'], label='Critical Errors', linewidth=2, color='red', linestyle='--')\n        ax_twin.set_ylabel('Critical Errors', color='red')\n        ax_twin.legend(loc='upper right')\n        ax_twin.tick_params(axis='y', labelcolor='red')\n        plot_idx += 1\n\n    # Learning rate\n    if 'learning_rate' in history and len(history['learning_rate']) > 0:\n        axes[plot_idx].plot(history['learning_rate'], color='green', linewidth=2)\n        axes[plot_idx].set_xlabel('Epoch')\n        axes[plot_idx].set_ylabel('Learning Rate')\n        axes[plot_idx].set_title('Learning Rate Schedule')\n        axes[plot_idx].set_yscale('log')\n        axes[plot_idx].grid(True, alpha=0.3)\n        plot_idx += 1\n\n    # Hide unused subplots\n    for i in range(plot_idx, len(axes)):\n        axes[i].axis('off')\n\n    plt.tight_layout()\n    os.makedirs('../data/figures', exist_ok=True)\n    plt.savefig('../data/figures/training_curves_novel.png', dpi=300)\n    plt.show()\n    print(\"‚úÖ Training curves saved with NOVEL features visualization\")\nelse:\n    print(\"‚ö†Ô∏è No training history available to plot.\")\n    print(\"   Please run training first (cell 11) or ensure training_history.csv exists.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Sample Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Show sample predictions vs ground truth\nprint(\"Sample Predictions vs Ground Truth:\")\nprint(\"=\" * 80)\n\n# Check if predictions and references exist\nif 'predictions' not in dir() or not predictions:\n    predictions = []\nif 'references' not in dir() or not references:\n    references = []\n\nif len(predictions) > 0 and len(references) > 0:\n    for i in range(min(5, len(predictions))):\n        print(f\"\\n--- Sample {i+1} ---\")\n        print(f\"\\nGround Truth:\")\n        print(references[i][:500] + \"...\" if len(references[i]) > 500 else references[i])\n        print(f\"\\nGenerated:\")\n        print(predictions[i][:500] + \"...\" if len(predictions[i]) > 500 else predictions[i])\n        print(\"-\" * 80)\nelse:\n    print(\"\\n‚ö†Ô∏è No predictions available yet!\")\n    print(\"   Predictions will be available after training completes (cell 11).\")\n    print(\"   Or run evaluation on test set in notebook 03_evaluation.ipynb.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Final Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Best results\nimport os\nimport pandas as pd\nimport numpy as np\n\n# Initialize history if needed\nif 'history' not in dir() or not history:\n    history = {}\n\n# Try to load from CSV if history is empty\nhistory_path = \"../data/statistics/training_history.csv\"\nif not history.get('train_loss') or len(history.get('train_loss', [])) == 0:\n    if os.path.exists(history_path):\n        history_df = pd.read_csv(history_path)\n        history = history_df.to_dict(orient='list')\n        if 'epoch' in history:\n            del history['epoch']\n\n# Check if we have data\nif history.get('bleu_4') and len(history['bleu_4']) > 0 and history.get('rouge_l') and len(history['rouge_l']) > 0:\n    # Find best epoch\n    combined_scores = [b4 + rl for b4, rl in zip(history['bleu_4'], history['rouge_l'])]\n    best_epoch = np.argmax(combined_scores)\n\n    print(\"=\" * 60)\n    print(\"TRAINING RESULTS SUMMARY\")\n    print(\"=\" * 60)\n    print(f\"\\nBest Epoch: {best_epoch + 1}\")\n    print(f\"\\nBest Metrics:\")\n    print(f\"  BLEU-1: {history['bleu_1'][best_epoch]:.4f}\")\n    print(f\"  BLEU-2: {history['bleu_2'][best_epoch]:.4f}\")\n    print(f\"  BLEU-3: {history['bleu_3'][best_epoch]:.4f}\")\n    print(f\"  BLEU-4: {history['bleu_4'][best_epoch]:.4f}\")\n    print(f\"  ROUGE-1: {history['rouge_1'][best_epoch]:.4f}\")\n    print(f\"  ROUGE-2: {history['rouge_2'][best_epoch]:.4f}\")\n    print(f\"  ROUGE-L: {history['rouge_l'][best_epoch]:.4f}\")\n    print(f\"\\nFinal Train Loss: {history['train_loss'][-1]:.4f}\")\n    print(f\"Final Val Loss: {history['val_loss'][-1]:.4f}\")\n\n    # Save results table\n    results_table = pd.DataFrame({\n        'Metric': ['BLEU-1', 'BLEU-2', 'BLEU-3', 'BLEU-4', 'ROUGE-1', 'ROUGE-2', 'ROUGE-L'],\n        'Score': [\n            history['bleu_1'][best_epoch],\n            history['bleu_2'][best_epoch],\n            history['bleu_3'][best_epoch],\n            history['bleu_4'][best_epoch],\n            history['rouge_1'][best_epoch],\n            history['rouge_2'][best_epoch],\n            history['rouge_l'][best_epoch],\n        ]\n    })\n    os.makedirs('../data/statistics', exist_ok=True)\n    results_table.to_csv('../data/statistics/best_results.csv', index=False)\n    print(\"\\n‚úÖ Results saved to ../data/statistics/best_results.csv\")\nelse:\n    print(\"=\" * 60)\n    print(\"TRAINING RESULTS SUMMARY\")\n    print(\"=\" * 60)\n    print(\"\\n‚ö†Ô∏è No training results available yet!\")\n    print(\"   Run training first (cell 11) to see results.\")\n    print(\"   Or ensure training_history.csv exists in ../data/statistics/\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (swin)",
   "language": "python",
   "name": "swin"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}