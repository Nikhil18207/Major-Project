{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XR2Text: Model Training with HAQT-ARR\n",
    "\n",
    "This notebook implements the complete training pipeline for the XR2Text model featuring our novel **HAQT-ARR (Hierarchical Anatomical Query Tokens with Adaptive Region Routing)** projection layer.\n",
    "\n",
    "## Novel Contribution: HAQT-ARR\n",
    "\n",
    "Our key innovation is the HAQT-ARR projection layer that bridges vision and language with anatomical awareness:\n",
    "\n",
    "1. **Hierarchical Anatomical Query Tokens**: Region-specific learnable queries for 7 anatomical regions\n",
    "2. **Spatial Prior Injection**: Learnable 2D Gaussian priors for anatomical locations\n",
    "3. **Adaptive Region Routing**: Dynamic weighting of anatomical region importance\n",
    "4. **Cross-Region Interaction**: Transformer layers modeling inter-region dependencies\n",
    "\n",
    "## Architecture\n",
    "```\n",
    "Input Image (384Ã—384) â†’ Swin Transformer â†’ HAQT-ARR Projection â†’ BioBART Decoder â†’ Report\n",
    "```\n",
    "\n",
    "**Authors**: S. Nikhil, Dadhania Omkumar  \n",
    "**Supervisor**: Dr. Damodar Panigrahy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\MajorProject\\swin\\lib\\site-packages\\torch\\cuda\\__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "SYSTEM CONFIGURATION\n",
      "==================================================\n",
      "CUDA Available: True\n",
      "GPU Connected: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "GPU Memory: 8.0 GB\n",
      "CUDA Version: 12.1\n",
      "PyTorch Version: 2.5.1+cu121\n",
      "\n",
      "Using Device: cuda\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# GPU/CUDA Check - Run this first!\n",
    "# ============================================\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from tqdm.notebook import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['savefig.dpi'] = 300\n",
    "\n",
    "# GPU Check\n",
    "print(\"=\" * 50)\n",
    "print(\"SYSTEM CONFIGURATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
    "    print(f\"CUDA Available: True\")\n",
    "    print(f\"GPU Connected: {gpu_name}\")\n",
    "    print(f\"GPU Memory: {gpu_memory:.1f} GB\")\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"PyTorch Version: {torch.__version__}\")\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    print(f\"CUDA Available: False\")\n",
    "    print(f\"WARNING: Running on CPU (Training will be slow)\")\n",
    "    print(f\"PyTorch Version: {torch.__version__}\")\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"\\nUsing Device: {device}\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Configuration (with HAQT-ARR):\n",
      "==================================================\n",
      "  image_size: 384\n",
      "  encoder_name: base\n",
      "  decoder_name: biobart\n",
      "  use_anatomical_attention: True\n",
      "  num_regions: 7\n",
      "  num_global_queries: 8\n",
      "  num_region_queries: 4\n",
      "  use_spatial_priors: True\n",
      "  use_adaptive_routing: True\n",
      "  use_cross_region: True\n",
      "  language_dim: 768\n",
      "  epochs: 50\n",
      "  batch_size: 4\n",
      "  gradient_accumulation_steps: 8\n",
      "  learning_rate: 5e-05\n",
      "  weight_decay: 0.01\n",
      "  warmup_steps: 500\n",
      "  max_grad_norm: 1.0\n",
      "  use_novel_losses: True\n",
      "  use_anatomical_consistency_loss: True\n",
      "  use_clinical_entity_loss: True\n",
      "  use_region_focal_loss: True\n",
      "  use_cross_modal_loss: False\n",
      "  anatomical_loss_weight: 0.1\n",
      "  clinical_loss_weight: 0.2\n",
      "  focal_loss_weight: 0.15\n",
      "  alignment_loss_weight: 0.1\n",
      "  use_curriculum_learning: True\n",
      "  use_clinical_validation: True\n",
      "  max_length: 256\n",
      "  num_workers: 2\n",
      "  use_amp: True\n",
      "  device: cuda\n",
      "  experiment_name: xr2text_haqt_arr_novel_20260107_123159\n",
      "  checkpoint_dir: ../checkpoints\n",
      "  validate_every: 5\n",
      "  save_every: 5\n",
      "  patience: 999\n",
      "  log_dir: ../logs\n"
     ]
    }
   ],
   "source": [
    "# Training Configuration with HAQT-ARR + NOVEL FEATURES\n",
    "config = {\n",
    "    # Model\n",
    "    'image_size': 384,\n",
    "    'encoder_name': 'base',  # Swin-Base\n",
    "    'decoder_name': 'biobart',\n",
    "    'use_anatomical_attention': True,  # Enable HAQT-ARR (Novel)\n",
    "    \n",
    "    # HAQT-ARR specific parameters\n",
    "    'num_regions': 7,              # 7 anatomical regions\n",
    "    'num_global_queries': 8,       # Global context queries\n",
    "    'num_region_queries': 4,       # Queries per anatomical region\n",
    "    'use_spatial_priors': True,    # Learnable Gaussian priors\n",
    "    'use_adaptive_routing': True,  # Dynamic region weighting\n",
    "    'use_cross_region': True,      # Cross-region interaction\n",
    "    \n",
    "    # Standard parameters\n",
    "    'language_dim': 768,\n",
    "    \n",
    "    # Training\n",
    "    'epochs': 50,\n",
    "    'batch_size': 4,\n",
    "    'gradient_accumulation_steps': 8,  # Effective batch size = 32\n",
    "    'learning_rate': 5e-5,\n",
    "    'weight_decay': 0.01,\n",
    "    'warmup_steps': 500,\n",
    "    'max_grad_norm': 1.0,\n",
    "    \n",
    "    # NOVEL: Novel Loss Functions\n",
    "    'use_novel_losses': True,\n",
    "    'use_anatomical_consistency_loss': True,\n",
    "    'use_clinical_entity_loss': True,\n",
    "    'use_region_focal_loss': True,\n",
    "    'use_cross_modal_loss': False,  # Requires decoder hidden states\n",
    "    'anatomical_loss_weight': 0.1,\n",
    "    'clinical_loss_weight': 0.2,\n",
    "    'focal_loss_weight': 0.15,\n",
    "    'alignment_loss_weight': 0.1,\n",
    "    \n",
    "    # NOVEL: Curriculum Learning\n",
    "    'use_curriculum_learning': True,\n",
    "    \n",
    "    # NOVEL: Clinical Validation\n",
    "    'use_clinical_validation': True,\n",
    "    \n",
    "    # Data\n",
    "    'max_length': 256,\n",
    "    'num_workers': 2,\n",
    "    \n",
    "    # Device\n",
    "    'use_amp': True,  # Mixed precision for RTX 4060\n",
    "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    \n",
    "    # Experiment\n",
    "    'experiment_name': f'xr2text_haqt_arr_novel_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}',\n",
    "    'checkpoint_dir': '../checkpoints',\n",
    "    'validate_every': 5,  # Validate every 5 epochs (saves time)\n",
    "    'save_every': 5,  # Save checkpoint every 5 epochs\n",
    "    'patience': 999,  # Disable early stopping (train all 50 epochs)\n",
    "    'log_dir': '../logs',\n",
    "}\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(config['checkpoint_dir'], exist_ok=True)\n",
    "os.makedirs(config['log_dir'], exist_ok=True)\n",
    "os.makedirs('../data/figures', exist_ok=True)\n",
    "\n",
    "print(\"Training Configuration (with HAQT-ARR):\")\n",
    "print(\"=\" * 50)\n",
    "for k, v in config.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Model and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-07 12:32:18.069\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.utils.device\u001b[0m:\u001b[36msetup_cuda_optimizations\u001b[0m:\u001b[36m86\u001b[0m - \u001b[1mEnabled cuDNN benchmark mode\u001b[0m\n",
      "\u001b[32m2026-01-07 12:32:18.072\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.utils.device\u001b[0m:\u001b[36msetup_cuda_optimizations\u001b[0m:\u001b[36m92\u001b[0m - \u001b[1mEnabled TF32 for matrix operations\u001b[0m\n",
      "\u001b[32m2026-01-07 12:32:18.072\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.utils.device\u001b[0m:\u001b[36msetup_cuda_optimizations\u001b[0m:\u001b[36m96\u001b[0m - \u001b[1mCleared CUDA cache\u001b[0m\n",
      "\u001b[32m2026-01-07 12:32:18.076\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.models.xr2text\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m92\u001b[0m - \u001b[1mBuilding Swin Transformer Encoder...\u001b[0m\n",
      "\u001b[32m2026-01-07 12:32:18.077\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.models.swin_encoder\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m81\u001b[0m - \u001b[1mInitializing Swin Encoder: swin_base_patch4_window7_224\u001b[0m\n",
      "\u001b[32m2026-01-07 12:32:18.080\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.models.swin_encoder\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m82\u001b[0m - \u001b[1mPretrained: True, Image Size: 384\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating XR2Text model with HAQT-ARR projection layer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-07 12:32:21.478\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.models.swin_encoder\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m96\u001b[0m - \u001b[1mSwin feature dimension: 1024\u001b[0m\n",
      "\u001b[32m2026-01-07 12:32:21.483\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.models.swin_encoder\u001b[0m:\u001b[36m_freeze_layers\u001b[0m:\u001b[36m136\u001b[0m - \u001b[1mFrozen 404,424 parameters in 2 layers\u001b[0m\n",
      "\u001b[32m2026-01-07 12:32:21.485\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.models.swin_encoder\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m117\u001b[0m - \u001b[1mSwin Encoder initialized successfully\u001b[0m\n",
      "\u001b[32m2026-01-07 12:32:21.487\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.models.xr2text\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m110\u001b[0m - \u001b[1mBuilding HAQT-ARR (Hierarchical Anatomical) Projection Layer...\u001b[0m\n",
      "\u001b[32m2026-01-07 12:32:21.489\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.models.anatomical_attention\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m601\u001b[0m - \u001b[1mInitializing HAQT-ARR Projection Layer\u001b[0m\n",
      "\u001b[32m2026-01-07 12:32:21.490\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.models.anatomical_attention\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m602\u001b[0m - \u001b[1m  Visual dim: 1024 -> Language dim: 768\u001b[0m\n",
      "\u001b[32m2026-01-07 12:32:21.492\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.models.anatomical_attention\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m603\u001b[0m - \u001b[1m  Regions: 7, Total queries: 36\u001b[0m\n",
      "\u001b[32m2026-01-07 12:32:21.494\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.models.anatomical_attention\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m604\u001b[0m - \u001b[1m  Spatial priors: True, Adaptive routing: True\u001b[0m\n",
      "\u001b[32m2026-01-07 12:32:21.522\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.models.anatomical_attention\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m219\u001b[0m - \u001b[1mAnatomicalQueryTokens: 8 global + 7x4 region queries\u001b[0m\n",
      "\u001b[32m2026-01-07 12:32:22.147\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.models.anatomical_attention\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m692\u001b[0m - \u001b[1mHAQT-ARR Projection Layer initialized successfully\u001b[0m\n",
      "\u001b[32m2026-01-07 12:32:22.148\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.models.xr2text\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m141\u001b[0m - \u001b[1mBuilding BioBART Decoder...\u001b[0m\n",
      "\u001b[32m2026-01-07 12:32:22.150\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.models.biobart_decoder\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m85\u001b[0m - \u001b[1mInitializing decoder: GanjinZero/biobart-base\u001b[0m\n",
      "\u001b[32m2026-01-07 12:32:22.152\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.models.biobart_decoder\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m86\u001b[0m - \u001b[1mLabel smoothing: 0.0\u001b[0m\n",
      "\u001b[32m2026-01-07 12:32:25.004\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.models.biobart_decoder\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m116\u001b[0m - \u001b[1mDecoder initialized with hidden_dim: 768\u001b[0m\n",
      "\u001b[32m2026-01-07 12:32:25.006\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.models.xr2text\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mXR2Text Model initialized successfully!\u001b[0m\n",
      "\u001b[32m2026-01-07 12:32:25.016\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.models.xr2text\u001b[0m:\u001b[36m_log_model_info\u001b[0m:\u001b[36m173\u001b[0m - \u001b[1mTotal parameters: 251,441,388\u001b[0m\n",
      "\u001b[32m2026-01-07 12:32:25.018\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.models.xr2text\u001b[0m:\u001b[36m_log_model_info\u001b[0m:\u001b[36m174\u001b[0m - \u001b[1mTrainable parameters: 251,036,964\u001b[0m\n",
      "\u001b[32m2026-01-07 12:32:25.019\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.models.xr2text\u001b[0m:\u001b[36m_log_model_info\u001b[0m:\u001b[36m175\u001b[0m - \u001b[1mFrozen parameters: 404,424\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "XR2Text Model with HAQT-ARR (Novel)\n",
      "==================================================\n",
      "Total parameters: 251,441,388\n",
      "Trainable parameters: 251,036,964\n",
      "Frozen parameters: 404,424\n",
      "\n",
      "Anatomical regions: ['right_lung', 'left_lung', 'heart', 'mediastinum', 'spine', 'diaphragm', 'costophrenic_angles']\n",
      "Total queries: 36\n"
     ]
    }
   ],
   "source": [
    "from src.models.xr2text import XR2TextModel, DEFAULT_CONFIG\n",
    "from src.models.anatomical_attention import ANATOMICAL_REGIONS\n",
    "from src.data.dataloader import get_dataloaders\n",
    "from src.utils.device import setup_cuda_optimizations\n",
    "\n",
    "# Setup CUDA optimizations for RTX 4060\n",
    "setup_cuda_optimizations()\n",
    "\n",
    "# Create model with HAQT-ARR (Novel Architecture)\n",
    "print(\"Creating XR2Text model with HAQT-ARR projection layer...\")\n",
    "model_config = {\n",
    "    'image_size': config['image_size'],\n",
    "    'use_anatomical_attention': config['use_anatomical_attention'],  # Enable HAQT-ARR\n",
    "    'encoder': {\n",
    "        'model_name': config['encoder_name'],\n",
    "        'pretrained': True,\n",
    "        'freeze_layers': 2,  # Freeze first 2 Swin layers\n",
    "    },\n",
    "    'projection': {\n",
    "        # HAQT-ARR parameters (Novel)\n",
    "        'language_dim': config['language_dim'],\n",
    "        'num_regions': config['num_regions'],\n",
    "        'num_global_queries': config['num_global_queries'],\n",
    "        'num_region_queries': config['num_region_queries'],\n",
    "        'use_spatial_priors': config['use_spatial_priors'],\n",
    "        'use_adaptive_routing': config['use_adaptive_routing'],\n",
    "        'use_cross_region': config['use_cross_region'],\n",
    "        'feature_size': 12,  # 384/32 = 12x12 patches\n",
    "    },\n",
    "    'decoder': {\n",
    "        'model_name': config['decoder_name'],\n",
    "        'max_length': config['max_length'],\n",
    "    }\n",
    "}\n",
    "\n",
    "model = XR2TextModel.from_config(model_config)\n",
    "model = model.to(config['device'])\n",
    "\n",
    "# Model summary\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"XR2Text Model with HAQT-ARR (Novel)\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Frozen parameters: {total_params - trainable_params:,}\")\n",
    "print(f\"\\nAnatomical regions: {model.get_anatomical_regions()}\")\n",
    "print(f\"Total queries: {config['num_global_queries'] + config['num_regions'] * config['num_region_queries']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-07 12:32:25.725\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.data.dataloader\u001b[0m:\u001b[36mget_dataloaders\u001b[0m:\u001b[36m47\u001b[0m - \u001b[1mCreating dataloaders...\u001b[0m\n",
      "\u001b[32m2026-01-07 12:32:25.750\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.data.dataset\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mLoading MIMIC-CXR dataset (split: train)...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-07 12:32:29.593\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.data.dataset\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m76\u001b[0m - \u001b[1mLoaded 30633 samples\u001b[0m\n",
      "\u001b[32m2026-01-07 12:32:29.597\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.data.dataset\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mLoading MIMIC-CXR dataset (split: validation)...\u001b[0m\n",
      "\u001b[32m2026-01-07 12:32:31.272\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.data.dataset\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m76\u001b[0m - \u001b[1mLoaded 3063 samples\u001b[0m\n",
      "\u001b[32m2026-01-07 12:32:31.275\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.data.dataset\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mLoading MIMIC-CXR dataset (split: test)...\u001b[0m\n",
      "\u001b[32m2026-01-07 12:32:32.888\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.data.dataset\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m76\u001b[0m - \u001b[1mLoaded 3064 samples\u001b[0m\n",
      "\u001b[32m2026-01-07 12:32:32.901\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.data.dataloader\u001b[0m:\u001b[36mget_dataloaders\u001b[0m:\u001b[36m117\u001b[0m - \u001b[1mTrain samples: 30633\u001b[0m\n",
      "\u001b[32m2026-01-07 12:32:32.903\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.data.dataloader\u001b[0m:\u001b[36mget_dataloaders\u001b[0m:\u001b[36m118\u001b[0m - \u001b[1mVal samples: 3063\u001b[0m\n",
      "\u001b[32m2026-01-07 12:32:32.906\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.data.dataloader\u001b[0m:\u001b[36mget_dataloaders\u001b[0m:\u001b[36m119\u001b[0m - \u001b[1mTest samples: 3064\u001b[0m\n",
      "\u001b[32m2026-01-07 12:32:32.907\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.data.dataloader\u001b[0m:\u001b[36mget_dataloaders\u001b[0m:\u001b[36m120\u001b[0m - \u001b[1mBatch size: 4\u001b[0m\n",
      "\u001b[32m2026-01-07 12:32:32.909\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.data.dataloader\u001b[0m:\u001b[36mget_dataloaders\u001b[0m:\u001b[36m121\u001b[0m - \u001b[1mTrain batches: 7658\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batches: 7658\n",
      "Val batches: 766\n",
      "Test batches: 766\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "print(\"\\nLoading datasets...\")\n",
    "tokenizer = model.get_tokenizer()\n",
    "\n",
    "train_loader, val_loader, test_loader = get_dataloaders(\n",
    "    tokenizer=tokenizer,\n",
    "    batch_size=config['batch_size'],\n",
    "    num_workers=config['num_workers'],\n",
    "    image_size=config['image_size'],\n",
    "    max_length=config['max_length'],\n",
    "    train_subset=None,  # Use full dataset, or set to e.g., 1000 for testing\n",
    ")\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Val batches: {len(val_loader)}\")\n",
    "print(f\"Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Novel loss functions initialized\n",
      "âœ… Curriculum learning scheduler initialized\n",
      "âœ… Clinical validator initialized\n",
      "\n",
      "Total optimization steps: 47862\n",
      "Warmup steps: 500\n",
      "Novel losses: True\n",
      "Curriculum learning: True\n",
      "Clinical validation: True\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import AdamW\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from src.training.scheduler import get_cosine_schedule_with_warmup\n",
    "from src.utils.metrics import compute_metrics\n",
    "\n",
    "# NOVEL: Import novel training components\n",
    "from src.training.losses import CombinedNovelLoss\n",
    "from src.training.curriculum import AnatomicalCurriculumScheduler, create_curriculum_dataloader\n",
    "from src.utils.clinical_validator import ClinicalValidator\n",
    "\n",
    "# Optimizer\n",
    "no_decay = ['bias', 'LayerNorm.weight', 'layer_norm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {\n",
    "        'params': [p for n, p in model.named_parameters() \n",
    "                   if p.requires_grad and not any(nd in n for nd in no_decay)],\n",
    "        'weight_decay': config['weight_decay'],\n",
    "    },\n",
    "    {\n",
    "        'params': [p for n, p in model.named_parameters() \n",
    "                   if p.requires_grad and any(nd in n for nd in no_decay)],\n",
    "        'weight_decay': 0.0,\n",
    "    },\n",
    "]\n",
    "\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=config['learning_rate'])\n",
    "\n",
    "# Scheduler\n",
    "total_steps = len(train_loader) * config['epochs'] // config['gradient_accumulation_steps']\n",
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=config['warmup_steps'],\n",
    "    num_training_steps=total_steps,\n",
    ")\n",
    "\n",
    "# Mixed precision scaler\n",
    "scaler = GradScaler() if config['use_amp'] else None\n",
    "\n",
    "# NOVEL: Initialize novel loss functions\n",
    "if config.get('use_novel_losses', False):\n",
    "    novel_loss = CombinedNovelLoss(\n",
    "        use_anatomical_consistency=config.get('use_anatomical_consistency_loss', True),\n",
    "        use_clinical_entity=config.get('use_clinical_entity_loss', True),\n",
    "        use_region_focal=config.get('use_region_focal_loss', True),\n",
    "        use_cross_modal=config.get('use_cross_modal_loss', False),\n",
    "        anatomical_weight=config.get('anatomical_loss_weight', 0.1),\n",
    "        clinical_weight=config.get('clinical_loss_weight', 0.2),\n",
    "        focal_weight=config.get('focal_loss_weight', 0.15),\n",
    "        alignment_weight=config.get('alignment_loss_weight', 0.1),\n",
    "    )\n",
    "    print(\"âœ… Novel loss functions initialized\")\n",
    "else:\n",
    "    novel_loss = None\n",
    "\n",
    "# NOVEL: Initialize curriculum learning scheduler\n",
    "if config.get('use_curriculum_learning', False):\n",
    "    curriculum_scheduler = AnatomicalCurriculumScheduler()\n",
    "    print(\"âœ… Curriculum learning scheduler initialized\")\n",
    "else:\n",
    "    curriculum_scheduler = None\n",
    "\n",
    "# NOVEL: Initialize clinical validator\n",
    "if config.get('use_clinical_validation', False):\n",
    "    clinical_validator = ClinicalValidator()\n",
    "    print(\"âœ… Clinical validator initialized\")\n",
    "else:\n",
    "    clinical_validator = None\n",
    "\n",
    "print(f\"\\nTotal optimization steps: {total_steps}\")\n",
    "print(f\"Warmup steps: {config['warmup_steps']}\")\n",
    "print(f\"Novel losses: {config.get('use_novel_losses', False)}\")\n",
    "print(f\"Curriculum learning: {config.get('use_curriculum_learning', False)}\")\n",
    "print(f\"Clinical validation: {config.get('use_clinical_validation', False)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training history\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'val_loss': [],\n",
    "    'bleu_1': [],\n",
    "    'bleu_2': [],\n",
    "    'bleu_3': [],\n",
    "    'bleu_4': [],\n",
    "    'rouge_1': [],\n",
    "    'rouge_2': [],\n",
    "    'rouge_l': [],\n",
    "    'learning_rate': [],\n",
    "}\n",
    "\n",
    "best_metric = 0.0\n",
    "patience_counter = 0\n",
    "patience = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-07 12:32:33.036\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.utils.device\u001b[0m:\u001b[36mget_device\u001b[0m:\u001b[36m27\u001b[0m - \u001b[1mUsing CUDA device: NVIDIA GeForce RTX 4060 Laptop GPU\u001b[0m\n",
      "\u001b[32m2026-01-07 12:32:33.037\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.utils.device\u001b[0m:\u001b[36msetup_cuda_optimizations\u001b[0m:\u001b[36m86\u001b[0m - \u001b[1mEnabled cuDNN benchmark mode\u001b[0m\n",
      "\u001b[32m2026-01-07 12:32:33.039\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.utils.device\u001b[0m:\u001b[36msetup_cuda_optimizations\u001b[0m:\u001b[36m92\u001b[0m - \u001b[1mEnabled TF32 for matrix operations\u001b[0m\n",
      "\u001b[32m2026-01-07 12:32:33.041\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.utils.device\u001b[0m:\u001b[36msetup_cuda_optimizations\u001b[0m:\u001b[36m96\u001b[0m - \u001b[1mCleared CUDA cache\u001b[0m\n",
      "\u001b[32m2026-01-07 12:32:33.065\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.training.trainer\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m130\u001b[0m - \u001b[1mNovel loss functions enabled\u001b[0m\n",
      "\u001b[32m2026-01-07 12:32:33.067\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.training.trainer\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m136\u001b[0m - \u001b[1mCurriculum learning enabled\u001b[0m\n",
      "\u001b[32m2026-01-07 12:32:33.068\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.training.trainer\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mClinical validation enabled\u001b[0m\n",
      "\u001b[32m2026-01-07 12:32:33.080\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.training.trainer\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m183\u001b[0m - \u001b[1mHAQT-ARR enabled with regions: ['right_lung', 'left_lung', 'heart', 'mediastinum', 'spine', 'diaphragm', 'costophrenic_angles']\u001b[0m\n",
      "\u001b[32m2026-01-07 12:32:33.082\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.training.trainer\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m185\u001b[0m - \u001b[1mTrainer initialized on cuda\u001b[0m\n",
      "\u001b[32m2026-01-07 12:32:33.083\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.training.trainer\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m186\u001b[0m - \u001b[1mTraining for 50 epochs\u001b[0m\n",
      "\u001b[32m2026-01-07 12:32:33.084\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.training.trainer\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m187\u001b[0m - \u001b[1mTotal optimization steps: 47862\u001b[0m\n",
      "\u001b[32m2026-01-07 12:32:33.086\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.training.trainer\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m188\u001b[0m - \u001b[1mLabel smoothing: 0.1\u001b[0m\n",
      "\u001b[32m2026-01-07 12:32:33.087\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.training.trainer\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m189\u001b[0m - \u001b[1mScheduled sampling: True (start=1.0, end=0.5)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-07 12:32:33.089\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.training.trainer\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m190\u001b[0m - \u001b[1mRegion regularization: True (weight=0.01)\u001b[0m\n",
      "\u001b[32m2026-01-07 12:32:33.089\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.training.trainer\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m191\u001b[0m - \u001b[1mEarly stopping patience: 999\u001b[0m\n",
      "\u001b[32m2026-01-07 12:32:33.091\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.training.trainer\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m274\u001b[0m - \u001b[1mStarting training...\u001b[0m\n",
      "Epoch 1:   1%|â–Š                                                                                                                   | 7/958 [00:57<1:02:34,  3.95s/step, loss=15.9440]"
     ]
    }
   ],
   "source": [
    "# Main training loop - Using XR2TextTrainer class\n",
    "from src.training.trainer import XR2TextTrainer\n",
    "\n",
    "print(\"Starting training...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create trainer with all novel features enabled\n",
    "trainer = XR2TextTrainer(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "# Run training\n",
    "final_metrics = trainer.train()\n",
    "\n",
    "# Extract history from trainer for visualization\n",
    "# The trainer's metrics_tracker stores per-epoch metrics\n",
    "history = {\n",
    "    'train_loss': trainer.metrics_tracker.get_history('train_loss'),\n",
    "    'val_loss': trainer.metrics_tracker.get_history('val_loss'),\n",
    "    'bleu_1': trainer.metrics_tracker.get_history('bleu_1'),\n",
    "    'bleu_2': trainer.metrics_tracker.get_history('bleu_2'),\n",
    "    'bleu_3': trainer.metrics_tracker.get_history('bleu_3'),\n",
    "    'bleu_4': trainer.metrics_tracker.get_history('bleu_4'),\n",
    "    'rouge_1': trainer.metrics_tracker.get_history('rouge_1'),\n",
    "    'rouge_2': trainer.metrics_tracker.get_history('rouge_2'),\n",
    "    'rouge_l': trainer.metrics_tracker.get_history('rouge_l'),\n",
    "    'learning_rate': [trainer.scheduler.get_last_lr()[0]] * (trainer.current_epoch + 1),\n",
    "}\n",
    "\n",
    "# Add clinical validation metrics if enabled\n",
    "if config.get('use_clinical_validation', False):\n",
    "    history['clinical_accuracy'] = trainer.metrics_tracker.get_history('clinical_accuracy')\n",
    "    history['clinical_f1'] = trainer.metrics_tracker.get_history('clinical_f1')\n",
    "    history['critical_errors'] = trainer.metrics_tracker.get_history('critical_errors')\n",
    "\n",
    "# Save training history\n",
    "history_df = pd.DataFrame(history)\n",
    "history_df['epoch'] = range(1, len(history_df) + 1)\n",
    "os.makedirs('../data/statistics', exist_ok=True)\n",
    "history_df.to_csv('../data/statistics/training_history.csv', index=False)\n",
    "\n",
    "# Store predictions and references for sample display (will be populated in next cell)\n",
    "predictions = []\n",
    "references = []\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TRAINING COMPLETE!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nFinal Metrics:\")\n",
    "for key, value in final_metrics.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  {key}: {value:.4f}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Curves Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 NOVEL: Enhanced Curriculum Learning Analysis\n",
    "\n",
    "This section provides detailed analysis of our curriculum learning strategy,\n",
    "showing how it affects training dynamics and final performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# ENHANCED CURRICULUM LEARNING ANALYSIS\n",
    "# ============================================\n",
    "from src.training.curriculum import AnatomicalCurriculumScheduler\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"NOVEL: CURRICULUM LEARNING ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Initialize curriculum scheduler\n",
    "curriculum = AnatomicalCurriculumScheduler()\n",
    "\n",
    "# Display curriculum stages\n",
    "print(\"\\n1. CURRICULUM STAGES\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"\\n{'Stage':<20} {'Epochs':<15} {'Description':<40}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "stage_descriptions = {\n",
    "    'normal_cases': 'Normal X-rays, simple findings (e.g., \"lungs are clear\")',\n",
    "    'single_region': 'Single anatomical region findings (e.g., cardiomegaly)',\n",
    "    'multi_region': 'Multiple regions, moderate complexity',\n",
    "    'complex_cases': 'Complex cases with multiple severe findings',\n",
    "}\n",
    "\n",
    "for stage in curriculum.stages:\n",
    "    name = stage['name']\n",
    "    epoch_range = f\"{stage['epoch_start']}-{stage['epoch_end']}\"\n",
    "    desc = stage_descriptions.get(name, 'Full dataset')\n",
    "    print(f\"{name:<20} {epoch_range:<15} {desc:<40}\")\n",
    "\n",
    "# Curriculum difficulty scoring\n",
    "print(\"\\n2. SAMPLE DIFFICULTY SCORING\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "sample_reports = [\n",
    "    \"Lungs are clear. Heart size is normal. No acute cardiopulmonary process.\",\n",
    "    \"Mild cardiomegaly. Lungs are clear bilaterally.\",\n",
    "    \"Bilateral pleural effusions. Cardiomegaly. Pulmonary edema.\",\n",
    "    \"Large right pneumothorax. Left lung consolidation. Cardiomegaly. Bilateral effusions. ETT in place.\",\n",
    "]\n",
    "\n",
    "print(\"\\nSample Reports with Difficulty Scores:\")\n",
    "for i, report in enumerate(sample_reports):\n",
    "    scores = curriculum.difficulty_scorer(report)\n",
    "    total_difficulty = scores.get('num_findings', 0) + scores.get('severity_score', 0)\n",
    "    print(f\"\\n[Sample {i+1}] Difficulty: {total_difficulty:.1f}\")\n",
    "    print(f\"   Report: {report[:70]}...\")\n",
    "    print(f\"   Findings: {scores.get('num_findings', 0)}, Regions: {scores.get('num_regions', 0)}\")\n",
    "\n",
    "# Simulated curriculum learning results\n",
    "print(\"\\n3. CURRICULUM LEARNING IMPACT\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# ============================================\n",
    "# NOTE: Curriculum learning benefits will be measured\n",
    "# after training completes. The above shows the CONCEPT.\n",
    "# Real performance comparison will be added post-training.\n",
    "# ============================================\n",
    "\n",
    "# NOTE: Curriculum learning benefits will be measured after training.\n",
    "# Real performance data will be added post-training.\n",
    "\n",
    "# ============================================\n",
    "# POST-TRAINING: Curriculum Learning Analysis\n",
    "# This will show real results after training completes\n",
    "# ============================================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Check if training history exists\n",
    "history_path = '../data/statistics/training_history.csv'\n",
    "if os.path.exists(history_path):\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"CURRICULUM LEARNING RESULTS (Real Data)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Load training history\n",
    "    df = pd.read_csv(history_path)\n",
    "    \n",
    "    # Analyze curriculum stage transitions\n",
    "    stage_transitions = [5, 15, 30]  # Epochs where curriculum changes\n",
    "    \n",
    "    print(\"\\nPerformance at Curriculum Stage Transitions:\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for i, epoch in enumerate([1, 5, 15, 30, 50]):\n",
    "        if epoch <= len(df):\n",
    "            row = df.iloc[epoch-1]\n",
    "            stage = ['Stage 1 (Normal)', 'Stage 1â†’2', 'Stage 2â†’3', 'Stage 3â†’4', 'Final'][i]\n",
    "            print(f\"Epoch {epoch} ({stage}):\")\n",
    "            print(f\"  BLEU-4: {row['bleu_4']:.4f}\")\n",
    "            print(f\"  ROUGE-L: {row['rouge_l']:.4f}\")\n",
    "            print(f\"  Loss: {row['val_loss']:.4f}\")\n",
    "            print()\n",
    "    \n",
    "    # Plot curriculum impact\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # BLEU-4 progression\n",
    "    axes[0].plot(df['epoch'], df['bleu_4'], linewidth=2, color='blue')\n",
    "    for trans in stage_transitions:\n",
    "        if trans <= len(df):\n",
    "            axes[0].axvline(x=trans, color='red', linestyle='--', alpha=0.5)\n",
    "            axes[0].text(trans, axes[0].get_ylim()[1]*0.9, 'Stage\\nChange', \n",
    "                        ha='center', fontsize=8)\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('BLEU-4')\n",
    "    axes[0].set_title('BLEU-4 Progression with Curriculum Stages')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Loss progression\n",
    "    axes[1].plot(df['epoch'], df['val_loss'], linewidth=2, color='orange')\n",
    "    for trans in stage_transitions:\n",
    "        if trans <= len(df):\n",
    "            axes[1].axvline(x=trans, color='red', linestyle='--', alpha=0.5)\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].set_ylabel('Validation Loss')\n",
    "    axes[1].set_title('Loss Progression with Curriculum Stages')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../data/figures/curriculum_impact.png', dpi=300)\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"âœ… Curriculum learning analysis complete!\")\n",
    "    print(\"   Figure saved: ../data/figures/curriculum_impact.png\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸  Training history not found yet.\")\n",
    "    print(\"   Run this cell again after training completes.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# TRAINING CURVES VISUALIZATION\n",
    "# ============================================\n",
    "# This works with both freshly trained history AND loaded history from CSV!\n",
    "\n",
    "# Load history from CSV if not already in memory (i.e., training was skipped)\n",
    "history_path = \"../data/statistics/training_history.csv\"\n",
    "if not history.get('train_loss') or len(history.get('train_loss', [])) == 0:\n",
    "    if os.path.exists(history_path):\n",
    "        print(\"ðŸ“‚ Loading training history from saved CSV...\")\n",
    "        history_df = pd.read_csv(history_path)\n",
    "        history = history_df.to_dict(orient='list')\n",
    "        if 'epoch' in history:\n",
    "            del history['epoch']\n",
    "        print(f\"   Loaded {len(history.get('train_loss', []))} epochs of history\")\n",
    "    else:\n",
    "        print(\"âš ï¸ No training history found!\")\n",
    "else:\n",
    "    # Save training history if it came from training\n",
    "    history_df = pd.DataFrame(history)\n",
    "    history_df['epoch'] = range(1, len(history_df) + 1)\n",
    "    history_df.to_csv(history_path, index=False)\n",
    "    print(\"ðŸ’¾ Training history saved to CSV\")\n",
    "\n",
    "# Check if we have history to plot\n",
    "if history.get('train_loss') and len(history['train_loss']) > 0:\n",
    "    # Plot training curves with NOVEL features\n",
    "    num_plots = 6 if config.get('use_novel_losses', False) or config.get('use_clinical_validation', False) else 4\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 10)) if num_plots > 4 else plt.subplots(2, 2, figsize=(14, 10))\n",
    "    axes = axes.flatten() if num_plots > 4 else axes\n",
    "\n",
    "plot_idx = 0\n",
    "\n",
    "# Loss curves\n",
    "axes[plot_idx].plot(history['train_loss'], label='Train Loss', color='blue', linewidth=2)\n",
    "axes[plot_idx].plot(history['val_loss'], label='Val Loss', color='orange', linewidth=2)\n",
    "axes[plot_idx].set_xlabel('Epoch')\n",
    "axes[plot_idx].set_ylabel('Loss')\n",
    "axes[plot_idx].set_title('Training and Validation Loss')\n",
    "axes[plot_idx].legend()\n",
    "axes[plot_idx].grid(True, alpha=0.3)\n",
    "plot_idx += 1\n",
    "\n",
    "# BLEU scores\n",
    "axes[plot_idx].plot(history['bleu_1'], label='BLEU-1', linewidth=2)\n",
    "axes[plot_idx].plot(history['bleu_2'], label='BLEU-2', linewidth=2)\n",
    "axes[plot_idx].plot(history['bleu_3'], label='BLEU-3', linewidth=2)\n",
    "axes[plot_idx].plot(history['bleu_4'], label='BLEU-4', linewidth=2)\n",
    "axes[plot_idx].set_xlabel('Epoch')\n",
    "axes[plot_idx].set_ylabel('Score')\n",
    "axes[plot_idx].set_title('BLEU Scores')\n",
    "axes[plot_idx].legend()\n",
    "axes[plot_idx].grid(True, alpha=0.3)\n",
    "plot_idx += 1\n",
    "\n",
    "# ROUGE scores\n",
    "axes[plot_idx].plot(history['rouge_1'], label='ROUGE-1', linewidth=2)\n",
    "axes[plot_idx].plot(history['rouge_2'], label='ROUGE-2', linewidth=2)\n",
    "axes[plot_idx].plot(history['rouge_l'], label='ROUGE-L', linewidth=2)\n",
    "axes[plot_idx].set_xlabel('Epoch')\n",
    "axes[plot_idx].set_ylabel('Score')\n",
    "axes[plot_idx].set_title('ROUGE Scores')\n",
    "axes[plot_idx].legend()\n",
    "axes[plot_idx].grid(True, alpha=0.3)\n",
    "plot_idx += 1\n",
    "\n",
    "# NOVEL: Novel loss components\n",
    "if config.get('use_novel_losses', False) and 'anatomical_consistency_loss' in history:\n",
    "    axes[plot_idx].plot(history['anatomical_consistency_loss'], label='Anatomical Consistency', linewidth=2, color='purple')\n",
    "    axes[plot_idx].plot(history['clinical_entity_loss'], label='Clinical Entity', linewidth=2, color='red')\n",
    "    axes[plot_idx].plot(history['region_focal_loss'], label='Region Focal', linewidth=2, color='green')\n",
    "    axes[plot_idx].set_xlabel('Epoch')\n",
    "    axes[plot_idx].set_ylabel('Loss')\n",
    "    axes[plot_idx].set_title('Novel Loss Components (NOVEL)')\n",
    "    axes[plot_idx].legend()\n",
    "    axes[plot_idx].grid(True, alpha=0.3)\n",
    "    plot_idx += 1\n",
    "\n",
    "# NOVEL: Clinical validation metrics\n",
    "if config.get('use_clinical_validation', False) and 'clinical_accuracy' in history:\n",
    "    ax_twin = axes[plot_idx].twinx()\n",
    "    axes[plot_idx].plot(history['clinical_accuracy'], label='Clinical Accuracy', linewidth=2, color='blue')\n",
    "    axes[plot_idx].plot(history['clinical_f1'], label='Clinical F1', linewidth=2, color='orange')\n",
    "    axes[plot_idx].set_xlabel('Epoch')\n",
    "    axes[plot_idx].set_ylabel('Score', color='black')\n",
    "    axes[plot_idx].set_title('Clinical Validation Metrics (NOVEL)')\n",
    "    axes[plot_idx].legend(loc='upper left')\n",
    "    axes[plot_idx].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Critical errors on secondary axis\n",
    "    ax_twin.plot(history['critical_errors'], label='Critical Errors', linewidth=2, color='red', linestyle='--')\n",
    "    ax_twin.set_ylabel('Critical Errors', color='red')\n",
    "    ax_twin.legend(loc='upper right')\n",
    "    ax_twin.tick_params(axis='y', labelcolor='red')\n",
    "    plot_idx += 1\n",
    "\n",
    "# Learning rate\n",
    "axes[plot_idx].plot(history['learning_rate'], color='green', linewidth=2)\n",
    "axes[plot_idx].set_xlabel('Epoch')\n",
    "axes[plot_idx].set_ylabel('Learning Rate')\n",
    "axes[plot_idx].set_title('Learning Rate Schedule')\n",
    "axes[plot_idx].set_yscale('log')\n",
    "axes[plot_idx].grid(True, alpha=0.3)\n",
    "\n",
    "# Hide unused subplots\n",
    "for i in range(plot_idx + 1, len(axes)):\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../data/figures/training_curves_novel.png', dpi=300)\n",
    "plt.show()\n",
    "print(\"âœ… Training curves saved with NOVEL features visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Sample Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sample predictions vs ground truth\n",
    "print(\"Sample Predictions vs Ground Truth:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i in range(min(5, len(predictions))):\n",
    "    print(f\"\\n--- Sample {i+1} ---\")\n",
    "    print(f\"\\nGround Truth:\")\n",
    "    print(references[i][:500] + \"...\" if len(references[i]) > 500 else references[i])\n",
    "    print(f\"\\nGenerated:\")\n",
    "    print(predictions[i][:500] + \"...\" if len(predictions[i]) > 500 else predictions[i])\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Final Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best results\n",
    "best_epoch = np.argmax([h['bleu_4'] + h['rouge_l'] for h in [dict(zip(history.keys(), v)) for v in zip(*history.values())]])\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TRAINING RESULTS SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nBest Epoch: {best_epoch + 1}\")\n",
    "print(f\"\\nBest Metrics:\")\n",
    "print(f\"  BLEU-1: {history['bleu_1'][best_epoch]:.4f}\")\n",
    "print(f\"  BLEU-2: {history['bleu_2'][best_epoch]:.4f}\")\n",
    "print(f\"  BLEU-3: {history['bleu_3'][best_epoch]:.4f}\")\n",
    "print(f\"  BLEU-4: {history['bleu_4'][best_epoch]:.4f}\")\n",
    "print(f\"  ROUGE-1: {history['rouge_1'][best_epoch]:.4f}\")\n",
    "print(f\"  ROUGE-2: {history['rouge_2'][best_epoch]:.4f}\")\n",
    "print(f\"  ROUGE-L: {history['rouge_l'][best_epoch]:.4f}\")\n",
    "print(f\"\\nFinal Train Loss: {history['train_loss'][-1]:.4f}\")\n",
    "print(f\"Final Val Loss: {history['val_loss'][-1]:.4f}\")\n",
    "\n",
    "# Save results table\n",
    "results_table = pd.DataFrame({\n",
    "    'Metric': ['BLEU-1', 'BLEU-2', 'BLEU-3', 'BLEU-4', 'ROUGE-1', 'ROUGE-2', 'ROUGE-L'],\n",
    "    'Score': [\n",
    "        history['bleu_1'][best_epoch],\n",
    "        history['bleu_2'][best_epoch],\n",
    "        history['bleu_3'][best_epoch],\n",
    "        history['bleu_4'][best_epoch],\n",
    "        history['rouge_1'][best_epoch],\n",
    "        history['rouge_2'][best_epoch],\n",
    "        history['rouge_l'][best_epoch],\n",
    "    ]\n",
    "})\n",
    "results_table.to_csv('../data/statistics/best_results.csv', index=False)\n",
    "print(\"\\nResults saved to ../data/statistics/best_results.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (swin)",
   "language": "python",
   "name": "swin"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
